% TODO check for the following spelling:
% Q-learning (not Q-Learning)
% Deep Q-learning (not Deep-Q-learning) same for Tabular
% value iteration (not value-iteration)
% REINFORCE (not Reinforce)
% Actor-Critic (not Actor Critic) and separated: Actor and Critic (not actor/critic)
% Q-value, value (not Q-Value or Value)
% Q-table (not Q-Table)
% Action (not Aktion)
% Reward (not reward)
% *-based (like policy-based)
% FrozenLake (not frozen lake or frozen-lake)
% CartPole Environment, but Cart-pole Problem (named in paper)
% RoboschoolPong Environment
% 'der' value / 'der' Q-value (not 'die' value or 'das' value)

% Allgemeine section Aufbau
% - Theorie
% - Einordnung (on-policy/off-policy, value-based/policy-based)
% - Implementationen
%   - Herausstellen wie Exploration und Exploitation umgesetzt ist
%   - falls vorhanden Graphs einbauen, die den Trainingsprozess veranschaulichen

\documentclass[11pt]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[german]{babel}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{listings}
\usepackage{graphicx}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\PassOptionsToPackage{hyphens}{url}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{subcaption}

\newcommand{\source}[1]{\vspace{-5pt} \caption*{\hfill \textbf{Source:} {#1}} }

\newenvironment{conditions}
  {\par\vspace{\abovedisplayskip}\noindent\begin{tabular}{>{$}l<{$} @{${}:{}$} l}}
  {\end{tabular}\par\vspace{\belowdisplayskip}}

\newcommand{\lbparagraph}[1]{\paragraph*{#1}\mbox{}\\}

\renewcommand{\baselinestretch}{1.2}
\geometry{
  left=3cm,
  right=3cm,
  top=3cm,
  bottom=3cm,
  bindingoffset=5mm
}


\usepackage{xcolor}
 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}


\begin{document}
% \sloppy

\begin{titlepage}
	\centering
	{\scshape\LARGE Hochschule für Technik und Wirtschaft Berlin \par}
	\vspace{2cm}
	{\Huge \scshape{Independent Coursework}\par}
	\vspace{2cm}
	{\LARGE \scshape{Reinforcement Learning}\par}
	{\scshape\Large Projekt Dokumentation\par}
	%\includegraphics[width=0.9\linewidth]{title.jpg}\par\vspace{1cm}
	\vspace{4cm}
	{\large\itshape Bruno Schilling,\\Philipp Bönsch\par}
	\vfill
	
% Bottom of the page
	{\large Betreuer: Prof. Dr.-Ing. Barthel \par}
	\vspace{1cm}
	{\large \today\par}
\end{titlepage}

\lstset{basicstyle=\ttfamily\small,breaklines=true}
\newpage
\tableofcontents
\newpage
\section{Introduction}

\subsection{Problemstellung}
Machine Learning Methoden werden zunehmend benutzt, um Probleme in der realen Welt zu
lösen und erhalten daher in der Zukunft einen immer höheren Stellenwert. Spiele im
speziellen stellen meist Problemstellungen dar, welche zum Teil auch schon durch Machine
Learning Verfahren absolviert werden konnten. Darunter fallen klassische Brettspiele, wie Go
oder Schach\cite{DM2018} aber auch Echtzeitspiele, wie Dota 2\cite{OA2019},
Quake\cite{DM2019} oder Starcraft\cite{DM2019_2}. Für das Supervised Research Independent Coursework
ergibt sich folgende Problemstellung: Welche Machine Learning Verfahren können für das
absolvieren eines Computerspieles verwendet werden und wie integriert man diese in die
virtuelle Umgebung?


\subsection{Zielsetzung}
Es soll innerhalb eines Supervised Research Independent Courseworks untersucht werden, wie
sich ein Agent in einer virtuellen Spielumgebung mit Hilfe von Machine Learning Methoden,
speziell Reinforcement Learning Verfahren, steuern lässt. Dafür werden initial
Untersuchungen durchgeführt, um den aktuellen Stand der Technik zu erfassen. Genauer
sollen Algorithmen mittels der von OpenAI Gym zur Verfügung gestellten Testumgebung
untersucht werden. Die dabei gesammelten Erfahrungen sollen benutzt werden, um einen
Agenten für ein Computerspiel zu entwickeln. Das Ziel des Research Projektes ist es, neue
Kenntnisse im Bereich Reinforcement Learning zu erhalten und bereits bestehendes Wissen in
den Schwerpunkten Artificial Intelligence und Machine Learning zu festigen. Weiterhin soll
dieses Wissen anschließend in einem Prototyp umgesetzt werden, bei dem ein Agent in einem
Echtzeitspiel gesteuert wird. In der ausgearbeiteten Dokumentation wird dabei sowohl auf die
Recherche und Aufarbeitung der theoretischen Teile von Reinforcement Learning als auch auf
die abschließende Implementierung der prototypischen Umsetzung eines Agenten eingegangen.
\newpage

\section{Grundlagen}
In diesem Abschnitt werden die Grundlagen des Machine Learning Bereichs Reinforcement Learning 
erläutert. Damit soll eine Grundlage für Themenbereiche in nachfolgenden Kapiteln geschaffen
werden, welche sich auf die Terminologie dieses Abschnittes beziehen.


\subsection{Reinforcement Learning}
Reinforcement Learning beschreibt eine Ansammlung von Machine Learning Verfahren, in denen die 
lernende Entität, auch \textit{Agent}, keine bezeichneten Beispiele für das Lösen eines Problemes 
erhält. Die einzige Information, welche mitgeteilt wird, ist Feedback zu gewählten Aktionen in Form
einer Belohnung oder Bestrafung. Diese Art des Feedbacks wird auch als Verstärkung bezeichnet.
Somit muss vom Lernenden selbstständig eine Strategie entwickelt werden, um das gegebene
Problem zu lösen, indem versucht wird den erhaltenen Belohnungswert zu maximieren. Die nachfolgenden
Kapitel beschreiben Bestandteile und Hauptakteure von Systemen im Bereich Reinforcement Learning.


\subsubsection{Agent}
Ein Agent nimmt seine Umgebung (\textit{Environment}) über Sensoren wahr und kann mittels Aktuatoren
Handlungen (\textit{Actions}) vollziehen um diese Umgebung zu beeinflussen oder zu verändern 
\cite[~S.60]{RN2009}. Für die ausgeführten Actions erhält der Agent einen Leitwert wie positiv oder
negativ sich diese Action auf die Umgebung oder den Agenten ausgewirkt hat. Wie bereits im vorherigen
Absatz erwähnt, kann der Agent als lernende Entität bezeichnet werden, da dieser seine Verhaltensweise
im Bereich  Reinforcement Learning selbstständig immer weiter in Richtung bestmöglicher Lösung einer 
Problemstellung optimiert. In einem Computerspiel kann ein Agent als menschlicher Spieler
repräsentiert werden. Dabei zählen die Augen und Ohren zu seinen Sensoren mit denen das Spiel
wahrgenommen wird und über seine motorischen Fähigkeiten und somit dem bedienen einer Tastatur
können Handlungen vollzogen werden. Ein im Spiel umgesetzter Nicht-Spieler-Charakter wird ebenfalls
als Agent gesehen, wobei dessen Wahrnehmung und Aktuatoren meist direkt im Spiel umgesetzt sind.


\subsubsection{Environment}
Das Environment definiert alles was den Agenten umgibt. Die Kommunikation zwischen einem Environment
und dem Agenten ist limitiert auf die \textit{Actions}, welche der Agent ausführen kann, die 
\textit{Observations}, welche der Agent nach dem Ausführen einer Action vom Environment erhält, 
sprich wahrnimmt, und durch die Bewertung der getätigten Aktionen, dem \textit{Reward} 
\cite[~S.5]{L2018}. Im Falle eines Computerspiels umfasst das Environment alle im Spiel festgelegten
Gesetzmäßigkeiten, aber auch die Netzwerkverbindung oder das Medium auf dem das Spiel ausgeführt
wird kann zum Environment gezählt werden.


\subsubsection{Action}
\label{sec:action}
Eine Action bietet die Möglichkeit für einen Agenten aktiv Einfluss auf das Geschehen im
Environment zu nehmen. Zu Actions zählen das Ausführen von Spielzügen, die durch das
Regelwerk erlaubt sind sowie die Änderung eines Zustandes in umfangreicheren Systemen. Die
Komplexität der Actions kann dabei ebenfalls variieren. Des Weiteren unterscheidet man im Bereich
Reinforcement Learning zwischen zwei Arten von Actions: diskrete und kontinuierliche Actions.
Diskrete Actions definieren eine endliche Menge an gegenseitig ausschließenden Handlungen die ein
Agent vollziehen kann \cite[~S.8]{L2018}. Dazu zählen Actions wie sich nach links und rechts zu
bewegen oder zu springen. Kontinuierliche Actions werden hingegen als Handlungen definiert, die
über einen zusätzlichen Wert beeinflusst werden, wie zum Beispiel das Steuern eines Fahrzeuges
mit einem Lenkrad. Dabei ist nicht ausschließlich die Lenkrichtung ausschlaggebend, sondern auch
der Einschlagswinkel des Lenkrades, welcher die Intensität der Lenkung festlegt.


\subsubsection{Observation}
Unter einer Observation versteht man die Wahrnehmung des Agenten, welche von dem Environment 
bereitgestellt wird. Dabei enthält eine Observation immer Informationen dazu, was gerade in der
Umgebung des Agenten passiert. Zusätzlich müssen Observations für den Agenten nicht über das
gesamte Environment Einblick geben, sondern können dessen Wahrnehmung auf notwendige oder auch
ausschließlich gewollte Informationen limitieren \cite[~S.8 f.]{L2018}. Eine Observation kann 
aus einzelnen Werten bestehen, welche zum Beispiel im Falle von Pong Informationen wie Position
und Geschwindigkeiten zu den Schlägern und dem Spielball zurückliefern. Zusätzlich kann eine
Observationen auch in Form eines Bildes oder einer Aneinanderreihung von Bildern umgesetzt sein.
Das Bild wird dabei von dem Agent verarbeitet und dieser führt dementsprechende Actions zum
visuell wargenommenen Zustand, sprich der Anordnung der Pixel aus. 


\subsubsection{Reward}
Im Gebiet Reinforcement Learning beschreibt der Reward einen Wert, welcher periodisch oder nach
dem Eintreffen einer definierten Gegebenheit dem Agenten vom Environment bereitgestellt wird.
Ziel des Rewards ist es dem Agenten Auskunft darüber zu geben, wie gut sich dieser verhält, sprich
diesen zu bewerten. Wie bereits beschrieben, versucht der Agent den erhaltenen Reward über den
Verlauf des Trainingsprozesses hinweg zu maximieren. Somit bildet der Reward als Feedback ein
Kernelement im Bereich Reinforcement Learning und definiert den \textit{bestärkenden} Anteil der 
Verfahren des machinellen Lernens \cite[~S.6 f.]{L2018}. Der Reward ist immer lokal zu betrachten,
dass heißt er gibt ausschließlich Auskunft über den Erfolg der aktuell ausgeführten Actions und
nicht über das bereits insgesamt gesammelte Ergebnis aller Erfolge und Misserfolge. Des Weiteren
bedeutet ein hoher lokaler Reward nicht, dass der daraus folgende Zustand global betrachet keine
noch größeren negativen Auswirkungen auf den Agenten haben wird.


\subsection{Markov Processes}
Der Makrov Process und seine Erweiterungen bilden das theoretische Konstrukt für Reinforcement Learning
Verfahren. Durch die Beleuchtung dieser Prozesse werden weitere Terminologien geklärt, welche ebenfalls
ihren Einsatz im Reinforcement Learning finden.


\subsubsection{Markov Process}
Die einfachste Variante in der Markov Familie ist der Markov Process (Markov-Kette). Ein Markov Process
beschreibt ein System, welches ausschließlich observiert werden kann, sprich es kann nicht in die Dynamik
des Systems eingegriffen werden. Jeder Markov Process enthält eine Menge von \textit{States}, zwischen
denen das System nach den Gesetzlichkeiten der Systemdynamik hin und her wechseln kann. Die Menge aller
möglichen States wird dabei als \textit{state space} bezeichnet \cite[~S.12]{L2018}. Wird das System
observiert ergibt sich durch die Wechsel von State zu State eine Folge, welche als \textit{History}
bezeichnet wird. 

Damit ein Prozess als Markov Process bezeichnet werden kann muss dieser die \textit{Markov Eigenschaft}
aufweisen, welche definiert, dass die zukünftige Dynamik des Systems von jedem State nur
von diesem State abhängen darf. Die Markov Eigenschaft legt somit fest, dass ein State
differenziert von jedem anderen State des Systems betrachtet werden kann und deshalb die
History für die Vorhersage der Systemdynamik nicht notwendig ist.

Ein Prozess der die Markov Eigenschaft aufweißt muss die Fähigkeit bieten, eine Vorhersage über
die Wahrscheinlichkeit des Eintreffens eines Folgezustandes bereitzustellen \cite[~S.12 f.]{L2018}. 
Dafür werden Wahrscheinlichkeiten für die Übergänge zwischen zwei States in einer
\textit{Transition Matrix} festgehalten. \autoref{fig:markov}: \nameref{fig:markov}
veranschaulicht ein Wettermodell in der Repräsentationsform eines Markov Process mit den States
\textit{Sunny} und \textit{Rainy} und deren Übergangswahrscheinlichkeiten in zukünftige
Folgezustände.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.19]{res/sunny-rainy-weather-model.png}
\caption{Sunny/Rainy Wettermodell}
\source{\cite[~S.13 - Chapter 1 - Figure 4]{L2018}}
\label{fig:markov}
\end{figure}


\subsubsection{Markov Reward Process}
\label{sec:markov_reward_process}
Die erste Erweiterung zum Markov Process ensteht durch die Hinzunahme eines weiteren Wertes zu den 
Übergängen zwischen zwei States, dem \textit{Reward} $R$. Dieser Wert definiert wie hoch die Belohnung
oder Bestrafung bei einem Wechsel der States ausfällt. Durch den Reward kann anschließend der
\textit{value of state} $V(s)$ berechnet werden. Für jeden State $s$ ist der value of state $V(s)$
der durchschnittliche (oder erwartete) Ertrag, welcher durch das Verfolgen des Markov Reward Process
entsteht und somit durch alleiniges Observieren die Bewertung von Zuständen ermöglicht.
Zusätzlich wird der \textit{discount factor} $\gamma$ eingeführt, welcher den erhaltenen Reward
für Übergänge die weiter in der Zukunft liegen verringert. Der value of state wird dabei wie
nachfolgend formuliert für eine \textit{Episode} berechnet, wobei eine Episode immer aus einer
Folge von States besteht und entweder durch das Eintreffen eines terminierenden States oder das
Erreichen einer Maximalanzahl von States beendet wird:

\begin{equation}
V(s)_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} ... = \sum_{k=0}^\infty \gamma^{k} R_{t+k+1}
\label{eq:value-of-state}
\end{equation} 

\begin{conditions}
 V(s)     	&  value of state \\
 R	     	&  Reward \\   
 \gamma 	&  discount factor für Rewards, die weiter in der Zukunft liegen
\end{conditions}


\subsubsection{Markov Decision Process}
Der Markov Decision Process erweitert den Markov Reward Process um eine endliche Menge an Actions.
Diese Menge entspricht dem \textit{action space} des Agenten. Dadurch kommt es nun nicht mehr
ausschließlich zur passiven Observierung, sondern der Agent kann zu jeder Zeit eine Action wählen um
somit die Wahrscheinlichkeiten des Zielzustandes selbstständig zu beeinflussen. Des Weiteren hängt 
der Reward, den der Agent erhält, nun nicht mehr ausschließlich von dem Zustand ab, in dem er endet,
sondern auch von der Action, die zu diesem Zustand führt. 

Die Verhaltensweise, die der Agent ausübt und welche für die Wahl der Actions verantwortlich ist,
wird als \textit{Policy} bezeichnet. Formal wird diese nach Lapan\cite[~S.22 f.]{L2018} als
Wahrscheinlichkeitsverteilung über Actions für jeden möglichen Zustand definiert.


\subsection{Weiterführende Taxonomie}
In diesem Kapitel werden diverse Kategorisierungen für Reinforcement Learning Verfahren beschrieben,
welche nachfolgend bei der Erläuterung der realisierten Verfahren für deren Einordnung genutzt werden.  


\subsubsection{Value learning - policy learning}
Bei \textit{value learning} Verfahren wird jedem State oder jeder Kombination aus State und Action ein
value/score zugewiesen, welcher diesen bewertet. Durch diese Bewertung kann der Agent anschließend eine
Wahl zur besten Action treffen \cite[~S.NA]{L2018}. \textit{Policy learning} bezeichnet hingegen das
direkte Lernen einer Verhaltensweise, wobei anhand einer Observation eine Wahrscheinlichkeitsverteilung
der zu wählenden Actions erzeugt wird. Der Agent wählt anschließend mithilfe der jeweiligen
Wahrscheinlichkeiten die möglichen Actions \cite[~S.NA]{L2018}.


\subsubsection{On-policy - off-policy}
Die Kategorisierung \textit{off-policy} beschreibt die Fähigkeit von Reinforcement Learning Verfahren auf
Grundlage alter, bereits vor längerer Zeit aufgenommener Daten zu trainieren \cite[~S.NA]{L2018}. 
Im Kontrast dazu definiert \textit{on-policy} die Notwendigkeit von immer neu aufgezeichneten 
Trainingsdaten, welche nach einem Trainingsdurchlauf wieder verworfen und somit neu gesammelt
werden müssen.


\subsubsection{Model-free - model-based}
Der Term \textit{model-free} bezieht sich auf Verfahren, welche kein Model der Umgebung oder des Rewards
während des Trainingsprozesses aufbauen. Es werden ausschließlich durch aktuelle Observations
zugehörige Actions berechnet. Model-based Methoden versuchen im Gegenzug vorherzusagen, was 
die nächste Observation oder wie hoch der nächste Reward sein wird \cite[~S.NA]{L2018}. Aufgrund dieser
Vorhersage versucht der Agent die bestmögliche Action zu wählen, wobei in den meisten
Fällen die Vorhersage mehrfach und dabei auch mit der Betrachtung zukünftiger Schritte durchgeführt
wird \cite[~S.NA]{L2018}.


\subsection{Exploration und Exploitation}
In diesem Abschnitt wird der Konflikt zwischen Exploration und Exploitation des
Reinforcement Learnings erläutert und anhand des k-armed-Bandit Problems veranschaulicht.
Die dabei realisierten Implementierungen werden beschrieben und deren Ergebnisse
vorgestellt.


\subsubsection{Das k-armed-Bandit Problem}
Nach R. Sutton \cite[~S.26]{SB1998} besteht das k-armed-Bandit Problem darin, aus $k$
verschiedenen Actions zu wählen, wobei nachdem eine Action gewählt wurde, der Agent einen
Reward erhält, der aus einer Wahrscheinlichkeitsverteilung gezogen wird. Der
Erwartungswert der Wahrscheinlichkeitsverteilung hängt von der gewählten Action ab. Das
Ziel besteht darin mit der Ausführung von $n$ Actions die höchstmögliche Summe von Rewards
zu erhalten.

Der Erwartungswert der hinter der Verteilung einer Action steht, wird als $value$ der Action
bezeichnet. Würden die values der einzelnen Actions dem Agenten zur Verfügung stehen, wäre die 
Wahl der Action einfach, indem die Action mit dem höchsten Erwartungswert gewählt werden 
könnte. Allgemein ist in Reinforcement Learning Problemen der value einer Action nicht bekannt.
Stattdessen gibt es ausschließlich Ab-schätzungen bezüglich des values einer Action, die aus den
gesammelten Erfahrungen hervorgehen. Um den value einer Action besser abschätzen zu können,
muss diese Action ausprobiert werden. Umso häufiger die Action ausgeführt wurde, umso besser
ist auch die Abschätzung des values dieser Action.

Der Vorgang des Ausprobierens wird $Exploration$ genannt. Im Gegensatz dazu steht die
$Exploitation$ bei der die gesammelten Erfahrungen genutzt werden, um die Action
auszuwählen, die den höchsten Reward verspricht.

Das k-armed-Bandit Problem gilt als Vereinfachung zu allgemeinen Reinforcement Learning
Problemen, da nicht zwischen verschiedenen Ausgangszuständen unterschieden wird, sprich
es wird keine Observation durchgeführt. Weiterhin wird der Erwartungswert des
Rewards einer Action nicht über die Dauer des Spiels verändert. Dies bedeutet, dass die
values der unterschiedlichen Actions am Anfang und Ende des Spiels identisch sind, wodurch
auch Erfahrungen, die zu Beginn des Spielens gemacht wurden, am Ende benutzt werden
können.

\subsubsection{Exploration-Strategien}
Im folgenden werden verschiedene Exploration-Strategien beleuchtet, die von R. Sutton und
A. Barto \cite[~S.19]{SB1998} vorgestellt wurden. Dazu werden diese erst theoretisch
erklärt und anschließend praktisch an einem eigenständig entwickelten k-armed-Bandit
Problem verglichen.

\paragraph{$\epsilon$ greedy / decaying $\epsilon$ greedy}
Eine Strategie, die zu jedem Zeitpunkt die Action wählt, die nach derzeitigem Wissen den
höchsten value hat, wird nach R. Sutton \cite[~S.20]{SB1998} $greedy$ genannt.
$\epsilon$-greedy-Strategien sind greedy Strategien, die mit einer gewissen
Wahrscheinlichkeit $\epsilon$ eine zufällige Action wählen. Sie wählen also mit einer
Wahrscheinlichkeit von $1 - \epsilon$ die Action, von der sie sich den höchsten Reward
versprechen und andernfalls eine zufällige Action.

In der Praxis werden häufig decaying-$\epsilon$-greedy-Strategien verwendet, bei denen der
$\epsilon$-Wert über den Trainingsprozess hinweg verringert wird. Dadurch werden zu Beginn
des Trainings viele neue Actions ausprobiert, während gegen Ende das Anwenden des
Gelernten stärker in den Vordergrund tritt.

\paragraph{Optimistic Initial Values}
Ein Agent, der ein k-armed-Bandit Problem lösen soll, besitzt eine interne Abschätzung der
values, um Actions wählen zu können, die einen besseren Reward versprechen. Die
Bewertung der Actions wird beeinflusst durch die erfahrenen Rewards nach dem Wählen einer
Action. Beispielsweise können die erfahrenen Rewards einer Action gemittelt werden, um
zukünftige Rewards dieser Action abschätzen zu können.

Neben $\epsilon$-greedy Verfahren ist es eine weitere Möglichkeit Exploration umzusetzen,
die initiale Bewertung der values sehr optimistisch vorzunehmen \cite[~S.26]{SB1998}.
Dadurch wird der Agent bei der Wahl einer Action vom erhaltenen Reward ''enttäuscht''
sodass sich die Einschätzung dieser Action verschlechtert. Wird nun immer die Action
gewählt, die am besten eingeschätzt ist, so werden über die Zeit alle Actions versucht,
bis sie sich dem tatsächlichen Erwartungswert nähern.

\paragraph{Upper Confidence Bound Action Selection}
Während $\epsilon$-greedy-Methoden zufällig zwischen allen nicht greedy Actions
auswählen, wählt die Upper Confidence Bound Ac\-tion Selection Methode Actions entsprechend
dreier Parameter aus.
Der erste Parameter ist $t$ die Anzahl der Actions, die bis jetzt gespielt wurden. Der
zweite ist der bis jetzt ermittelte $value$ der Action $a$ bezeichnet mit $Q(a)$ und der
dritte Parameter ist die Anzahl, wie oft Action $a$ schon ausprobiert wurde. Die nächste
zu spielende Action wird dann nach folgender Gleichung gewählt \cite[~S.27]{SB1998}:

\begin{align}
  A = \argmax_a\left( Q(a) + c*\sqrt\frac{\ln(t)}{N(a)} \right)
\end{align}

$A$ ist die gewählte Action. $N(a)$ ist die Anzahl, wie oft die Action $a$ schon gewählt
wurde. Existiert eine Action, die noch nie gespielt wurde, also $N(a)=0$, so wird eine der
Actions gespielt, die noch nicht gespielt wurden. Durch den Term
$c*\sqrt\frac{\ln(t)}{N(a)}$ wird sichergestellt, dass selten benutzte Actions eine höhere
Bewertung erhalten und damit wahrscheinlicher ausprobiert werden. Über den Hyperparameter
$c$ lässt sich bestimmen, wie stark selten benutzte Actions ausprobiert werden sollen.

Ein Vorteil gegenüber $\epsilon$-greedy Strategien ist, dass durch die Einbeziehung von
$Q(a)$ Actions häufiger ausprobiert werden, die besser erscheinen, während Actions, die
sehr schlecht wirkende Actions weniger oft ausprobiert werden.

\subsubsection{Implementierung}
Das k-armed-Bandit Problem sowie verschiedene Varianten der Action-Selection wurden
implementiert und sind unter \url{https://github.com/Bluemi/rl_testbed} abrufbar.

In der Klasse \lstinline!NArmedBandit! wird das k-armed-Bandit Problem umgesetzt. Es
werden zu Beginn die values der einzelnen Actions aus einer Standardnormalverteilung
gezogen und gespeichert. Wird nun eine Action auf dem erstellten Bandit gespielt, so wird
eine Zufallszahl aus einer Normalverteilung mit dem Erwartungswert der jeweiligen Action
gezogen und als Reward zurück gegeben.
Um auf einem Bandit zu spielen wurde ein \lstinline!Solver! umgesetzt, der eine interne
Abschät\-zung der Action values ermittelt, indem die erfahrenen Rewards dieser Action
gemittelt werden. Der Solver wählt Actions wobei unterschiedliche Exploration Strategien
ausprobiert werden können. Die Unterklasse \lstinline!EpsilonGreedySolver! implementiert
beispielsweise die $\epsilon$-greedy Strategie.

In \lstinline!src/main.py! werden unterschiedliche Solver anhand des gleichen Bandits
getestet und die Ergebnisse geplottet. Jeder Solver trainiert 5000 Actions auf einem
10-armed-Bandit. Die dabei resultierenden Rewards sind in Abbildung
\ref{fig:karmed_bandit} graphisch dargestellt. Die zu sehenden Rewards wurden über 2000
Experimente gemittelt, um ein anschaulicheres Ergebnis zu erhalten.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.3]{res/k_armed_bandit_epsilon.png}
\includegraphics[scale=0.3]{res/k_armed_bandit_optimistic_upper.png}
\caption{Ergebnisse 10-armed-Bandit}
\label{fig:karmed_bandit}
\end{figure}

\noindent
Auf der linken Seite werden 3 unterschiedliche $\epsilon$ greedy Verfahren verglichen.
Erfolgt ein Vergleich zwischen den Verfahren ohne decay (blau und orange), so kann man
erkennen, dass $\epsilon = 0.1$ zu Beginn besser funktioniert, da mehr Zeit für
Exploration verwendet wird und so schneller bessere Actions gefunden werden. Dafür
funktioniert $\epsilon = 0.01$ nach circa 1500 Action Selections besser, da mehr Zeit
verwendet wird, um die schon gefundenen guten Actions zu exploiten.

Um die Vorteile beider Möglichkeiten zu nutzen, verwendet das dritte Experiment einen
decay, um $\epsilon$ auf $0$ zu senken. Dadurch werden zu Beginn viele zufällige Actions
ausgewählt, um die values der einzelnen Actions besser einzuschätzen. Wenn durch den decay
$\epsilon$ gegen Ende des Experiments sinkt, werden die gefundenen Actions benutzt und ein
vergleichsweise hoher Reward entsteht.

In der rechten Abbildung sind das Verfahren Optimistic Initial Values und Upper Confidence
Bound Action Selection zu sehen. Zusätzlich ist der $\epsilon$-greedy Solver mit
$\epsilon=0.01$ aus der ersten Abbildung zum Vergleich eingezeichnet. Die obere schwarz
gestrichelte Linie deutet den value der optimalen Action an.

Tabelle \ref{tab:explorationstrategies} zeigt die erreichten durchschnittlichen Rewards
der unterschiedlichen Verfahren.

\begin{table}[ht]
  \begin{center}
    \begin{tabular}{l | l}
      \textbf{Verfahren} & \textbf{Mean Reward} \\
      \hline
      Optimal & $1.579$\\
      Upper Confidence Bound & $1.556$ \\
      $\epsilon$-greedy with decay ($\epsilon=0.01$) & $1.523$ \\
      Optimistic Initial Values & $1.520$ \\
      $\epsilon$-greedy ($\epsilon=0.01$) & $1.439$ \\
      $\epsilon$-greedy ($\epsilon=0.1$) & $1.437$ \\
    \end{tabular}

    \caption[ExplorationStrategies]{Vergleich unterschiedlicher Exploration Strategien}
    \label{tab:explorationstrategies}
  \end{center}
\end{table}

\noindent
Im gezeigten Experiment erhielten die einfachen Versionen der $\epsilon$-greedy Solver die
kleinsten Rewards. Deutlich besser funktionierte die erweiterte Version mit decay, die
vergleichbar mit dem Verfahren Optimistic Initial Values ist. Am besten schneidet die
Upper Bound Confidence Action Selection ab.


\newpage
\subsection{Open AI Gym}
Die Python Bibliothek \textit{gym}, welche von der Firma Open AI umgesetzt wurde umfasst ein
Toolkit, welches zur Entwicklung und zum Vergleich von Reinforcement Learning Algorithmen
eingesetzt werden kann. Das Toolkit definiert keine Strukturen für die Umsetzung der Agenten,
wodurch die Wahl der Machine Learning Bibliothek dem Entwickler frei zur Verfügung steht. In
der gym Bibliothek wird hingegen eine Sammlung an Testumgebungen (Environments) zur Nutzung
bereitgestellt, welche alle eine festgelegte Problemstellung definieren \cite{OAI2016}. 
Diese Environments bieten eine einheitliche Schnittstelle, was das Implementieren von
generellen Algorithmen für nahezu jede Art von Environment ermöglicht. Da Reinforcement
Learning Verfahren in komplexen Problemstellungen immer bessere Resultate erzielten, jedoch
keine standartisierte Umsetzung für die Entwicklung und anschließende Nutzung von Environments
und somit auch keine Möglichkeit für ein Benchmarking verschiedener Verfahren bestand,
beschloss Open AI mit der gym Bibliothek genau diesen Problemen entgegenzuwirken
\cite{OAI2016}.

Nachfolgend soll der Aufbau und die Nutzung eines Environments durch die Bibliothek gym
erläutert werden und es wird auf eine Referenzimplementierung eines Environments für das
Spiel Tic-Tac-Toe eingegangen.


\subsubsection{Konzept}
Ein Environment ist eine von der Klasse gym.Env erbende Unterklasse, welche durch die
Hauptprogrammierschnittstelle des Toolkits die folgenden Methoden definiert.

\begin{itemize}
\itemsep-6pt
\item \textit{step}
\item \textit{reset}
\item \textit{render}
\item \textit{close}
\item \textit{seed}
\end{itemize}  

Mittels der \textbf{step(action)} Methode kann eine Action für einen timestamp im
Environment ausgeführt werden. Diese Action kann sowohl diskrete Werte als auch 
kontinuierliche Werte repräsentieren. Nachdem die Action im Environment ausgeführt wurde,
wird die für den Agent bereitgestellte aktuelle \textit{Observation} des Environments
aufgezeichnet. Des Weiteren wird ein, durch die Action initiierter und durch den Wechsel
in den neuen Zustand erhaltener \textit{Reward} für den Agenten ermittelt sowie ein 
\textit{done} Flag, welches Auskunft darüber gibt, ob die aktuelle Episode beendet wurde.
Weitere Informationen werden im \textit{info} Dictionary gesammelt und dienen für optionale
Analyseinformationen. Das Tuple \textit{observation, reward, done, info} wird anschließend
an den Aufrufenden zurückgegeben. \autoref{lst:step-method} veranschaulicht eine
Beispielimplementierung der \textit{step} Methode gegen die von Open AI bereitgestellte API.

\begin{lstlisting}[language=Python, caption=step method, label=lst:step-method]
def step(self, action):
	# executes the action in the environment and validates it
	valid = self._take_action(action)
	
	if not valid:
		print("invalid action: player ", self._current_player)
	
	# calculates the obtained reward
	reward = self._get_reward()
	
	# gathers current observation of the environment
	observation = (self._player_0, self._player_1)
	
	# information if the episode is finished
	is_done = self._check_if_done(reward)
    
	return observation, reward, is_done or not valid, {}
\end{lstlisting}
\noindent
Die Funktionen \textit{\_take\_action(action)}, \textit{\_get\_reward()} und
\textit{\_check\_if\_done(reward)} setzen die Kommunikation mit dem Environment um. In
diesem Bespiel besteht die Observation ausschließlich aus den Zuständen der beiden Spieler.
Nach der Änderung der Systemdynamik und dem Erfassen der Observation wird das bereits
beschriebene Tupel an Umgebungsinformationen zurückgeliefert.

Die Methode \textbf{reset()} dient zum vollständigen Zurücksetzen des Environments und aller
darin gespeicherten Zustände. Anschließend gibt die Methode den Ausgangszustand als Observation
an den Aufrufenden zurück. Reset kommt zum Einsatz, sobald eine Episode abgeschlossen wurde
(done Flag) damit erneut Actions darin ausgeführt werden können.
\newline
\noindent
Mit \textbf{render(mode='human')} kann der zwischenzeitliche Zustand des Environments
visualisiert werden. Dabei legt die gym Bibliothek keine explizit zu nutzenden Renderverfahren
fest, per Konvention sind aber folgende Verfahren definiert: 

\begin{itemize}
\itemsep0pt
\item \textit{human} - für Menschen veranschaulicht, 
\item \textit{rgb\_array} - ein numpy.ndarray mit RGB Pixelwerten, um dieses in einem Bild oder einem 
Video zu visualisieren und 
\item \textit{ansi} - als terminal-style Textrepräsentation.
\end{itemize}
\noindent
Abschließend kann durch \textbf{close()} das Environment und somit gegebenenfalls die 
dahinterliegende Anwendung geschlossen werden. Durch den Aufruf von 
\textbf{seed(seed)} kann ein Seed für den \textit{random number generator}
des Environments festgelegt werden.


Neben den beschriebenen Methoden exisiteren die Properties \textit{action\_space}, welcher die
Art und Anzahl der zu wählenden Actions vorgibt (z.B. \textit{Discrete(9)} für 9 verschiedene
diskrete Actions), \textit{observation\_space}, welcher gleiches für die Observation des
Environments angibt und \textit{reward\_range}, welcher einen Wertebereich für den zu erhaltenen
lokalen Reward definiert.

Für das Spiel Tic-Tac-Toe wurde ein Einspieler Environment implementiert, welches im
Unterverzeichnis \textit{preparation/05\_v\_learning/} unter dem nachfolgenden Link abgerufen
werden kann: \url{github.com/dephiloper/independent-coursework-rl/}


\subsubsection{Verwendung}
Das nachfolgende Listing \autoref{lst:env-usage} veranschaulicht die Nutzung eines von Open AI
bereitgestellten Environments mit der Bezeichnung CartPole-v0. Initial wird das Environment
mit \textit{gym.make('CartPole-v0')} erzeugt und daraufhin mit \textit{reset()} vollständig
zurückgesetzt, sodass alle internen Zustände wieder in die Ausgangssituation gebracht werden. 
Anschließend werden in 1000 Frames das Environment mit \textit{render()} visualisiert und es
wird eine zufällige Action aus dem action\_space des Environments gesampled und in der
\textit{step(action)} Methode ausgeführt. Durch das Ausüben der Action wird eine neue
Observation mit zugehörigem Reward vom Environment generiert und anschließend zurückgegeben.
Nach 1000 Iterationen wird das Environment geschlossen. 
\begin{lstlisting}[language=Python, caption=environment usage, label=lst:env-usage]
import gym
env = gym.make('CartPole-v0')
env.reset()
for _ in range(1000):
    env.render()
    # take a random action, gather observation and reward
    obs, reward, done, _ = env.step(env.action_space.sample())
env.close()
\end{lstlisting}

Durch den definierten Aufbau der bereitgestellten API bietet ein Environment in den meisten Fällen
die Möglichkeit Agenten mit verschiedenen Verfahren zur Ermittelung der zu tätigenden Action zu
implementieren, sodass diese nach dem Aufsetzen einer Grundstruktur auch gut ausgetauscht werden
können.


\newpage
\section{Cross-Entropy-Method}
\label{sec:cross-entropy-method}
In den nachfolgenden Kapiteln werden diverse Algorithmen in Bereich Reinforcement Learning
erläutert. Das erste zu beleuchtende Reinforcement Learning Verfahren ist die
Cross-Entropy-Method. Die Cross-Entropy-Method ist eine Monte Carlo Methode für
Stichprobenentnahme nach Wichtigkeit (importance sampling) und Optimierung (optimization)
\cite[~S.29 ff.]{R2004}. Im folgenden Kapitel wird das Verfahren erläutert und in die bereits
beschriebene Taxonomie eingeordnet. Des Weiteren erfolgt die Anwendung des Verfahrens innerhalb
von zwei Testumgebungen. 

\subsection{Einordnung}
Bei dem Verfahren handelt es sich um eine on-policy Methode, da im Trainingsprozess ausschließlich
mittels neu aufgezeichneter Trainingsdaten gelernt werden kann. Prinzipiell hat das Verfahren das
Hauptziel, den maximalen Reward innerhalb einer Episode anzusammeln. Um dieses Ziel zu erreichen
orientiert es sich an klassischen Machine Learning Verfahren und führt eine nichtlineare trainierbare
Funktion ein, die als Input die Observation des Agenten erhält und diesen auf einen Output abbildet.
Durch diese Charakteristik fällt die Cross-Entropy-Method in die Familie der policy learning Verfahren
(policy-based), da mittels neuronalen Netz eine Policy erzeugt wird, welche für jede Observation die
zu wählende Action in Form einer Wahrscheinlichkeitsverteilung über alle Actions ausgibt, sprich es
wird eine Observation über direktem Wege auf eine zu wählende Action abgebildet. Die
Methode ist nicht value-based, da der zu erwartende Reward nicht approximiert wird.

\subsection{Verfahren}
Die zu lernende Policy ist wie bereits erwähnt als Wahrscheinlichkeitsverteilung über die 
zu wählenden Actions repräsentiert und dadurch sehr ähnlich zu einem Klassifizierungsproblem,
wobei die Anzahl der Klassen hier der Anzahl von auszuführenden Actions entspricht. Durch
diese Verfahrensweise ist die Umsetzung des Agenten als eher einfach zu betrachten, da
diese ausschließlich die Observation des Environments an das Netz weitergeben muss, von
dem Netz anschließend die Wahrscheinlichkeitsverteilung erhält und anhand dieser dann eine
Zufallsstichprobe (random sampling) unter Berücksichtigung der Wahrscheinlichkeiten aller
Actions vollzieht \cite[~S.78]{L2018}. Nachdem der Agent durch das random sampling eine
Action gewählt hat, führt er diese erneut im Environment aus und erhält die nächste
Observation sowie den Reward für die zuvor ausgeführte Action.

Diese Schritte werden anschließend solange wiederholt, bis eine Ansammlung an Episoden (siehe 
\autoref{sec:markov_reward_process}: \nameref{sec:markov_reward_process}), dass heißt eine Folge
von Observations, Actions und Rewards, gesammelt wurde. Innerhalb jeder Episode wird nun der
insgesamt erwirtschaftete Reward, der \textit{total eward}, berechnet. Dadurch kann bewertet werden
wie gut eine Episode im Vergleich zu anderen Episoden ausgefallen ist. Aufgrund der Zufälligkeit
bei der Selektion von Actions kommt es dazu, dass einige Episoden besser ausfallen als andere.
Die Cross-Entropy-Method macht sich dieses Verhalten zu nutze, verwirft schlechtere Episoden und
trainiert ausschließlichen auf besseren Episoden. Das Kernprinzip des Verfahrens wird wie nachfolgend
aufgelistet in drei Schritte untergliedert \cite[~S.80 f.]{L2018}:

Initial wird eine definierte Anzahl $N$ an Episoden mit dem aktuellen Model im Environment gespielt
oder auch wahrgenommen. Anschließend erfolgt die Berechnung des total Rewards für jede gespielte
Episode. Zusätzlich wird in diesem Schritt eine Obergrenze für den Reward festgelegt 
(\textit{reward boundary}). Daraus folgt, dass die besten 30 Prozent der Episoden behalten werden,
wohingegen die restlichten schlechteren Episoden verworfen werden. Anschließend wird im letzen Schritt
das Model auf den übrigen \textit{elite} Episoden trainiert, wobei eine Observation als Input in das
Netz gegeben wird und die zuvor gewählten Actions als gewünschter Output vom Netzes gefordert werden.
Während des Trainingsprozesses werden diese Schritte solange wiederholt bis ein gewünschtes
Ergebnis erzielt wurde. In \autoref{fig:cross_entropy_procedure} werden die beschriebenen Schritte
zusätzlich visuell dargestellt:

\begin{figure}[htp]
\centering
\includegraphics[scale=0.45]{res/cross-entropy-procedure.png}
\caption{Cross-Entropy Verfahren}
\label{fig:cross_entropy_procedure}
\end{figure}

\subsection{CartPole}
Das CartPole Environment beschreibt ein eher einfach einzustufendes Environment, welches durch seine
simple Nutzung gerne als Einstiegspunkt für die Entwicklung von Reinforcement Learning Algorithmen 
dient. Das Environment besteht aus einem Stab (pole), welcher über ein Gelenk an einem Wagen (cart) 
befestigt ist \cite{OAI2016_2}. Der Agent kann den Wagen durch das Aufbringen einer horizontalen Kraft 
von -1 oder +1 steuern, wobei sich der Wagen dementsprechend nach links oder rechts bewegt. Nach 
dem Zurücksetzen des Environments startet der Stab in einer vertikalen Ausrichtung, nahezu orthogonal
zentriert auf dem Wagen. Das zu erreichende Ziel für den Agenten ist es den Stab vom Umkippen zu hindern,
wobei der Stab als umgekippt zählt sobald dessen Ausrichtung mehr als 12 Grad von einem genauen
senkrechten Winkel abweicht. Des Weiteren zählt eine Episode als beendet, wenn sich der Wagen mehr
als 2.4 Units vom Ausgangspunkt wegbewegt. Das Environment liefert dem Agenten einen Reward von +1
für jeden Frame in dem der Stab und der Wagen die beschriebenen Anforderungen erfüllen. Des Weiteren
erhält der Agent als Observation die aktuelle Position und Geschwindigkeit des Wagens, sowie den Winkel
der Ausrichtung des Stabs und die Geschwindigkeit die auf die Spitze des Stabs wirkt.

In dem Toolkit von Open AI existieren diverse Versionen von CartPole, wobei sich die eben
erläuterte Beschreibung auf das Environment CartPole-v0 bezieht, des von Barto, Sutton und
Anderson beschriebenen Cart-pole Problems \cite[~S.838 f.]{BSA1983}. Erreicht der Agent einen
durchschnittlichen Reward von mindestens 195,0 über 100 aufeinanderfolgende Versuche hinweg gilt das
Problem als gelöst. \autoref{fig:cart-pole} veranschaulicht die beschriebene Problemstellung aus dem 
Paper\cite{BSA1983} von Barto, Sutton und Anderson:

\begin{figure}[htp]
\centering
\includegraphics[scale=0.15]{res/cart-pole.png}
\caption{Illustration des Cart-pole Problems}
\source{\cite[~S.838]{BSA1983}}
\label{fig:cart-pole}
\end{figure}

\paragraph*{Evaluierung}
\noindent
\newline
Das Cross-Entropy-Verfahren wurde im CartPole Environment mittels neuronalen Netz mit
einem Hidden Layer von 128 Nodes und einer Learning Rate von 0.01 trainiert. Als
Optimizer wurde Adam verwendet und als Aktivierungsfunktion zwischen Input Layer und
Hidden Layer die ReLu Funktion. Zur Berechnung eines Loss zwischen Vorhersage des Netzes
und den während des Spielens aufgenommenen Actions wird der Cross-Entropy-Loss verwendet.

Die nachfolgenden Graphen in \autoref{fig:cross-entropy-graph} veranschaulichen den
\textit{loss}, den stetig wachsenden \textit{reward\_bound} (Reward Obergrenze) und den
durchschnittlich erhaltenen Reward beim Spielen einer Episode (\textit{reward\_mean}).
Das Model des orange dargestellten Graphs verwendet bei der Auswahl der elite Episoden
einen Perzentil von 70, das blau dargestellte Model einen Perzentil von 80 und das
rot dargestellte Model einen Perzentil von 90. Ein Perzentil definiert eine Menge von
Werten einer Verteilung die unter oder gleich einem festgelegten Wert sind (z.B. Median 
= 50\%-Perzentil). 

\begin{figure}[htp]
\centering
\includegraphics[scale=0.385]{res/cross-entropy_loss.png}
\includegraphics[scale=0.385]{res/cross-entropy_reward-bound.png}
\includegraphics[scale=0.385]{res/cross-entropy_reward-mean.png}
\caption{Ergebnisse Cross-Entropy-Method Cart-Pole-v0}
\label{fig:cross-entropy-graph}
\end{figure}

Die Implementierung der Cross-Entropy-Method zum Cart-pole Problem ist unter
\url{github.com/dephiloper/independent-coursework-rl} im Unterverzeichnis 
\textit{preparation/04\_cross\_entropy} in der Datei
\href{https://github.com/dephiloper/independent-coursework-rl/blob/master/preparation/04_cross_entropy/cart_pole_torch.py}{\nolinkurl{cart\_pole\_torch.py}} umgesetzt.


\subsection{FrozenLake}
\label{sec:frozen-lake}
Bei dem zweiten Environment FrozenLake handelt es sich um ein Environment mit diskretem action-
und diskretem observation-space, das heißt, dass sowohl die zu tätigende Action diskret ist,
aber auch die Observation als diskreter Zahlenwert von 0-15 wahrgenommen wird. Der Zahlenwert
der Observation repräsentiert dabei ausschließlich die Position des Agenten. Das Ziel ist es
einen Avatar vom Startpunkt in der linken oberen Ecke eines Spielfeldes auf die untere rechte
Seite zum Ziel zu steuern. Auf dem Weg zum Ziel befinden sich Löcher, die das Spiel beenden, wenn
sich der Avatar auf diese bewegt. Das Spielfeld ist in Abbildung \ref{fig:frozen_lake} zu sehen.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.25]{res/frozen_lake.png}
\caption{FrozenLake Spielfeld}
\source{\cite[~S.90 - Chapter 4 - Figure 5]{L2018}}
\label{fig:frozen_lake}
\end{figure}

\noindent
Der Agent kann die Actions ''left'', ''up'', ''right'' und ''down'' ausführen.
Erschwert wird die Steuerung aber dadurch, dass die Oberfläche rutschig ist, weshalb
die Folgeposition des Avatars nicht deterministisch ist. Wählt man bespielsweise die
Action ''down'', so bewegt sich der Avatar mit einer Wahrscheinlichkeit von jeweils
$1/3$ nach unten, rechts oder links. Allgemein gesprochen ist die Richtung, in die
sich der Avatar nach der Wahl einer Action bewegt, eine zufällige Richtung, wobei
jedoch nie die entgegengesetzte Action der Wahl vollzogen wird.


\paragraph*{Evaluierung}
\label{sec:cross_entropy_results}
\noindent
\newline
Anders als das CartPole Environment erhält der Agent bei dem FrozenLake Environment 
ausschließlich einen +1 Reward, sobald der gesteuerte Avatar das Ziel erreicht. Daraus folgt,
dass anhand des \textit{total Reward} einer Episode nicht darauf geschlossen werden kann,
wie gut eine Episode gespielt wurde (Avatar kann lange umherwandern bevor er auf das Ziel kommt).
Somit lässt sich auch keine sinnvolle Wahl von elite Episoden treffen, da ausschließlich
durchgespielte Episoden mit einem total Reward von +1 oder 0 bestehen und die Episoden mit 0
deutlich häufiger auftreten.

Aufgrund dieser Probleme werden folgende Einschränkungen der Cross-Entropy-Method 
aufgeführt \cite[~S.92 f.]{L2018}:
\begin{itemize}
\itemsep0pt
\item endliche und kurze Episoden notwendig
\item detailierte Unterscheidung des total Reward zwischen guten und schlechten Episoden
\item Reward für die Erfüllung von Zwischenschritten
  \label{itm:cross_entropy_limits}
\end{itemize}

Diese Einschränkungen können jedoch addressiert werden, indem mehr Episoden gespielt werden, 
um somit die Wahrscheinlichkeit zu erhöhen erfolgreiche Episoden zu erhalten. Des Weiteren
muss der discount factor $\gamma$ auf die Berechnung des total Reward angewandt werden, damit
kürzere Episoden einen höheren total Reward erzielen, als lange Episoden. Abschließend ist es
sinnvoll elite Episoden für mehrere Iterationen zu behalten damit auf diesen mehrmals
trainiert werden kann \cite[~S.93]{L2018}.  

Die nachfolgenden Graphen veranschaulichen die Anwendung der Cross-Entropy-Method ohne (grün)
und mit Anpassungen (grau) zur Erzielung besserer Resultate. Durch die Erweiterung des
Verfahrens kann eine Verbesserung erzielt werden, sodass der Trainingsprozess bei einer
etwa 50\% erfolgreichen Lösung des Environments stagniert.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.485]{res/frozen-lake_reward-mean_bad.png}
\includegraphics[scale=0.485]{res/frozen-lake_reward-mean.png}
\caption{Ergebnisse Cross-Entropy-Method FrozenLake-v0}
\label{fig:frozen-lake_cross-entropy}
\end{figure}

Die Implementierung der Cross-Entropy-Method zum FrozenLake Environment ist unter
\url{github.com/dephiloper/independent-coursework-rl} im Unterverzeichnis 
\textit{preparation/04\_cross\_entropy} in der Datei 
\href{https://github.com/dephiloper/independent-coursework-rl/blob/master/preparation/04_cross_entropy/frozen_lake_torch.py}{\nolinkurl{frozen\_lake\_torch.py}} umgesetzt.
\newpage


\section{Q-learning}
In diesem Abschnitt wird das Verfahren Q-learning erläutert. Anschließend wird die Methode
an zwei praktischen Anwendungen evaluiert. Einerseits in der Testumgebung FrozenLake
(siehe \autoref{sec:frozen-lake}) und zum Anderen in einer selbstständig entwickelten
Tic-Tac-Toe Anwendung, in der das Verfahren anhand eines zufällig spielenden Gegner
validiert wird.


\subsection{Einordnung}
Das Q-learning Verfahren gehören zur Familie der value learning Verfahren, die
unterschiedlichen Actions unterschiedliche values zuordnen, um daraufhin diejenigen Actions
wählen zu können, die den höchsten value aufweisen. Der value einer Action ist dabei der
zu erwartende Reward, welcher vom Environment zurückgegeben wird, wenn man die gewählte
Action im wahrgenommenen State ausführt.

Weiterhin gehören Q-learning Verfahren zu den off-policy Verfahren, da auch ältere
Trainingsdaten (Erfahrungen die in der Vergangenheit aufgezeichnet wurden) benutzt
werden können. Die Q-learning Methoden, die nachfolgend beschrieben werden sind model-free,
was bedeutet, dass nicht versucht wird die nächsten States oder die folgenden Rewards
vorherzusagen.

Wie oben bereits beschrieben, basieren Q-learning Verfahren auf einer Abschätzung des
values eines States oder einer Action. Die Abschätzung des values einer Action $a$ in
einem State $s$ wird als $Q(s, a)$ bezeichnet. Dieser value kann auf unterschiedliche
Weise ermittelt werden.

Eine erste Variante von Q-learning benutzt Tabellen, in denen die Abschätzungen
für unterschiedliche States oder Actions gespeichert werden. Dieses Verfahren wird nach
\cite[~S.193]{L2018} auch als ''Tabular Q-learing'' bezeichnet.


\subsection{Tabular Q-learning}
\label{sec:tab-q-learning}
In diesem Abschnitt wird auf das Tabular Q-learning eingegangen, wie es von Lapan
\cite[~S.193]{L2018} vorgestellt wird. Um die Ver\-ständ\-lich\-keit zu verbessern wird
das Verfahren anhand des FrozenLake Environments veranschaulicht.

% TODO Bruno citation needed
\subsubsection{Value iteration Verfahren}
Da sich das Spielfeld von FrozenLake nicht verändert, beschränken sich die Zustände des
Environments auf die Positionen des Spielers. Durch die geringe Anzahl an Spielzuständen
bietet sich das Tabular Q-learning Verfahren an, da es möglich ist, alle erreichbaren
Zustände in einer Tabelle zu speichern und deren values zu bestimmen. Diese Tabelle
enthält für jeden State eine Abschätzung der unterschiedlichen Actions.
Tabelle \ref{tab:q_table} zeigt eine sehr kleine beispielhafte Q-table mit zwei Actions
und zwei States.

\begin{table}[ht]
  \begin{center}
    \begin{tabular}{c | c | c}
       & $a_0$ & $a_1$ \\
      \hline
      $s_0$ & $Q(s_0, a_0)$ &$ Q(s_0, a_1)$ \\
      \hline
      $s_1$ & $Q(s_1, a_0)$ & $Q(s_1, a_1)$ \\
    \end{tabular}

    \caption[Q-table]{Q-table}
    \label{tab:q_table}
  \end{center}
\end{table}

\noindent
Das Ziel des Verfahren ist es die Q-values der Tabelle zu ermitteln. Dazu werden die
Q-values mit Null initialisiert, um diese anschließend iterativ zu bestimmen. Das Verfahren 
sieht vor, dass zufällige Actionen ausgewählt und in einem Environment ausprobiert werden.

Daraus resultieren ein State, eine Action, ein Reward und ein Folgestate, auch SARS-Tupel
genannt. Der erste State $s$ ist der Ausgangszustand, in dem man die zufällig gewählte
Action $a$ getätigt hat. Als Folge auf das Ausführen dieser Action erhält man einen
unmittelbaren Reward $r$, der vom Environment bestimmt wird, sowie den Folgestate $s'$,
also den State, den das Environment nach Ausführung von $a$ erreicht hat.

Ist das SARS-Tupel gefunden, werden die Q-values der Tabelle angepasst, wobei der folgende
Zusammenhang, der aus der Bellman Equation hervorgeht, verwendet wird
\cite[~S.193 ff.]{L2018}.
\begin{align}
  Q(s, a) \leftarrow r_{s,a} + \gamma \max_{a' \in A}Q(s', a')
  \label{aln:QValues}
\end{align}
\noindent
$Q(s, a)$ ist der value der Action $a$ im wahrgenommenen State $s$, also der long-term
Reward, der erwartet wird, führt man in State $s$ die Action $a$ aus. Das Ziel ist es
$Q(s, a)$ so anzupassen, dass es sich dem tatsächlich zu erwartenden long-term Reward
annähert. Der unmittelbare Reward $r_{s,a}$ kann dem SARS-Tupel entnommen werden, ebenso
wie der Folgestate $s'$.

Befindet sich der Agent im State $s'$, so ist der maximal zu erwartende Reward, der 
zu erwartende Reward der besten Action im State $s'$. Das bedeutet, dass der Term
$\max_{a'\in A} Q(s', a')$ der erwartete long-term Reward der besten Action des
Folgezustandes, also auch der bestmögliche long-term Reward, den man in State $s'$
erwarten kann, ist. Nach (\ref{aln:QValues}) gleicht $Q(s, a)$ dem unmittelbaren Reward
$r_{s,a}$ addiert mit dem besten zu erwartenden long-term Reward des Folgezustandes. Der
Wert des Folgezustandes wird dabei mit dem discount factor $\gamma$ multipliziert, wobei 
$\gamma$ standardmäßig zwischen $0.9$ und $0.99$ gewählt wird.

Mit \autoref{aln:QValues} wird ein neuer Wert für $Q(s, a)$ ermittelt, sprich es wird ein
neuer Wert in die Q-table eingetragen. Da die Wirkung einer Action in FrozenLake nicht
deterministisch ist und auch im Allgemeinen nicht davon ausgegangen werden kann, ist es
nicht ausreichend den Wert der Q-table gleich dem ermittelten Wert zu setzen. Stattdessen
wird der Wert der Tabelle in Richtung des ermittelten Wertes angepasst. Dies geschieht durch 
die in \autoref{aln:QValueAdapt} dargestellte Zuweisung.

\begin{align}
  Q(s, a) \leftarrow (1 - \alpha)Q(s, a) +
  \alpha(r_{s, a} + \gamma \max_{a'\in A}Q(s', a'))
\label{aln:QValueAdapt}
\end{align}
\noindent

Wobei der Hyperparameter $\alpha$ zwischen 0 und 1 gesetzt wird, um den Wert $Q(s, a)$ bei
jeder Iteration schrittweise zu überblenden. Dadurch nähern sich die Q-values der Tabelle
den tatsächlichen Q-values an, wodurch sich die Problemstellung des Environments erfolgreich 
bewältigen lässt.

\subsubsection{Evaluierung}
Um zu testen, wie gut die ermittelten Q-values funktionieren, wird wie folgt vorgegangen.
Ein neues Environment wird initialisiert und der Agent wählt in jedem erfahrenen State die
Action, die den höchsten Q-value besitzt.

Das vorgestellte Experiment wird als gelöst angesehen, wenn 80\% von 20 Spielen erfolgreich
beendet wurden. Dieser Aufbau benötigt für 4x4 FrozenLake zwischen 6000 und 42000
Iterationen, was zwischen einer und sechs Sekunden benötigt.
\autoref{fig:frozen-lake_q_learning} visualisiert den typischen Trainingsablauf, wobei der
durchschnittliche Reward aus 20 Spielen gezeigt wird. Die unterschiedlichen Graphen zeigen
unterschiedliche Abläufe des gleichen Experiments. Weiterhin sind die Daten geglättet, um
ein anschaulicheres Ergebnis zu erhalten. Transparent im Hintergrund sind die nicht
geglätteten Daten zu erkennen, welche jeweils einen Höchstwert von 0.85 aufweisen nachdem
das Experiment als gelöst angesehen wurde.
\begin{figure}[htp]
\centering
\includegraphics[scale=0.7]{res/frozen_lake_reward_tabular_q.png}
\caption{Ergebnisse Tabular Q-learning FrozenLake-v0}
\label{fig:frozen-lake_q_learning}
\end{figure}

\subsection{Tic-Tac-Toe}
Neben der Umsetzung von FrozenLake wurde ebenfalls eine Tic-Tac-Toe Implementation
entwickelt, in der ein Agent gegen einen zufällig spielenden Gegner trainiert und
anschließend getestet wurde. Unter \url{https://github.com/Bluemi/tictactoe_rl} ist
der Quelltext einsehbar. Das umgesetzte Verfahren ist eine Form der value-based
Verfahren unterscheidet sich aber von Q-learning.

Ebenfalls wurde ein Tic-Tac-Toe Environment entwickelt, das das Interface eines OpenAI
Gyms implementiert. Dieses kann unter
\url{https://github.com/dephiloper/independent-coursework-rl} in
\lstinline!preparation/05_v_learning/gym_tictactoe.py! aufgerufen werden.

Beim Training des Agent werden wie auch in Q-learning values ermittelt. Im Gegensatz zu
der im letzten Kapitel gezeigten Q-learning Methode werden in diesem Verfahren allerdings 
die values unterschiedlicher States und nicht die values der Actions ermittelt. Die zu
ermittelnde Funktion ist nicht mehr $Q(s, a)$, sondern $V(s)$. Das Verfahren setzt dafür
voraus, dass aus einem gegebenen State und einer gegebenen Action der Folgezustand berechnet
werden kann, was durch das deterministische Spiel Tic-Tac-Toe gegeben ist.

Die Approximationen der values der States werden in einem Dictionary gespeichert und mit
dem Startwert 0 initialisiert. Anschließend wird ein Spiel gegen einen zufällig spielenden
Gegner gespielt, wobei alle erreichten States zwischengespeichert werden. Am Ende des
Spieles wird ein Reward vergeben, der davon abhängt, ob eine Partie gewonnen oder verloren
wurde.

Daraufhin werden alle $V(s)$ Werte der zwischengespeicherten States in Richtung des
erhaltenen Rewards verändert. Um zu bestimmen wie stark sich der erhaltene Reward auf
$V(s)$ auswirkt wird ein $step\_size$ Parameter verwendet, der wie in 
\autoref{aln:ValueUpdate} benutzt wird.
\begin{align}
  V(s) \leftarrow V(s) + step\_size * (reward - V(s))
  \label{aln:ValueUpdate}
\end{align}
\noindent

Über den Trainingsprozess hinweg wird der step\_size Parameter stufenweise herabgesetzt,
um das Verfahren konvergieren zu lassen.

Damit der trainierte Agent sein gewonnenes Wissen exploiten kann, berechnet er aus den
zur Verfügung stehenden Actions die daraus resultierenden States und wählt die Action, die
zum State mit dem höchsten value führt.

\subsubsection{Evaluierung}
Für die Evaulation des Verfahrens wurden Agenten mit unterschiedlicher Anzahl an
Trainingsspielen trainiert, wobei ein Agent entweder immer das Spiel eröffnete oder
den zweiten Zug hatte. Als step\_size wurde initial 0.1 genutzt. Nach jedem Spiel wurde
die step\_size dann mit einem decay multipliziert, um die Konvergenz der State values
zu garantieren.

\autoref{tab:win_probability_tictactoe_begin} zeigt die Gewinnwahrscheinlichkeiten eines
trainierten Agenten, der gegen einen zufällig spielenden Gegner getestet wurde. Jede Zelle
enthält zwei Gewinnwahrscheinlichkeiten. Die erste gibt die Gewinnwahrscheinlichkeit für
den Fall an, dass der Agent das Spiel eröffnet hat, die zweite die
Gewinnwahrscheinlichkeit, falls der Gegner das Spiel eröffnet hat.
\begin{table}[ht]
  \begin{center}
    \begin{tabular}{l | c | c | c}
      Anzahl Trainingsspiele & Gewonnen & Verloren & Unentschieden \\
      \hline
      100 & 68.9\% / 40.8\% & 14.3\% / 54.5\% & 16.9\% / 4.7\% \\
      % 500 & 80.7\% / 53.6\% & 28.6\% / 37.1\% & 14.0\% / 9.3\% \\
      1.000 & 83.6\% / 58.4\% & 10.1\% / 26.1\% & 6.4\% / 15.5\% \\
      10.000 & 94.6\% / 77.3\% & 0.5\% / 7.7\% & 4.9\% / 15.0\% \\
      100.000 & 98.1\% / 83.0\% & 0.0\% / 1.5\% & 1.9\% / 14.6\% \\
      1.000.000 & 99.1\% / 87.7\% & 0.0\% / 0.7\% & 0.9\% / 11.5\% \\
    \end{tabular}

    \caption[Gewinnwahrscheinlichkeiten Tic-Tac-Toe]{Gewinnwahrscheinlichkeiten Tic Tac
    Toe}
    \label{tab:win_probability_tictactoe_begin}
  \end{center}
\end{table}

\noindent
Eine weitere Steigerung der Anzahl an durchgeführten Trainingsspielen bewirkte
anschließend keine Änderung der Gewinnwahrscheinlichkeiten. In allen gezeigten
Experimenten ist die Gewinnwahrscheinlichkeit höher, wenn der Agent die Partie beginnt.
Des Weiteren wird die Wahrscheinlichkeit zu verlieren größer, wenn der Agent nicht den
eröffnenden Zug spielt.

Auch ist deutlich erkennbar, dass eine erhöhte Anzahl von Trainingsspielen, die
Gewinnwahrscheinlichkeit des Agenten steigert. Während die Wahrscheinlichkeit zu gewinnen
beim beginnenden Spieler nach 100 Trainingsspielen noch bei 68.9\% liegt, steigt diese nach
100.000 Trainingsspielen auf 98.1\% an. Zusätzlich sinkt die Wahrscheinlichkeit zu verlieren mit
zunehmendem Training, wobei nach 100.000 Trainingsspielen der beginnende Agent nicht mehr zu
schlagen ist.

Interessant ist vor allem, dass auch bei einer Millionen Trainingsspielen der nicht
beginnende Agent verlieren kann. Dies kann gegen einen zufällig spielenden Gegner
vernünftig sein, da ein kleines Risiko einzugehen dazu führen kann, dass deutlich häufiger
gewonnen wird. % TODO Bruno den Abschlusssatz musst du mir mal erklären


\subsection{Limitierungen von Tabular Q-learning}
\label{sec:tabular_q_learning_limits}
Nach Lapan\cite[~S.192]{L2018} ist die Einsetzbarkeit dieser Verfahren dadurch
beschränkt, dass es in vielen Environments zu viele unterschiedliche States gibt, sodass
dessen values oder die values der Actions in diesen States nicht mehr ausreichend
approximiert werden können.

Dies kann durch die Betrachtung komplizierterer Environments verdeutlicht werden. Lapan
\cite[~S.192]{L2018} zeigt, dass ein Videospiel mit einer Auflösung von 210x160 Pixeln
mehr als $10^{70802}$ mögliche States besitzten kann. Daraus ergibt sich für
kompliziertere Environments, dass das Tabular Q-learning nicht praktikabel ist.

% TODO Bruno more citation needed
\newpage
\section{Deep Q-learning}
\label{sec:dqn}
Die Limitierung von Tabular Q-learning wird in dem nächsten vorgestellten Verfahren
''Deep Q-learning'' überwunden indem ein neuronales Netz eingeführt wird, um die values
von Actions $Q(s, a)$ zu bestimmen \cite[~S.199 ff.]{L2018}. Diese werden nicht mehr in
einer Tabelle gespeichert und aus dieser gelesen, sondern von einem Model, sprich einem
neuronalen Netz, berechnet, das zwischen ähnlichen States abstrahieren kann.

In diesem Kapitel wird zuerst eine Einordnung des Verfahrens vorgenommen, um anschließend
das Verfahren aufbauend auf Tabular Q-learning zu beschreiben. Am Ende werden die	
Implementierung eines Agenten für das RoboschoolPong Environment und die damit erzielten
Ergebnisse vorgestellt.

\subsection{Einordnung}
Deep Q-learning funktioniert ähnlich, wie das schon vorgestellte Tabular Q-learning. Auch
hier werden wahrgenommene Observations abgebildet auf values unterschiedlicher Actions, um
Actions, welche einen hohen Reward versprechen, wählen zu können. Damit ist das Deep 
Q-learning Verfahren als value-based Methode einzuordnen.

Wie auch für Tabular Q-learning können vergangene Trainingsdaten verwendet werden, um die
values der unterschiedlichen Actions zu approximieren, wodurch Deep Q-learning zu den
off-policy learning Verfahren gezählt wird.

\subsection{Verfahren}
\label{sec:deepq-procedure}
Da das Verfahren deutliche Parallelen zu Tabular Q-learning aufweist, läuft der
Trainingsprozess in diesem ebenfalls ähnlich ab. Zuerst wird ein neuronale Netz
initialisiert, um anschließend zufällige Actions im Environment auszuprobieren
und somit ein Tupel aus wahrgenommenem State, gespielter Action, erhaltenem
Reward und neu erreichten Folgestate zu erhalten.

Daraufhin wird erneut der schon bei Tabular Q-learning genutzte Zusammenhang der Bellman
Equation (\ref{aln:DeepQ}) ausgenutzt, um die Ergebnisse des neuronalen Netzes anzupassen.
Die Bezeichnungen der Variablen sind identisch zu denen im Tabular Q-learning. Der State
$s$ ist der wahrgenommene State, $a$ ist die gewählte Action, $r$ ist der dadurch
erhaltene unmittelbare Reward und $s'$ ist der Folgestate, der aus dem Anwenden der
gewählten Action hervorgeht.
\begin{align}
  Q(s, a) \leftarrow r_{s,a} + \gamma \max_{a' \in A}Q(s', a') \label{aln:DeepQ}
\end{align}
\noindent
In Tabular Q-learning wurde anschließend der Wert $Q(s, a)$ der Tabelle angepasst. Da 
$Q(s, a)$ nun durch ein neuronales Netz repräsentiert wird, kann $Q(s, a)$ nicht dem
gefundenen Wert zugewiesen werden. Anstelle dessen wird ein Loss formuliert, auf dem
das Netz mittels Stochastic Gradient Descent trainiert werden kann. Die Formel für
diesen Loss unterscheidet sich abhängig davon, ob es sich bei $s'$ um einen Endzustand
oder einen Zustand mit Folgezustand handelt. Ist $s'$ ein Endzustand, so wird die
Lossfunktion verwendet, welche in \autoref{aln:DeepQLoss1}) veranschaulicht wird.
Besitzt der aktuelle Zustand hingegen einen Folgezustand, wird die Lossberechnung
aus \autoref{aln:DeepQLoss2} eingesetzt.
\begin{align}
  L & = \left(Q(s, a) - r\right)^2 \label{aln:DeepQLoss1} \\
  L & = \left(Q(s, a) - \left(r + \gamma \max_{a'} Q(s', a')\right)\right)^2 \label{aln:DeepQLoss2}
\end{align}
\noindent
Der Wert $Q(s, a)$ definiert dabei die Vorhersage des Netzes über den long-term Reward.
Ist $s$ ein Endzustand, so ist der long-term Reward gleich dem unmittelbaren Reward $r$,
da nach einem Endzustand kein Reward mehr erreicht werden kann. Daraus leitet sich der in
(\ref{aln:DeepQLoss1}) gezeigte Loss für einen Endzustand ab. Hier wird der Mean Squared
Error (auch Mittlere quadratische Abweichung) zwischen der Vorhersage des Netzes und dem
tatsächlich zu erwartenden Reward eingesetzt.

Handelt es sich bei $s$ um keinen Endzustand, so muss der long-term Reward des
Folgezustandes $s'$ hinzuaddiert werden, um den long-term Reward in $s$ zu berechnen. Aus
diesem Grund wird der value der besten Action des Folgezustandes, skaliert mit dem 
discount factor $\gamma$, auf den unmittelbaren Reward addiert. Dabei wird das Netz selbst
verwendet, um die Action values des Folgestates zu berechnen.

Diesen Vorgang wird daraufhin solange wiederholt, bis die vom Netz vorhergesagten Q-values
den tat\-säch\-lich\-en values der Actions entsprechen.

Dies ist die grundlegende Funktionsweise von Deep Q-learning, allerdings muss nach Lapan
\cite[~S.202]{L2018} der Algorithmus, um in der Praxis zu funktionieren, noch weiter
angepasst werden. Wählt man bei der Exploration ausschließlich zufällige Actions, so
werden interessante Situationen nur sehr selten erreicht und es werden sehr viele Samples
benötigt, um zu einer erfolgreichen Lösung der Problemstellung zu gelangen. Aus diesem
Grund wird eine decaying-$\epsilon$-greedy Exploration Strategie benutzt, um gegen Ende
des Trainingsprozesses die schon gelernten Actions des Netzes zu verwenden, damit neue
Erkenntnisse gewonnen werden können.

Eine weitere Optimierung, die den Trainingsprozess verbessert, ist die Verwendung eines
Replay Buffers, in dem Erfahrungen gespeichert werden, um eine breitere Menge an
Trainingsdaten zu erhalten. Nach Lapan \cite[~S.204]{L2018} tritt ohne diesen Replay
Buffer das Problem auf, dass sich die gesammelten Daten aus ein Episode zu stark ähneln,
sodass das Training des Netzes suboptimal funktioniert.

Die letzte Änderung, die vollzogen werden muss, bezieht sich auf die Berechnung des Loss
Wertes aus \autoref{aln:DeepQLoss2}. Für die Berechnung des Losses, mit dem das Netz $Q$
trainiert wird, wird die Approximation des Folgestates von $Q$ selbst verwendet. Durch
diese Selbstreferenzierung kann es passieren, dass das Training instabil wird und nicht
konvergiert. Um dieses Problem zu lösen wird ein neues Netz $Q'$ eingeführt, das als
Target Netz bezeichnet wird. Dieses Netz wird für die Loss Berechnung verwendet, wie in
\autoref{aln:DeepQTarget} gezeigt.
\begin{align}
  L & = \left(Q(s, a) - \left(r + \gamma \max_{a'} \textbf{Q'(s', a')}\right)\right)^2
  \label{aln:DeepQTarget}
\end{align}
\noindent
Nach einer gewissen Anzahl an Trainingsiterationen werden die Gewichte des Target Netzes
mit den Gewichten des trainierten Netzes überschrieben, sodass sich die Netze
synchronisieren.

\subsection{Implementierung}
Mit den eben beschriebenen Änderungen wurde ein Deep Q-learning Algorithmus für das
RoboschoolPong Environment implementiert und getestet. Der Quelltext kann unter
\url{https://github.com/dephiloper/independent-coursework-rl/} im Unterverzeichnis
\href{https://github.com/dephiloper/independent-coursework-rl/tree/master/preparation/07_deep_q_learning}
{\nolinkurl{preparation/07\_deep\_q\_learning}} gefunden werden.


\subsubsection{Das RoboschoolPong Environment}
Für die Umsetzung des Deep Q-learning Verfahrens wurde ein Pong Environment verwendet, das
unter \url{https://github.com/openai/roboschool} zu finden ist. In diesem Environment wird
das bekannte Pong Spiel in einer drei dimensionalen Darstellung simuliert. Zwei Spieler
bewegen jeweils einen Balken mit dem ein Ball auf die Seite des Gegners gestoßen werden
kann. Verfehlt einer der Spieler den Ball, sodass dieser auf der eigenen Seite hinter dem
Spieler das Spielfeld verlässt, so hat der verfehlende Spieler verloren und dessen
Gegenspieler gewonnen. \autoref{fig:pong_env} zeigt das gerenderte Environment.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.4]{res/pong_env.png}
\caption{RoboschoolPong Environment}
\label{fig:pong_env}
\end{figure}
\noindent
In der verwendeten Variante von Pong können die Spieler ihren Balken nicht nur parallel zu
ihrer Grundlinie bewegen, sondern auch auf ihren Gegenspieler zu oder von diesem weg.
Dadurch lässt sich der Ball stärker, sprich mit einer erhöhten Geschwindigkeit, in Richtung
des Gegenspielers stoßen. Als Interface bietet das Environment dabei die Möglichkeit den
Balken in beliebige Richtungen mit beliebiger Geschwindigkeit zu bewegen. Damit handelt es
sich um ein Environment mit kontinuierlichem action space, welches sich mit dem
beschriebenen Verfahren nicht umsetzen lässt. Aufgrund dessen wurde die Menge der Actions auf
neun diskrete Actions beschränkt, mit denen sich der Balken in acht verschiedene Richtungen
bewegen lässt oder stehen bleibt.

Um zwischen den unterschiedlichen Actions unterscheiden zu können, erhält der Agent vom
Environment die Koordinaten und Geschwindigkeiten der Balken sowie des Balles. Zusätzlich
wird die Nummer des aktuellen Frames als Information geliefert.

Den Reward erhält der Agent vor allem dann, wenn er den Ball am gegnerischen Spieler vorbei
auf dessen Grundlinie stößt. Zusätzlich gibt es einen kleineren Reward, wenn der Ball mit
dem Balken getroffen wird.

\subsubsection{Agent}
Für die Lösung des RoboschoolPong Environments wurde das Deep Q-learning Verfahren implementiert,
wobei die Q-values von einem neuronalen Netz berechnet wurden. Es wurden unterschiedliche
Konfigurationen getestet, wobei die Anzahl der Layer zwischen drei und vier, als auch die
Anzahl der Knoten pro Layer variiert wurden. Als Aktivierungsfunktion wurde die
ReLu-Funktion verwendet. Da die Observation nicht als Bild gegeben ist, werden keine
Convolutional Layer verwendet. Die Learning Rate wurde auf $10^{-4}$ gesetzt.

Als discount factor wurde $\gamma = 0.99$ und eine Replaybuffergröße von 10000
Samples gewählt. Nach 1000 Frames wird das Target Net durch das trainierte Netz geupdated.

\subsubsection{Ergebnisse}
Abbildung \ref{fig:pong_results} zeigt eine Auswahl der Testläufe im RoboschoolPong Environment.
Eine Abbildung aller Testläufe ist im Anhang in Abbildung \ref{fig:pong_all_results} zu
sehen.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.4]{res/pong_results.png}
\caption{Ergebnisse RoboschoolPong}
\label{fig:pong_results}
\end{figure}
\noindent
Die verschiedenen Versuche unterscheiden sich in der Netzarchitektur. Hellblau zeigt die
Ergebnisse eines Agenten mit 3 Hidden Layern mit jeweils 64 Knoten. Wie zu erkennen ist,
stagniert der Prozess bei einem Reward von -7.

Besser funktionieren die folgenden Experimente, bei denen 4 Hidden Layer verwendet wurden.
Grau zeigt den durchschnittlichen Reward mit 64 Knoten pro Layer und Grün den Reward für
ein Netz mit 128 Knoten pro Layer.

Damit wurde kurzzeitig ein durchschnittlicher Reward von bis zu 6.9 erreicht. Der
trainierte Agent kann in einem Video unter \url{https://www.youtube.com/watch?v=xp4XtrYNKzQ}
betrachetet werden.

\newpage
\section{Policy Gradients}
Policy Gradients beschreibt eine Unterfamilie und somit Gruppe von Reinforcement Learning 
Methoden aus dem Bereich der policy learning Verfahren. Anders als die beiden zuvor
beschriebenen Verfahren Tabular Q-learning und Deep Q-learning fokussieren sich policy
Verfahren auf das Erlernen einer direkten policy $\pi(s)$. Dies bringt den Vorteil mit sich,
dass Policy Gradients auf Environments mit kontinuierlichen action space, sprich Actions mit
Werterepräsentationen in Gleitkommadarstellung, angewandt werden können (siehe
\ref{sec:action} \nameref{sec:action} - Einschlagwinkel eines Lenkrads). Q-learning Verfahren
hingegen weisen jedem State oder jeder Action immer ein value zu, was das Optimierungsproblem
bei einem kontinuierlichen action space deutlich erschwert \cite[~S.242]{L2018}. Ein
zusätzlicher Vorteil der Policy Gradients ist die stochastische Eigenschaft von Environments.
Anders als Q-learning wird es nach Lapan\cite[~S.242]{L2018} mit Policy Gradients möglich,
die zugrundeliegende Wahrscheinlichkeitsverteilung des Environments besser aufzugreifen,
da das Verfahren, statt anhand von Erwartungswerten des aktuellen und zukünftigen Rewards zu
rechnen, mit einer Wahrscheinlichkeitsverteilung von Actions arbeitet. In der nachfolgenden
Abbildung (\ref{fig:prob-dist-pg}) wird genau diese Wahrscheinlichkeitsverteilung von
diskreten Actions nach Anwendung der Policy Approximation visualisiert.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.4]{res/policy_approxim.png}
\caption{Wahrscheinlichkeitsverteilung von Actions durch Policy Approximation}
\source{\cite[~S.243 - Chapter 9 - Figure 1]{L2018}}
\label{fig:prob-dist-pg}
\end{figure}

Zusätzlich kommt, durch die Repräsentation der Actions als Wahrscheinlichkeit, der Vorteil einer
\textit{smooth representation} des Netzes zum tragen \cite[~S.243]{L2018}. 
Das heißt, würde der Output des Netzes über diskrete Werte ausgedrückt werden, kann schon eine
minimale Änderung an den Gewichten des Netzes zu einer komplett anderen Action bei der
Vorhersage führen. Aufgrund der Wahrscheinlichkeitsverteilung bewirken Anpassungen hingegen
nur moderate Änderungen in der resultierenden Verteilung des Output.


\subsection{REINFORCE Method}
Das nachfolgend beschriebene Verfahren REINFORCE oder Monte Carlo Policy Gradient, weist
deutliche Ähnlichkeiten zur bereits erläuterten \nameref{sec:cross-entropy-method} auf.
Genau wie das Cross-Entropy Verfahren, erhält das Netz als Input die Observation des
Environments und liefert eine Wahrscheinlichkeitsverteilung der Actions an den Agent
zurück. Der Unterschied kommt durch die in \autoref{aln:PG} festgehaltene Definition
des $PG$ (Policy Gradient) zustande.

\begin{align}
\mathcal{L}=-Q(s,a)\log\pi(a|s)
\label{aln:PG}
\end{align}

PG definiert dabei die Richtung, in welche die Gewichte des Netzes verschoben werden müssen,
um die Policy im Hinblick auf den kumulierten total Reward zu verbessern \cite[~S.244]{L2018}.
Die Skalierung des Gradienten ist hierbei proportional zum value der zu wählenden Action
$Q(s,a)$ und der Gradient selbst entspricht in der Formel dem Gradienten der logarithmischen
Wahrscheinlichkeit der ausgewählten Action. Dadurch wird es möglich die Wahrscheinlichkeit
von Actions mit einem hohen total Reward zu erhöhen und im Gegenzug die Wahrscheinlichkeit
der Action mit einem niedrigen oder negativen total Reward zu verringern.

Ein weiterer Unterschied zur Cross-Entropy-Method ist, dass diese als PG Methode mit einem
$Q(s,a)=1$ für Episoden mit hohem total Reward und $Q(s,a)=0$ für schlechte Episoden
definiert werden kann, wohingegen REINFORCE wirkliche Berechnungen für $Q(s,a)$ durchführt
und somit eine detaillierte Unterteilung der Episoden ermöglicht.

\subsubsection{Einordnung}
Wie bereits erwähnt ist die REINFORCE Method ein policy learning Verfahren. Das Verfahren ist
on-policy, da ausschließlich anhand neu aufgezeichneter Trainingsdaten gelernt werden kann.
Demzufolge kommt das Verfahren ohne die Verwendung eines Replay Buffers (siehe 
\autoref{sec:deepq-procedure}) aus. Des Weiteren erfolgt die Einordnung als model-free Methode,
da nicht versucht wird Annahmen zu zukünftigen States oder Rewards zu treffen. Prinzipiell
basieren alle Policy Gradient Verfahren auf der in Formel \ref{aln:PG} festgelegten Definition,
wobei ausschließlich Anpassungen am Kernkonzept des Verfahrens vorgenommen werden
\cite[~S.244 f.]{L2018}. Anders als die bereits behandelten value-learning Verfahren ist
kein explizites Explorieren notwendig, da das REINFORCE Verfahren durch die Nutzung einer
Wahrscheinlichkeitsverteilung über alle Actions ein automatisches ''Erforschen'' 
des Environments und der zu tätigenden Actions vollzieht. Bei einer Initialisierung mit
zufälligen Gewichten gibt das Netz eine uniforme Wahrscheinlichkeitsverteilung zurück und
somit die Verhaltensweise eines random Agents.


\subsubsection{Verfahren}
Zu Beginn des Verfahrens werden die Parameter des Netzes $\theta$ als zufällige Gewichte
initialisiert. Nachfolgend werden, genau wie bei der value-based Methode Q-learning, werden
multiple Episoden im Environment gespielt, wobei ein Batch an SARS-Tupeln produziert wird.
Anschließend erfolgt die Berechnung des discounted total Reward für die Folgeschritte eines
jeden steps. Hierbei entspricht ein step einem SARS-Tupel in einer Episode. Diese Berechnung
wird durch die nachfolgend aufgeführte Formel \ref{aln:reinforce-total-reward} beschrieben,
in welcher $t$ den aktuellen step und $k$ die Episode in welcher der step $t$ enthalten
ist, definiert. 

\begin{align}
Q_{k,t}=\sum_{i=0} \gamma^{i}r_{i}
\label{aln:reinforce-total-reward}
\end{align}

Da im REINFORCE Verfahren die Berechnung des discounted total Reward $Q_{k,t}$ direkt durch
Aufsummierung des aktuellen Rewards und der in zukünftigen Zuständen erhaltenen Rewards
ermittelt werden kann, ist eine Approximation von $Q$ wie in Deep Q-learning nicht notwendig,
was zusätzlich die Verwendung des Target Netz überflüssig macht (siehe 
\autoref{sec:deepq-procedure}). Mittels $Q_{k,t}$ kann daraufhin die Berechnung des Loss
anhand der in Formel \ref{aln:reinforce-loss} festgehaltenen Gleichung ermittelt werden.

\begin{align}
\mathcal{L}=-\sum_{k,t}Q_{k,t}\log(\pi(s_{k,t},a_{k,t}))
\label{aln:reinforce-loss}
\end{align}

Auf Grundlage dieser Berechnung wird ein Update der Gewichte mittels Stochastic
Gradient Descent durchgeführt und somit das Minimieren des Loss erzielt. Das Verfahren
wird daraufhin solange ausgeübt bis es konvergiert.

\subsubsection{Limitierungen von REINFORCE}
\label{sec:limit-reinforce}
Zu den Limitierungen des REINFORCE Verfahrens zählen laut Lapan\cite[~S.252]{L2018}, dass für den
Trainingsprozess jede Episode immer bis zum Schluss gespielt werden muss. Andernfalls kann der
discounted total Reward nicht vollständig berechnet werden. Des Weiteren können bessere Resultate
erzielt werden, umso länger trainiert wird. Dies kann bei einem Environment mit lang andauernden
Episoden zeitaufwendig werden, da nach jedem Trainingsschritt immer neue Trainingsdaten vom
Environment generiert werden müssen. Um diesen Problemen entgegen zu wirken existiert ein weiteres
Verfahren, die \textit{Actor-Critic Method}, welche im nächsten Kapitel erläutert wird. 

Zusätzlich weist die REINFORCE Method eine hohe Varianz bei der Berechnung des Gradienten auf.
Dies kommt aufgrund der Skalierung des Gradienten mittels des discounted total Rewards $Q(s,a)$ zustande.
Hauptgrund dafür ist, dass der Wertebereich des zu erhaltenden Rewards von Environment zu Environment
stark variiert und in diversen Environments zu Beginn der Anteil an Punishment (negativer Reward) den
Erhalt des positiven Reward deutlich dominiert. Durch die Einführung einer Baseline, einem Wert, der
von dem total Reward $Q$ subtrahiert wird, kommt es zu einer Stabilisierung des Verfahrens. Neben
diesen Erweiterungen existieren weitere, welche im nachfolgenden Kapitel analysiert werden. Aufgrund
der Instabilität des Trainingsprozesses findet das REINFORCE Verfahren selten Anwendung in komplexen
Environments.   
\newpage


\section{Actor-Critic}
Das letzte zu erläuternde Verfahren reiht sich als Erweiterung in die Policy Gradients Familie ein.
Bei der \textit{Actor-Critic Method} handelt es sich um ein state-of-the-art Reinforcement Learning
Verfahren mit der Bestrebung die Stabilität und den Prozess des Konvergierens im Trainingsablauf
zu verbessern. Wie bereits in \autoref{sec:limit-reinforce}: \nameref{sec:limit-reinforce} beschrieben
kann durch die Einführung einer Baseline, sprich einem Wert, welcher in der Berechnung des Loss von dem
discounted total Reward $Q(s,a)$ abgezogen wird, die Varianz des Gradienten reduziert werden, wodurch
der Trainingsprozess robuster wird. Die Baseline kann dabei eine Konstante sein, die dem Mittelwert 
aller discounted total Rewards entspricht oder als gleitender Mittelwert (moving average) realisiert
werden, welcher während des Trainingsverlauf stetig angepasst wird. Eine weitere Option ist den Baseline
Wert zustandsabhängig zu realisieren, sprich in jedem Zustand wird der Wert $V(s)$ ermittelt, der Auskunft
darüber gibt, wie gut dieser Zustand einzuschätzen ist. Anschließend kann ein \textit{advantage} Wert
$A(s,a)$ ermittelt werden, indem von dem berechneten discounted total Reward $Q(s,a)$ der \textit{value}
des States $V(s)$ abgezogen wird \cite[~S.268 ff.]{L2018}. Hierbei ist zu erwähnen das die Berechnung von
$Q(s,a)$ genau wie im REINFORCE Verfahren (siehe \autoref{aln:reinforce-total-reward}) umgesetzt wird.
Die nachfolgende Formel \ref{aln:actor-critic-advantage} veranschaulicht den Zusammenhang zwischen der
value des States $V(s)$ und dem discounted total Reward $Q(s,a)$.

\begin{align}
Q(s,a)=V(s)+A(s,a)
\label{aln:actor-critic-advantage}
\end{align}

Der advantage Wert $A(s,a)$ wird anschließend für die Skalierung des Gradienten verwendet und gibt Auskunft
darüber, wie viel besser oder schlechter eine gewählte Action im Kontrast zum durchschnittlichen value des
States ausfällt. Um $A(s,a)$ zu berechnen muss initial $V(s)$ ermittelt werden. Dafür wird neben dem bereits
bestehenden ein weiteres neuronales Netz eingeführt, welches den Wert von $V(s)$ approximiert und sich damit
an der Verfahrensweise von \nameref{sec:dqn} orientiert.

In der Realisierung des Actor-Critic Verfahrens kann dieses mittels einer Zusammenführung beider Netze als
\textit{shared net} umgesetzt werden  \cite[~S.269]{L2018}. Im Hinblick darauf wird die erhaltene
Observation als Input in ein \textit{Common net} gegeben, welches im Falle von Bildern als Input aus
multiplen Convolutional Layern bestehen könnte. Anschließend werden die ermittelten Features sowohl an
ein \textit{Policy net} $\pi(a|s)$ als auch an ein \textit{Value net} $V(s)$ zur Verarbeitung
weitergegeben. Die beschriebene Architektur des Verfahrens ist in \autoref{fig:actor-critic}
visualisiert.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.5]{res/actor-critic.png}
\caption{Actor-Critic - shared net Architektur}
\source{\cite[~S.268 - Chapter 10 - Figure 5]{L2018}}
\label{fig:actor-critic}
\end{figure}

Um den Trainingsprozess umzusetzen, kann die selbe Prozedur wie bei Deep Q-learning verwendet werden, wobei
die Bellman Equation zur Ermittlung des discounted total Rewards $Q(s,a)$ angewandt wird. Anschließend wird
der Mean-Square-Error Loss zwischen berechnetem und vorhergesagtem $Q(s,a)$ minimiert, um die Approximation
von V(s) zu bewirken.


\subsection{Einordnung}
Bei dem Actor-Critic Verfahren handelt es sich um eine policy learning Methode, da final Actions anhand der
Wahrscheinlichkeitsverteilung des Policy net ausgewählt werden. Aufgrund der parallelen Nutzung des Value
net kann das Verfahren aber ebenfalls als Mischform betrachtet werden. Genau wie bei der REINFORCE Method
kann ausschließlich aufgrund neu aufgezeichneter Daten aus dem Environment trainiert werden (on-policy).
Das Verfahren fällt des Weiteren in den Bereich der model-free Verfahren, da kein Model zu möglichen
Folgezuständen und Rewards aufgebaut wird. 

\subsection{Verfahren}
Wie bei dem Großteil aller erläuterten Verfahren werden zu Beginn die Netzwerkparameter $\theta$ mit
zufälligen Werten initialisiert. Anschließend werden $N$ Schritte im Environment anhand der aktuellen
Policy $\pi(s|a)$ gespielt, wobei jeweils ein Tupel aus State $st$, Action $at$ und Reward $rt$
gespeichert werden. Nach dem Abschluss einer Episode werden anschließend die vollzogenen Schritte in
umgekehrter Reihenfolge verarbeitet. Darauf folgend werden die Gewichte des Netzes aktualisiert, indem
die akkumulierten Gradienten beider Netze in Richtung des Policy Gradients $\partial\theta_\pi$
und entgegen der Richtung des value Gradients $\partial\theta_v$ verschoben werden. Diese Schritte
werden anschließend solange wiederholt, bis das System konvergiert \cite[~S.268 f.]{L2018}.

Das Policy net wird dabei als \textit{Actor} bezeichnet, da man von diesem ableiten kann, welche
Action ausgeführt werden soll. Das Value net wird hingegen als \textit{Critic} bezeichnet, da es mit
diesem möglich wird, zu bewerten wie gut die ausgeführten Actions waren, um somit bei einem hohen
advantage Wert die Wahrscheinlichkeiten der gewählten Action mehr in diese Richtung zu bewegen.

\subsection{Erweiterungen}
Obwohl das Actor-Critic Verfahren bereits als Erweiterung zum klassischen Policy Gradient Ansatz
gezählt wird existieren weitere Maßnahmen zur Verbesserung und Beschleunigung der Methode.

\paragraph*{Entropy Bonus} 
\noindent
\newline
Durch die Einführung des Entropy Bonus wird der explorierende Ansatz des Verfahrens bestärkt.
Der Bonus wird als Entropy Wert zur bestehenden Loss-Funktion hinzugefügt (siehe
\ref{aln:actor-critic-entropy-bonus}):

\begin{align}
\mathcal{L}_H=\beta\sum_i\pi_\theta(s_i)\log_{\pi_\theta}(s_i)
\label{aln:actor-critic-entropy-bonus}
\end{align}

Da diese Funktion an ihrem Minimum ankommt, sobald die Wahrscheinlichkeitsverteilung uniform,
sprich gleichverteilt ist, kann durch anfügen des Ergebnisses zur Loss-Funktion der Agent davon
abgehalten werden sich ''zu sicher'' bei der Berechnung von Actions zu sein 
\cite[~S.269 f.]{L2018}. Im Umkehrschluss bedeutet dies, dass bei einer
Wahrscheinlichkeitsverteilung bei der ausschließlich eine Action zu 100 Prozent als wählbare
Action identifiziert wurde, der berechnete Loss nicht einem Ergebnis von 0 entspricht.

\paragraph*{Multiple Environments} 
\label{sec:multiple_envs}
\noindent
\newline
Eine zusätzliche Erweiterungsmöglichkeit im Bereich der Stabilität des Verfahrens ist die Nutzung
von mehreren parallelen Environments \cite[~S.270]{L2018}. Dadurch können simultan mehrere
Observations gesammelt werden was aufgrund der on-policy Eigenschaft des Verfahrens den
Trainingsprozess deutlich beschleunigt. Zusätzlich erhalten die aufgezeichneten Daten eine
gewisse Unabhängigkeit voneinander, da Observations abwechselnd aus den jeweiligen Environments
gesampled werden können. Diese Erweiterungsmöglichkeit wird auch als \textit{Asynchronous Advantage
Actor Critic} oder in Kurzform A3C bezeichnet.

\newpage
\section{Projektarbeit}
Nach der Aufarbeitung der theoretischen Bereiche von Reinforcement Learning sowie der Umsetzung von
Algorithmen und Erweiterungen befasst sich der abschließende Teil des Independent Coursework damit,
ein eigenes Environment zu entwickeln. Anhand dieses Environments soll ein Reinforcement Learning
Verfahren implementiert werden, mit dem Ziel die Problemstellung des Environments erfolgreich zu 
lösen. In diesem Kapitel erfolgt initial die Festlegung auf ein Spiel für die Entwicklung des
Environments. Nachfolgend wird auf die Entwicklung des Environments und die darin enthaltenen
Funktionalitäten eingegangen. Abschließend wird aus den bereits vorgestellten Algorithmen ein
Verfahren gewählt und dieses in dem entwickelten Environment angewandt. 

\subsection{Mögliche Spiele}
Wie in der Zielsetzung dieser Ausarbeitung beschrieben, soll nachfolgend ein Spiel ausgewählt werden,
für welches mittels der Open AI Bibliothek ein Environment umgesetzt wird. Für die abschließende
Festlegung wurden zu Beginn folgende Spiele in Betracht gezogen:

\subsubsection{Doom}
Doom (id Software, 1993) ist ein First-Person Shooter, welcher im Dezember 1993 auf dem Betriebssystem
MS-DOS veröffentlicht wurde. Im Spiel steuert man einen Space-Marine, welcher als ''Doomguy'' bezeichnet
wird und dessen Aufgabe es ist, sich durch eindringende Dämonenhorden aus der Hölle zu kämpfen
\cite{D1993}. Es existieren bereits Realisierungen in denen Agenten mittels Reinforcement Learning
Verfahren das Spiel lernen zu spielen. Diese verwenden meist alle eine, explizit für die Entwicklung
von künstlicher Intelligenz, bereitgestellte Modifikation des Hauptspiels - ''ViZDoom'' \cite{ViZDoom}. 
ViZDoom ermöglicht es das Spiel in beschleunigter Form (7000 Bilder pro Sekunde) auszuführen. Des
Weiteren enthält die Modifikation die Implementierung einer eigens angefertigten API sowie weitere
durch die Community bereitgestellte Anpassungen.

Die Steuerung der Kernmechaniken im Spiel umfassen das nach links und rechts Drehen des Spielers,
das Bewegen in die vier Haupthimmelsrichtungen sowie das Abfeuern einer Waffe (sieben diskrete
Actions). Die Modifikation des Hauptspiels ist unter \url{https://github.com/mwydmuch/ViZDoom}
aufrufbar. Zusätzlich wäre auch die Anfertigung eines eigenen Environments in Doom möglich. Dafür
stehen weitere Portierungen zur Verfügung wie zum Beispiel \url{https://zdoom.org/index}, welches
das Spiel auf modernen Geräten ausführbar macht. 

\subsubsection{Teeworlds}
Bei dem Spiel Teeworlds handelt es sich um ein kostenloses online Mehrspieler Spiel, welches seine
Erstveröffentlichung am 27. Mai 2007 hatte. Das Spiel wurde am Anfang ausschließlich von dem
Entwickler Magnus Auvinen programmiert, 2010 als Open Source Projekt unter der BSD-Lizenz
veröffentlicht und seitdem von mehreren Entwicklern verwaltet. In Teeworlds nimmt man die Form
eines ''Tee'' an, manövriert diesen durch die Spielwelt, weicht gegnerischen Projektilen aus und
umgeht Stacheln. Neben dem Schusswechsel mit anderen Spielern definiert eine Greifhakenmechanik den
Hauptaugenmerk des Spiels. Zusätzlich erlaubt das Spiel das Aufsammeln von spielrelevanten Objekten
wie Herzen zum Auffüllen der Lebensenergie.

Die Hauptmechaniken zur Steuerung des Avatars bestehen aus der horizontalen Bewegung, einem Befehl
zum Springen sowie einer 360 Grad Ausrichtung des Greifhaken oder der ausgerüsteten Waffe.
Das Spiel ist unter \url{https://github.com/teeworlds/teeworlds} zu finden.

\subsubsection{Starcraft II}
Im Github Repository \url{https://github.com/Blizzard/s2client-proto} wurde eine API für das Spiel
Starcraft II der Firma Blizzard umgesetzt, welche eine vollständige Kontrolle des Spiels durch
externe Entitäten ermöglicht. Starcraft II (2010) ist ein Science Fiction Echtzeit-Strategiespiel,
in dem Einheiten von jeweils drei verschiedenen Spezies gegeneinander antreten. Neben dem
Management von Militärtruppen müssen Basen gebaut und Rohstoffe verwaltet werden. Die Steuerung
des Spiels ist deutlich komplexer als die der zuvor beschriebenen Spiele. Es existiert jedoch
bereits die Umsetzung eines mit Reinforcement Learning Verfahren nutzbaren Environments der Firma
DeepMind (\url{https://github.com/deepmind/pysc2}). 

\subsection{Festlegung auf ein Spiel}
Aufgrund der hohen Komplexität und der Vielschichtigkeit der Steuerung wurde sich gegen die Umsetzung
eines Environments für das Spiel Starcraft II entschieden. Die Spiele Teeworlds und Doom bieten
deutlich einfachere Verfahren zur Steuerung durch einen Agenten. Da bereits Realisierungen für Doom
existieren und das Spiel Teeworlds durch den öffentlich verfügbaren Source Code vollständig modifizierbar
ist, wurde Teeworlds abschließend für die Realisierung eines Environments ausgewählt. Zudem erfüllt
Teeworlds die zu Beginn festgelegte Anforderung eines Spieles in Echtzeit.

Nahezu die gesamte Codebase von Teeworlds ist in C++ entwickelt worden, wobei durch das Kompilieren
des Spiels zwei ausführbare Dateien entstehen, ein Teeworlds Client und ein Teeworlds Server. Der
Client kann ausschließlich in Kombination mit einem Server genutzt werden. Über eine UDP Verbindung
werden zwischen Client und Server Informationen zur Veränderung des Game State, der Spielerpositionen
oder über eingesammelte Objekte ausgetauscht.

\subsection{Umsetzung des Environments}
Um das Spiel Teeworlds für einen Reinforcement Learning Algorithmus zugänglich zu machen,
wird das Spiel in ein Python Environment eingebetet. Dabei ist zu beachten, dass
Informationen sowohl mit dem Teeworlds Client als auch mit dem Teeworlds Server ausgetauscht
werden müssen. Zusätzlich wurde darauf geachtet, dass das eigens entwickelte Environment dem
Standard eines OpenAI gyms entspricht.

In diesem Abschnitt wird beschrieben, wie die Umarbeitung des Teeworlds-Clients und
Servers realisiert wurde. Dazu wurde das Teeworlds Github Repository geforkt und
modifiziert. Der Fork ist unter der url \url{github.com/nevs-devs/teeworlds} einsehbar.
Das Python OpenAI Environment ist unter \url{github.com/dephiloper/independent-coursework-rl/}
im Unterverzeichnis \textit{teeworlds\_env} in der Datei
\href{github.com/dephiloper/independent-coursework-rl/teeworlds_env/gym_teeworlds.py}
{\nolinkurl{gym\_teeworlds.py}} zu finden.

Besonderes Augenmerk wurde auf die standardkonforme API des Environments gelegt, wodurch
es auch anderen Reinforcement Learning Interessierten möglich ist, das entwickelte
Environment zu nutzen.

\subsubsection{Abwandlung des Spiels}
Das Spiel Teeworlds verfügt mit dem Greifhaken über kontinuierliche Actions und ist als
Mehrspieler online Spiel eine komplexe Problemstellung. Um die in den vorhergehenden
Kapiteln beschriebenen Verfahren einsetzen zu können, wurde für Umsetzung des Environments
das Teeworlds Spiel stark vereinfacht.

Da das Problem von kontinuierlichen Actions durch keines der vorgestellten Verfahren
ausreichend gelöst wurde, wurde die Greifhakenmechanik und das Schießen bei der
abschließenden Realisierung nicht verwendet. Anstelle dessen wurden sechs diskrete
Actions umgesetzt. Diese sechs Actions zur Bewegung des Avatars sind ''links'',
''stehen bleiben'', ''rechts'', ''nach links springen'', ''springen'',
''nach rechts springen''.

Die Hauptaufgabe im Spiel wurde dahingehend verändert, dass es das Ziel ist erfolgreich
durch ein Level zu navigieren und auf dem Weg plazierte Schilder und Herzen einzusammeln.
Die Schilder resultieren in einem Reward von $1$. Das Einsammeln des Herzens ist das
schlussendliche Ziel, führt zu einem Reward von $5$ und beendet die Episode. Das Spielen
gegen andere Spieler ist in dieser Umsetzung des Environments nicht vorgesehen. 
Es ist jedoch möglich das Environment zu erweitern, sodass auch kontinuierliche Actions
des Greifhakens und des Schießens sowie die Einbindung der Mehrspielerkomponente
unterstützt werden.

\subsubsection{Architektur des Environments}
\label{sec:architecture-env}
Dieser Abschnitt beschreibt die Umsetzung der Kommunikation, zwischen dem Teeworlds Client
und Server sowie den Informationsaustausch zum Python Environment. Wie eingangs schon
erwähnt funktioniert Teeworlds, indem sich der Teeworlds Client auf einen Teeworlds Server
verbindet. Es gibt zahlreiche öffentliche Server, die benutzt werden können. Für den
Anwendungsfall dieser Arbeit ist es allerdings notwendig einen eigenen Server zu erstellen,
da so sichergestellt wird, dass keine fremden Spieler eine Verbindung zum Server aufbauen.
Des Weiteren wird durch das Bereitstellen des Servers die Administration von diesem möglich,
weshalb das festgelegte Level auf dem gespielt wird bestimmt werden kann. Zusätzlich wurde
es dadurch möglich, den Spielzustand neu zu starten und eine Kommunikation zwischen Python
Environment und Teeworlds Server zu implementieren.

Aus diesem Grund startet das Environment zuerst einen Teeworlds Server und nach einer
kurzen Verzögerung den Client als Subprocess. Der Client verbindet sich daraufhin mit dem
neu gestarteten Server und öffnet ein Fenster in dem das Spiel zu sehen ist.

Um den aktuellen Spielstand zu erfahren, nimmt das Environment mithilfe des Python
Packages \lstinline!mss! ein Screenshot vom Teeworlds Fenster auf. Dieses Bild wird
später als Schwarzweißbild für die Observation benutzt. Da sich aus einem einzelnen Bild
keine Bewegungsrichtung ableiten lässt, diese aber für ein erfolgreiches Spielen von
Echtzeitspielen notwendig ist, werden die letzten vier Bilder des Spiels, ähnlich wie die
Farbkanäle eines Bildes, übereinander gelegt, wie von V. Mnih \cite[~S.5]{mnih2013playing}
vorgeschlagen. Für die Implementierung wurde eine \lstinline!deque! verwendet, wie in
Listing \ref{lst:env_observation} gezeigt. Wird das Environment zurückgesetzt, so wird
ein neues Bild aufgenommen und drei mal in den ImageBuffer geschrieben.
\begin{lstlisting}[language=Python, caption=Observation des Environments,
label=lst:env_observation]
class TeeworldsEnv(gym.Env):
  def reset(self):
    # ...
    self.image_buffer = deque(
      [self.capture_game_image()] * NUMBER_OF_IMAGES,
      maxlen=NUMBER_OF_IMAGES
    )

  def step(self, action):
    # ...
    game_image = self.capture_game_image()
    self.image_buffer.append(game_image)
    observation = np.array(self.image_buffer)
\end{lstlisting}
Damit die vom Agenten getätigten Actions ausgeführt werden, müssen diese zum Teeworlds Client
übertragen werden. Damit diese Übertragung bewerkstelligt werden kann, wurde die Bibliothek
ZeroMQ \url{https://zeromq.org/} verwendet, die es ermöglicht Datenpackete zwischen
unterschiedlichen Prozessen zu transferieren. Die gewählten Actions werden dann in Python
in einer Bitmaske kodiert und übertragen. Der modifizierte Teeworlds Client empfängt diese
Informationen und manipuliert den Zustand des Spiels so, als hätte ein Spieler die
entsprechenden Befehle gegeben. Neben den Actions kann an dieser Stelle ein reset des
Spielstandes sowie der Wechsel des aktuellen Levels initiiert werden.
\begin{lstlisting}[language=C++, caption=Empfangen der Actions im Teeworlds Client,
label=lst:teeclient_receive]
void receive_update() {
  uint8_t buffer[5];

  while (true) {
    int size = zmq_recv(actions_receiver, buffer,
                        5, ZMQ_DONTWAIT);
                        
    // if no more messages are available
    if (size == -1) break; 

    unsigned char action_byte = *(buffer + 4);
    jump = action_byte & 0b00000001;
    hook = action_byte & 0b00000010;
    fire = action_byte & 0b00000100;
    reset = action_byte & 0b00100000;
    rotate = action_byte & 0b01000000;

    // set direction
    uint8_t dir_bits = (uint8_t)(action_byte & 0b00011000) >> 3u;
    if (dir_bits == 0b00000001) direction = 1;
    else if (dir_bits == 0b00000010) direction = -1;
    else direction = 0;
  }
}
\end{lstlisting}
\noindent
Um den Reward zu berechnen, den das Environment an den Spieler zurück gibt, muss
übertragen werden, ob ein Schild oder ein Herz eingesammelt wurde oder, ob der Spieler mit
Stacheln kollidiert ist. Es stellte sich heraus, dass der Teeworlds Client diese Informationen
nicht selbst berechnet, sondern den neuen Spielstand durch den Teeworlds Server übertragen bekommt.
Dadurch wurde es notwendig, ebenfalls eine Verbindung zwischen Environment und Teeworlds Server
umzusetzen. Der modifizierte Teeworlds Server schickt diese Spielinformationen ebenfalls über
ZeroMQ an das Environment zurück. Abbildung \ref{fig:teeworlds_env} zeigt eine Übersicht der
Architektur.
\begin{figure}[htp]
  \centering
  \includegraphics[scale=0.5]{res/teeworlds_env.png}
  \caption{Teeworlds Environment Aufbau}
  \label{fig:teeworlds_env}
\end{figure}
\noindent
Weiterhin wurden kleinere Änderungen an Server und Client vorgenommen, wie das Entfernen
unnötiger Statusanzeigen, auf die der Agent fälschlicherweise achten könnte, oder die
Möglichkeit verschiedene Level in einem Environment spielen zu können.

Wie schon in Abschnitt \ref{sec:multiple_envs} beschrieben, können mehrere Environments
gleichzeitig verwendet werden, um den Trainingsprozess zu beschleunigen. In der Datei
\lstinline!dqn\_parallel.py!, die im gleichen Verzeichnis zu finden ist wie das Teeworlds
Environment, findet sich eine \lstinline!Worker! Klasse, die von \lstinline!Process! erbt
und ein Environment erstellt. Im Trainingsprozess werden vier Worker erstellt und
gleichzeitig verwendet, um das Sammeln der Erfahrung zu beschleunigen. Gleichzeitig kann
das Spiel auch mit einer höheren Geschwindigkeit gespielt werden. \autoref{fig:multi_env}
zeigt eine typische Trainingssituation mit vier Workern.
\begin{figure}[htp]
  \centering
  \includegraphics[scale=1.0]{res/multi_env.png}
  \caption{Multiple Teeworlds Environments}
  \label{fig:multi_env}
\end{figure}
\noindent

Weiterhin wurden mehrere Level erstellt, welche speziell für die Problemstellung von
ausschließlich drei Spielobjekten - Schilde, Herzen und Stacheln -  entworfen wurden.
Diese wurden im Anhang unter \autoref{sec:levels} angefügt. Zusätzlich wurde darauf
geachtet, unterschiedliche Passagen in die Level einzubauen.

Eine große Herausforderung bei der Implementierung des Environments bestand darin, die
Kommunikation zwischen den einzelnen Entitäten herzustellen, da die Kommunikation den
sendenden oder empfangenden Prozess niemals blockieren durfte. Grund dafür ist, dass
ein blockierendes Element in Teeworlds das Spiel oder im Environment das regelmäßige
Abfragen der Observation stören würde. Die Nutzung von ZeroMQ erwies bei dieser
Problemstellung als hilfreich, da nicht blockierende Abfragen von dieser Bibliothek
unterstützt werden.

Eine weitere Herausforderung war es den Agenten in mehreren Environments auszuführen, da
die Multiprocessing Realisierung von Pytorch eine Schwierigkeit darstellte. Auch die
Orientierung im Teeworlds Quelltext und dessen Umarbeitung stellte sich als Hürde dar.

\subsection{Wahl des Verfahrens}
In diesem Abschnitt werden die zuvor behandelten Reinforcement Learning Verfahren
verglichen und in Bezug auf deren Anwendbarkit auf das Teeworlds Environment abgewägt.
Anschließend wird ein Verfahren ausgewählt, welches zur Lösung des entwickelten
Environments eingesetzt wird.

\paragraph{Cross-Entropy}
Das erste beschriebene Verfahren ist die Cross-Entropy Methode. Wie in
\autoref{itm:cross_entropy_limits} erläutert, existieren für die Cross-Entropy Methode
Einschränkungen, die dazu führen können, dass das System nicht konvergiert oder anderweitig
ohne Erfolg verläuft. Zum einen sollte das Environment Zwischenschritte belohnen,
damit schneller in diese Richtung optimiert werden kann. Dies ist durch die im Level
platzierten Schilde gegeben. Damit einher geht eine Unterscheidung zwischen eher guten
und schlechten Episoden, welche für eine erfolgreiche Umsetzung des Verfahrens notwendig
ist. Ein weiteres Problem mit der Cross-Entropy Methode ist, dass auf langen Episoden
nicht gut gelernt werden kann, da die Methode nicht nachvollzieht, welche Actions gut
und welche Actions weniger gut funktioniert haben. 

Eine Episode im Teeworlds Environment gilt als beendet, wenn in Stacheln gesprungen wurde, 
wenn das Herz eingesammelt wurde oder wenn ein Zeitlimit abläuft. Das Zeitlimit wurde für
die im weiteren Verlauf beschriebenen Experimente auf 40 Sekunden gesetzt, was bei einer
Framerate von 5 Frames pro Sekunde eine maximale Episodenlänge von 200 Frames bedeutet,
was mit dem CartPole Environment vergleichbar ist. %TODO lass das nochmal diskutieren

Wie in Kapitel \ref{sec:cross_entropy_results} gezeigt, eignet sich die Cross-Entropy
Methode in ihrer Einsetzbarkeit nur begrenzt und komplexere Environments können nicht
erfolgreich gelöst werden.

\paragraph{Tabular Q-learning}
Tabular Q-learning speichert die values der unterschiedlichen State Action Paare in einer
Tabelle, was, wie in Kapitel \ref{sec:tabular_q_learning_limits} gezeigt, zu Problemen bei
komplexeren Environments führt. Die Ermittlung der unterschiedlichen values für jeden
möglichen State sind aufgrund der Observation als Bild nicht praktikabel.

\paragraph{Deep Q-learning}
Mit Deep Q Learning wird das Problem des zu großen observation spaces gelöst, da die
values der States nicht mehr in einer Tabelle gespeichert werden müssen, sondern über ein
neuronales Netz berechnet werden. Auch wurden mit Deep Q Learning komplexere Environments
gelöst, sodass davon auszugehen ist, dass sich dieses Verfahren prinzipiell dazu eignet.

\paragraph{REINFORCE Method}
Da die REINFORCE Methode eine Erweiterung der Cross-Entropy Methode ist, weißt sie
ähnliche Limitierungen wie diese auf. In Abschnitt \ref{sec:limit-reinforce} wurden
bereits die Limitierungen der REINFORCE Methode beschrieben. Dazu zählt, dass lange
Episoden einen langen Trainingsprozess mit sich bringen, da nach jedem Training neue 
Daten generiert werden müssen. Des Weiteren spricht die hohe Varianze bei der Berechnung
der Gradienten gegen die Nutzung der Methode, da dadurch der Trainingsprozess instabil
wird.

\paragraph{Actor-Critic}
Die Actor-Critic Methode ist eine Erweiterung der REINFORCE Methode, die auch in komplexen
Environments funktioniert. Unter anderem wurde sie schon im Atari Environment erfolgreich
getestet. Außerdem erlaubt es die policy-based Actor-Critic Methode nach Lapan,
kontinuierliche Actions zu verwendet, was \cite[~S.346]{L2018} mit value-based Verfahren
schwierig umzusetzen ist.

\subsubsection{Auswahl}
Für die Umsetzung des Prototypen sind vor allem Deep Q-learning und Actor-Critic relevant,
da Tabular Q-learning für das gegebene Environment nicht umsetzbar ist und Actor-Critic
eine Verbesserung zur Cross-Entropy und REINFORCE Methode darstellt. Da im Vorfeld mehr
praktische Experimente mit Deep Q-learning durchgeführt wurden, wurde diese Methode für
die Umsetzung des Prototypen gewählt. Die Festlegung auf ausschließlich diskrete Actions
im Environment bestärkt diese Auswahl.

% Umsetzung Deep Q erläutern
%   - wichtige Methoden klären (mal in den Quelltext gucken)
%   - SummaryWriter
%   - Ablauf des Trainings
%     - Hyperparameter
%     - zuerst Referenz Settings finden
%     - dann Erweiterungen zuschalten
%   - Trainingsdauer vergleichen mit anderen Experimenten

\subsection{Vanilla Deep Q-learning}
Initial erfolgte die Implementierung des Deep Q-learning Verfahrens ohne jede Form einer Erweiterung,
welche auch als Vanilla Deep Q-learning bezeichnet wird. Das Environment wurde dafür auf eine Auswahl
von vier Level beschränkt (siehe \autoref{sec:levels}), wobei diese sich in Ausrichtung und Aufbau
unterscheiden. Jedes Level besitzt neun einsammelbare Schilde (+1 Reward) und ein einsammelbares
Herz (+5 Reward), wodurch ein maximal zu erhaltender Reward von 14 entsteht. Hierzu ist jedoch zu
erwähnen, dass diverse Schilder nicht in der, zumindest zeitlich betrachteten, optimalen Laufbahn
des Agenten liegen, wodurch das Environment bereits bei Werten unter dem maximal Reward als
erfolgreich absolviert anerkannt wird. Aus dieser Feststellung ergibt sich folgende
Lösungsdefinition:

\textit{Sobald das Teeworlds Environment einen durchschnittlichen Gesamtreward von über 12 in allen
vier Level erzielt und der Agent abschließend das Herz einsammelt, gilt die Problemstellung als 
gelöst.}

Das Environment wurde aus Gründen der Performance ohne zusätzliche Beschleunigung des Spiels
ausgeführt, wobei jeweils 4 Worker (siehe \nameref{sec:architecture-env}) simultan den
explorierenden Anteil im Environment vollziehen. Das entwickelte Gesamtsystem wurde für den
Trainingsprozess auf einem PC (Spezifikation siehe \autoref{sec:specs}) mit CUDA Unterstützung
für 38 Stunden ausgeführt und sammelte dabei 2.25 Millionen Frames. Die nachfolgenden Graphen
veranschaulichen den, während des Trainingsprozesses erhaltenen durchschnittlichen Reward sowie
den, für den Verlauf des Explorationprozesses verwendeten $\epsilon$ Wert (\ref{fig:vanilla-dqn}):

\begin{figure}[htp]
\centering
\includegraphics[scale=0.25]{res/vanilla_dqn.png}
\includegraphics[scale=0.35]{res/epsilon_vanilla_dqn.png}
\caption{Ergebnisse Vanilla Deep Q-learning}
\label{fig:vanilla-dqn}
\end{figure}

\newpage
Die Abbildung veranschaulicht, dass nach dem Abfall von Epsilon auf 0.02 der gesammelte Reward deutlich
ansteigt. Nach 2.2 Millionen gespielten Frames erreicht das Verfahren seinen Bestwert bei einem
durchschnittlichen Reward von 12.9. Aufgrund der zuvor festgelegten Lösungsdefinition ist dieses
Ergebnis als ein erfolgreiches Lösen der definierten Problemstellung einzuordnen.

\subsection{Erweiterungen}
In den anschließenden Unterabschnitten erfolgt die Erläuterung und Auswertung zu den
Erweiterungen der Standardimplementation des Deep Q-learning Verfahrens. Die Erweiterungen
wurden dabei ebenfalls auf den zuvor festgelegten Levels für eine annähernd gleiche Anzahl
an gesammelten Frames trainiert.

Sämtliche Erweiterungen wurden als optionale Einstellungen im Trainingsprozess implementiert,
damit diese Schrittweise hinzugefügt werden können, um einen guten Vergleich der daraus
resultierenden Ergebnisse zu erhalten.

\subsubsection{Noisy Network}
Als erste Erweiterung wurde eine Maßnahme zur Verbesserung der Exploration des Environments implementiert.
In dem Paper ''Noisy Networks for Exploration'' beschreiben Fortunato et al.\cite[~S.1 f.]{FAPMOGM2017}
ein Verfahren zum Erlernen der Explorationsmerkmale während des Trainings, statt auf die Verwendung von
Explorationsstrategien zurückzugreifen, wie $\epsilon$-greedy. Durch die Hinzunahme von Rauschen (noise)
zu den Gewichten der Fully-Connected Layer und dem anschließenden Anpassen der Gewichtsparameter des
Rauschens durch Backpropagation wird ein effizientes Explorieren möglich. Das Verfahren ist laut der
Authoren des Papers verhältnismäßig simpel in dessen Umsetzung und fügt ausschließlich minimal erhöhten
Overhead zur Vanillaversion des Deep Q-learning Verfahrens hinzu. Als wichtigste Erkenntnis wird im Paper
beschrieben, dass eine einzige Änderung des Gewichtsvektors eine konsistente und potenziell sehr komplexe
zustandsabhängige Änderung der Policy über mehrere Zeitschritte hinweg induzieren kann, wohingegen bei
Dithering-Ansätzen\footnote{einfachste und am häufigsten verwendete Ansätze zur Erforschung von greedy
Actions durch den Einsatz von zufälligem Dithering \cite[~S.2]{ORRW2019}}, dekorreliertes (und im Falle von
greedy Verfahren, zustandsunabhängiges) Rauschen bei jedem Schritt zur Policy, sprich der Wahl einer
Action, hinzugefügt wird \cite[~S.2]{FAPMOGM2017}. 

\paragraph*{Implementierung}
\noindent
\newline
Die nachfolgende Implementierung orientiert sich an der Vorlage nach Lapan\cite[~S.179]{L2018}.
Zur Umsetzung des Noisy Layer wird die Klasse \textit{NoisyLayer} implementiert, die von der
Superklasse \lstinline!nn.Linear! erbt. Listing \ref{lst:noisy-layer} definiert den Konstruktor
der Klasse, in dem initial eine trainierbare Gewichtsmatrix $\sigma$ erzeugt wird. Durch den
Aufruf der \lstinline!self.register_buffer! Methode wird ein Tensor für das Netz erzeugt, welcher
während des Trainings nicht durch Backpropagation aktualisiert wird. Gleiches gilt für die noisy
Biases des Netzwerks. Der Startwert von sigma wird wie im Paper empfohlen \cite[~S.6]{FAPMOGM2017}
auf $\sigma=0.017$ gesetzt.

\begin{lstlisting}[language=Python, caption=Konstruktor Noisy Layer, label=lst:noisy-layer]
class NoisyLinear(nn.Linear):
	def __init__(self, in_features, out_features, sigma_init=0.017, 
	bias=True):
		
		super(NoisyLinear, self).__init__(in_features, out_features,
		bias=bias)
		
		self.sigma_weight = nn.Parameter(torch.full((out_features,
		in_features), sigma_init))
		
		self.register_buffer("epsilon_weight", 
		torch.zeros(out_features, in_features))
		
		if bias:
    		self.sigma_bias = nn.Parameter(torch.full((out_features,)
    		,sigma_init))
    		
    		self.register_buffer("epsilon_bias", 
    		torch.zeros(out_features))
    		
		self.reset_parameters()
\end{lstlisting}

Durch den Aufruf von \lstinline!self.reset_parameter! (\ref{lst:noisy_reset-params}) werden die
Gewichte und Biases nach der im Paper beschriebenen Vorhergehensweise initialisiert. Dabei wird
jedem Gewicht ein Wert einer unabhängigen Gleichverteilung
$\mathcal{U}[-\sqrt{\frac{3}{p}},+\sqrt{\frac{3}{p}}]$ zugewiesen, wobei $p$ der Anzahl der
Input Nodes des Linear Layers entspricht.
 
\begin{lstlisting}[language=Python, caption=Reinitialisierung der Gewichte und Biases, 
label=lst:noisy_reset-params]
	def reset_parameters(self):
		std = np.math.sqrt(3 / self.in_features)
		self.weight.data.uniform_(-std, std)
		self.bias.data.uniform_(-std, std)
\end{lstlisting}

Die \lstinline!forward(self, input)! Funktion wird jedes mal aufgerufen, wenn mit der Instanz
des Netzes die Berechnung des Outputs vollzogen wird. Durch den Aufruf von 
\lstinline!normal_()! der Gewichte und Biases werden für beide Parametermengen 
\textit{Random Noise} Werte gesampled. Anschließend erfolgt mittels \lstinline!F.linear()!
die lineare Transformation der Input Daten, wie in der \lstinline!nn.Linear! Klasse 
\cite[~S.179 ff.]{L2018}.

\begin{lstlisting}[language=Python, caption=\textit{forward()} Funktion Noisy Layer, 
label=lst:noisy_forward]
	def forward(self, input):
	self.epsilon_weight.normal_()
	bias = self.bias
	if bias is not None:
	    self.epsilon_bias.normal_()
	    bias = bias + self.sigma_bias * self.epsilon_bias.data
		
	return F.linear(input, self.weight + self.sigma_weight *
	self.epsilon_weight.data, bias)
\end{lstlisting}

Die Verwendung der Layer funktioniert anschließend genau wie bei der Erstellung des Netzes mit
Linear Layer, wobei diese durch die \textit{NoisyLayer} Klasse ausgetauscht werden.

\paragraph*{Auswertung}
\noindent
\newline
Der Trainingsprozess nach dem Einbinden der Erweiterung beträgt 36 Stunden, wodurch 2.13 Millionen
Frames aufgezeichnet wurden. Durch die Einbindung der Noisy Layer wird der Trainingsprozess
deutlich stabiler, was mit der alternativen Form der Exploration des Environments erklärt werden
kann. Abschließend konnte durch die Einbindung der Noisy Layer Erweiterung, ein
gleichbleibenderes Trainingsergebnis erzielt werden, jedoch konnte der trainierte Agent
das Level \textit{new\_level\_02} nicht vollständig lösen. In \autoref{fig:noisy-dqn}
wird erneut die Entwicklung des durchschnittlichen Rewards des Noisy Deep Q-learning
Verfahrens (blau) im Vergleich zur Entwicklung des Vanilla Verfahrens (rot) veranschaulicht:

\begin{figure}[htp]
\centering
\includegraphics[scale=0.38]{res/noisy_dqn.png}
\caption{Avg. Reward: Noisy (blau) im Vergleich zu Vanilla DQN (rot)}
\label{fig:noisy-dqn}
\end{figure}

\subsubsection{Double DQN}
Die nächste Erweiterung zur Vanillaimplementierung von Deep Q-learning befasst sich mit der
Stabilität des Verfahrens. In dem Paper ''Deep Reinforcement Learning with Double Q-learning''
von DeepMind\cite{HGD2015} wird das Überwerten von Q-values bei dem Eintreten definierter
Zuständen überprüft. In dem Paper werden genau diese Overestimations bei dem Trainieren auf
Atari 2600 Spielen mittels Vanilla Deep Q-learning festgestellt und es wird eine Lösung mit
der Bezeichnung \textit{Double Deep Q-learning} vorgestellt.

Grund für die Überbewertung ist die Nutzung des $\max$ Operators in der Bellman Equation.
Wie bereits in \autoref{sec:tab-q-learning}:\nameref{sec:tab-q-learning} und \autoref{sec:dqn}:
\nameref{sec:dqn} behandelt wird diese zur Berechnung der Q-values verwendet. Da beide
Verfahren die gleichen values sowohl zur Auswahl, als auch zur Evaluierung von Actions nutzen,
ist es laut Hasselt et al.\cite[~S.2]{HGD2015} wahrscheinlicher, dass überbewertete values
ausgewählt werden, was wiederum zu einer überoptimistischen Schätzung zu\-künf\-ti\-ger values
führen kann. Um diesem Problem entgegenzuwirken können die values zur Auswahl von Actions von
denen zur Evaluierung entkoppelt werden. In der Vanilla Ausführung des Deep Q-learning
Verfahrens wird die Berechung der Q-values wie folgt realisiert:
\begin{align}
Q(s_t, a_t) = r_t + \gamma\max_{a}Q'(s_{t+1},a_{t+1})
\label{aln:double-dqn-q-values}
\end{align}
Dabei wird $Q'(s_{t+1}, a_{t+1})$ durch den Aufruf des Target Netz (siehe 
\autoref{sec:deepq-procedure}) berechnet, welches zyklisch bei dem Erreichen einer Anzahl von
$n$ Schritten des Trainingsvorgangs aktualisiert wird. Als Alternative dazu wird im Paper
folgende Rechnung vorgestellt:
\begin{align}
Q(s_t, a_t) = r_t + \gamma\max_{a}Q'(s_{t+1},\argmax_{a}Q(s_{t+1},a))
\label{aln:double-dqn-q-values-improved}
\end{align}
Anders als im initialen Ansatz erfolgt hier die Wahl der Action für den nächsten State $s+1$
durch das trainierte Netz $Q$, die Berechnung der Q-values wird hingegen weiterhin durch das
Target Netz $Q'$ vollzogen.

\paragraph*{Auswertung}
\noindent
\newline
Durch die Einbindung der Double Deep Q-learning Erweiterung konnten deutliche
Verbesserungen im durchgeführten Trainingsprozess erzielt werden. Anders als in der zuvor
realisierten Erweiterung konnte der Agent alle Level erfolgreich durchspielen, was als
eine Verbesserung des Verfahrens betrachtet werden kann. Die Dauer des Trainingsprozesses
wurde durch die Anpassung mit einer benötigten Laufzeit von 41 Stunden für die Generierung
von 2.5 Millionen Frames geringfügig erhöht. In der nachfolgenden Abbildung wird der
durchschnittlich erhaltene Reward für das Double DQN Verfahren veranschaulicht. Der darin
grün dargestellte Graph beschreibt den Verlauf des Double DQN Verfahren in Kombination
mit dem zuvor beschriebenen Verfahren Noisy Net, wohingegen der blaue Graph den Verlauf
der bereits behandelten Erweiterung ohne das Double Verfahren visualisiert.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.38]{res/double_dqn_results.png}
\caption{Avg. Reward: Double DQN und Noisy (grün) im Vergleich zu Noisy (blau)}
\label{fig:double-dqn}
\end{figure}

In Abbildung \ref{fig:double-dqn-value-mean} sind die durchschnittlichen value Vorhersagen
des Agenten visualisiert. Obwohl das Double DQN Verfahren in grün die Overestimation der
values verhindert, liegen die durchschnittlichen vorhergesagten Werte des Agenten mit
Double DQN höher als beim Vergleichswert ohne Double DQN (blau). Erfolgt der Vergleich mit
dem Vanilla Verfahren (rot), ist zu sehen, dass die mean values niedriger ausfallen. Für
eine abschließende Untersuchung wäre ein Vergleich zwischen Vanilla DQN und Double DQN
ohne zusätzliche Erweiterungen sinnvoll.

\begin{figure}[htp]
  \centering
  \includegraphics[scale=0.38]{res/double_dqn_values_mean.png}
  \caption{Avg. values Double DQN}
  \label{fig:double-dqn-value-mean}
\end{figure}

\subsubsection{Prioritized Replay Buffer}
Die nächste Erweiterung befasst sich mit der Zwischenspeicherung von Samples, die
anschließend im Trainingsprozess verwendet werden. Während die normale Trainingsprozedur
zufällig Samples aus dem Replay Buffer auswählt und trainiert, wurde von T. Schaul und
anderen \cite{schaul2015prioritized} eine Methode vorgeschlagen, bei der die zu
trainierenden Samples abhängig von dem von ihnen produzierten Loss ausgewählt werden.
Dadurch werden Samples, die eine neue Information beinhalten häufiger in Betracht
gezogen, sodass diese schneller genutzt werden können.

Nach Lapan \cite[~S. 282]{L2018} speichert der Replay Buffer neben den Samples eine
Priorität für jeden Eintrag. Diese Priorität wird nach dem Training proportional zum
entstandenen Loss aktualisiert. Wird ein neues Sample hinzugefügt, so bekommt dieses
Sample eine maximale Priorität, sodass neue Samples bevorzugt werden. Die
Wahrscheinlichkeit $P(i)$, dass der i-te Sample für das Training verwendet wird, berechnet
sich aus \autoref{aln:bufprio}.
\begin{align}
  P(i) = \frac{p_i^\alpha}{\sum_k p_k^i}
\label{aln:bufprio}
\end{align}
\noindent
Der Hyperparameter $\alpha$ entscheidet dabei darüber, wie stark höher priorisierte
Samples bevorzugt werden.

Durch dieses Verfahren erhalten manche Samples eine größere Aufmerksamkeit, wodurch es
nach \cite{schaul2015prioritized} dazu kommen kann, dass das neuronale Netz auf die
priorisierten Samples overfitted. Um dem entgegenzuwirken, bekommt jedes Sample ein
weight $w_i$, mit dem der Loss multipliziert wird, sodass der overfitting Effekt nicht
mehr auftritt. Der Wert für $w_i$ wird nach Formel \ref{aln:weight_bufprio} bestimmt.
\begin{align}
  w_i = \left(N * P(i)\right)^{-\beta}
\label{aln:weight_bufprio}
\end{align}

Der neue Hyperparameter $\beta$ bestimmt dabei, wie stark der Einfluss der Samples
verringert wird. In dem Paper von Schaul wird empfohlen $\beta$ mit 0.6 zu initialisieren
und anschließend langsam bis 1 wachsen zu lassen.

\paragraph*{Implementierung}
\noindent
\newline
Für die Implementation des Prioritized Replay Buffers wurde eine neue Klasse
implementiert. Im Gegensazu zum ursprünglichen Replay Buffer speichert die Erweiterung
neben den erfahrenen Samples Prioritäten, die Auskunft darüber geben, wie stark das
jeweilige Sample bevorzugt wird.

Um neue Samples hinzuzufügen, wurde die \lstinline!append()! Methode geschrieben, die in
Listing \ref{lst:prio_buf_append} beschrieben ist.
\begin{lstlisting}[language=Python, caption=append()-Funktion des PrioritizedReplayBuffers,
label=lst:prio_buf_append]
class PrioritizedReplayBuffer:
  # ...
  def append(self, experience):
    max_priority = self.priorities.max() if self.buffer else 1.0

    if len(self.buffer) < self.capacity:
        self.buffer.append(experience)
    else:
        self.buffer[self.pos] = experience

    self.priorities[self.pos] = max_priority
    self.pos = (self.pos + 1) % self.capacity
  # ...
\end{lstlisting}
\noindent
Neben dem hinzufügen des Samples in \lstinline!self.buffer! wird die Priorität des neuen
Samples gesetzt. Neu hinzugefügte Samples bekommen die maximale Priorität, die entweder
die höchste bisherige Priorität ist oder, falls noch kein Sample vorhanden ist, der Wert
$1.0$. Der Index \lstinline!self.pos! bestimmt die Position an die das nächste Samples
hinzugefügt wird.

Um die Prioritäten der Samples zu erneuern, werden nach dem Training die Prioritäten auf
den durch die Samples erzeugten Loss gesetzt. Dadurch werden Samples häufiger zum
Training benutzt, die einen hohen Loss erzeugt haben.

Um anschließend die ermittelten Prioritäten zu nutzen, werden diese in der
\lstinline!sample()!-Funktion des Replay Buffers berücksichtigt. Eine vereinfachte Version
der dieser Funktion ist in Listing \ref{lst:prio_buf_sample} gezeigt.
\begin{lstlisting}[language=Python, caption=sample()-Funktion des PrioritizedReplayBuffers,
label=lst:prio_buf_sample]
class PrioritizedReplayBuffer:
  def sample(self, batch_size, beta=0.4):
    probs = self.priorities ** self.prob_alpha
    probs /= probs.sum()

    indices = np.random.choice(len(self.buffer),
                               batch_size,
                               p=probs)
    samples = zip(*[self.buffer[idx] for idx in indices])

    weights = (len(self.buffer) * probs[indices]) ** (-beta)
    weights /= weights.max()

    return samples, weights, indices
\end{lstlisting}
\noindent
Zuerst werden die Wahrscheinlichkeiten \lstinline!probs!, dass die einzelnen Samples
trainiert werden aus den Prioritäten berechnet. Anschließend findet das zufällige
auswählen der Samples mit der Funktion \lstinline!np.random.choice()! statt. Am Ende
werden die Gewichte berechnet, mit denen der Loss Wert beim Training skaliert wird. Diese
Gewichte werden zusammen mit den ausgesuchten samples und deren Indexen zurück gegeben.
Die Indexe werden benötigt, um die Prioritäten der Samples nach dem Training wieder
anpassen zu können.

\paragraph*{Auswertung}
Abbildung \ref{fig:prio_buf_results} zeigt die Ergebnisse des Prioritized Replay Buffers
in grau im Vergleich zu einem Experiment ohne Replay Buffer in grün, wobei die über 10
Episoden gemittelten Rewards erneut abgebildet sind. Der Trainingsprozess wurde für 37
Stunden ausgeführt, wobei 2.2 Millionen Samples gesammelt wurden.
\begin{figure}[htp]
\centering
\includegraphics[scale=0.3]{res/prio_buffer_results.png}
\caption{Ergebnisse Prioritized Replay Buffer}
\label{fig:prio_buf_results}
\end{figure}
\noindent
Wie zu erkennen ist, hat sich das Ergebnis durch diese Erweiterung verschlechtert. Der
durchschnittliche Reward ist geringer ausgefallen und nicht alle Levels konnten
erfolgreich absolviert werden. Es lässt sich vermuten, dass die Priorisierung einzelner
Samples für das entwickelte Environment nicht eignet ist oder das zufällige Abweichungen
beim Trainingsprozess zu einem schlechteren Ergebnis geführt haben.

\subsubsection{Dueling DQN}
Die Dueling DQN Erweiterung ist inspiriert von der Actor-Critic Methode und wurde von
Z. Wang \cite{wang2015dueling} vorgestellt. Wie auch in Actor-Critic werden die Q-values
nicht direkt berechnet, sondern der value des States wird separat berechnet und ein
Advantage Wert für jede Action hinzugerechnet. Daraus ergibt sich der Zusammenhang
$Q(s, a) = V(s) + A(s, a)$, der schon bei Actor-Critic behandelt wurde. Jede Action erhält
eine Advantage, die aussagt, wieviel besser oder schlechter diese Action im Vergleich zu
den anderen Actions ist. Dabei gilt die Einschränkung, dass die Summe der Advantage Werte
auf 0 liegen soll. Um diese Charakteristik umzusetzen, wird der Durchschnitt der Advantage
Werte von den Advantage Werten abgezogen, sodass deren Durchschnitt 0 entspricht, wie in
\autoref{aln:dueling_q_values} gezeigt wird.

\begin{align}
  Q(s, a) = V(s) + A(s, a) - \frac{1}{N} \sum_{k=0}^{N-1} A(s, k)
  \label{aln:dueling_q_values}
\end{align}

\paragraph*{Implementierung}
\noindent
\newline
Wie auch in Actor-Critic kann dies implementiert werden, indem nach den Convolutional
Layern des Netzes zwei distinkte Netze angeschlossen werden. Abbildung
\ref{fig:dueling_dqn_nets} veranschaulicht die Netzarchitektur.
\begin{figure}[htp]
  \centering
  \includegraphics[scale=0.30]{res/dueling_dqn_nets.png}
  \caption{Dueling Netzarchitektur}
  \source{\cite[~S.283 - Chapter 7 - Figure 11]{L2018}}
  \label{fig:dueling_dqn_nets}
\end{figure}
\noindent
Die obere Abbildung zeigt den ursprünglichen Netzaufbau, während die untere Abbildung die
Architektur des Dueling Deep Q-Networks zeigt. Die Implementation ist in Listing
\ref{lst:dueling_dqn_net} gezeigt.
\begin{lstlisting}[language=Python, caption=Dueling Net Implementation,
label=lst:dueling_dqn_net]
class DuelingNet:
  def __init__(self, ...):
    # init convolutional layers as usual...

    # init advantage net
    self.fc_adv = nn.Sequential(
      self.layers[0],
      nn.ReLU(),
      self.layers[1]
    )
    # init value net
    self.fc_val = nn.Sequential(
      self.layers[2],
      nn.ReLU(),
      self.layers[3]
    )

  def forward(self, x):
    conv_out = self.conv(fx)  # calculate convolutional layers

    val = self.fc_val(conv_out)  # calculate value
    adv = self.fc_adv(conv_out)  # calculate advantage
    return val + adv - adv.mean()  # calculate Q-values
\end{lstlisting}
\noindent
In der \lstinline!__init__()!-Funktion des Dueling Netzes werden neben den Convolutional
Layern, die wie gewöhnlich initiatisiert werden, zwei weitere Netze initialisiert, von denen
das erste die Advantage Werte und das zweite den value des States berechnet.
In der \lstinline!forward()!-Funktion wird der Output der Convolutional Layer
als Input für das Advantage und das Value net verwendet. Anschließend wird, wie schon
angesprochen, der Durchschnitt des Advantage Wertes abgezogen, um zu verhindern, dass der
value des States über die Advantage Werte ausgedrückt wird.

\paragraph*{Auswertung}
\noindent
\newline
Abbildung \ref{fig:dueling_dqn_results} zeigt in blau den Reward Verlauf eines Agenten,
der in Kombination mit den anderen vorgestellten Erweiterungen das DuelingNet Verfahren
verwendet. Als Vergleichswert dient ein Agent, dessen Reward in der Abbildung grau
dargestellt wird, welcher ebenfalls alle vorhergehenden Erweiterungen benutzt, 
jedoch nicht die DuelingNet Erweiterung umsetzt.
\begin{figure}[htp]
  \centering
  \includegraphics[scale=0.25]{res/dueling_dqn_results.png}
  \caption{Ergebnisse DuelingNet}
  \label{fig:dueling_dqn_results}
\end{figure}
\noindent
Wie man aus der Entwicklung des Rewards erkennen kann, erreicht der Agent, der das
DuelingNet benutzt einen höheren maximalen Reward von 13.1 und lernt das Environment mit
weniger Samples (2.1 Millionen Samples in 38.5 Stunden). Alle Level der Problemstellung
konnten erfolgreich durchgespielt werden. Des Weiteren erreichte das Verfahren zum Ende
des Trainingsprozesses durchgängig konstante Ergebnisse, wobei der Reward über eine Stunde
einen Schwellwert von 11 nicht unterschritt.

\subsubsection{N-step DQN}
Eine zusätzliche Verbesserung kann im Deep Q-learning Verfahren erziehlt werden, indem die
Bellman Equation erneut angepasst wird. Das Verfahren wird im Paper \textit{Learning to
Predict by the Methods of Temporal Differences} von Richard Sutton\cite{S1988} initial
vorgestellt und erweitert den herkömmlichen Ansatz zur Berechnung von Q-values um die
Betrachung weiterer zukünftiger Schritte. Da die Ausgangsgleichung zur Berechnung der Q
values (\ref{aln:double-dqn-q-values}) rekursiv formuliert wurde, kann $Q(s_{t+1},
a_{t+1})$ ebenfalls wie folgt formuliert werden, wobei der Ausdruck in den eckigen
Klammern einer Auflösung von $Q'$ entspricht \cite[~S.168]{L2018}:
\begin{align}
Q(s_t, a_t) = r_t + \gamma \max_{a}[r_{a,t+1}+\gamma\max_{a'}Q(s_{t+2},a')]
\label{aln:nstep-dqn-q-values-improved}
\end{align}
Wird davon ausgegangen, dass die Action $a$ im Schritt $t+1$ nahezu optimal ausgeführt wird,
kann die $\max_{a}$ Operation umgeformt werden, wodurch die folgende Gleichung
(\ref{aln:nstep-dqn-q-values-improved-dissolved}) entsteht \cite[~S.168]{L2018}:
\begin{align}
Q(s_t, a_t) = r_t + \gamma r_{t+1}+\gamma^2 \max_{a'}Q(s_{t+2},a')
\label{aln:nstep-dqn-q-values-improved-dissolved}
\end{align}
Da diese Gleichung wiederum einen rekursiven Aufruf enthält, wird es möglich weitere
Auflösungsschritte vorzunehmen. Dadurch können Q-values gleich beim Erhalt eines lokalen
Rewards über die $n$ vergangenen Schritte, die zum Erreichen dieses Rewards geführt haben,
aktualisiert werden. Dies ist bei der in \autoref{aln:double-dqn-q-values} definierten
Vorhergehensweise nicht möglich, da ausschließlich \textit{ein} zukünftiger Schritt geschätzt
wird und die Aktualisierung lediglich auf dem Q value des Ausgangszustandes angewandt wird. 
Der Hauptvorteil dieser Erweiterung liegt in der Beschleunigung des Trainingsprozesses.

In die schlussendliche Realisierung des Agenten wurde diese Erweiterung jedoch nicht
aufgenommen, da zum Auflösen zukünftiger Schritte Aneinanderreihungen von Actions im
Environment ausgeführt werden müssen. Dazu hätten des Weiteren Kopien der aktuellen
Zustände des Environments für die jeweiligen Auflösungsschritte $n$ angelegt werden
müssen, von welchen das Spiel anschließend weitergelaufen wäre. Aufgrund der
Echtzeiteigenschaft des Environments wurde abschließend jedoch von einer Implementierung
abgesehen.

\subsection{Erkentnisse}
\label{sec:results}
In der finalen Umsetzung des Agenten wurde es bewerkstelligt mehrere Level im Environment
erfolgreich zu absolvieren. Neben der Testreihe mit vier Leveln wurde zusätzlich ein
Experiment mit sechs Leveln vollzogen, welches ebenfalls ein erfolgreiches Ergebnis
lieferte. Des Weiteren wurde mit dem Experiment ()%TODO ref
festgestellt, dass der Agent ein bisher unbekanntes Level (nicht) erfolgreich durchspielen
konnte. Daran lässt sich beurteilen, dass das Verfahren (zwischen unterschiedlichen
Situationen abstrahiert) overfitted und die zu spielenden Levels (nicht) ''auswendig'' lernt.

Aufgrund der Tatsache, dass das in Experiment () % TODO ref
beschriebene ''schwerere'' Level durchgespielt werden konnte, lässt sich ableiten, dass sich
das Verfahren ebenfalls in komplizierteren Aufgabenstellungen einsetzen lässt.

Während der Realisierung des Environments wurden Experimente mit unterschiedlicher Beschleunigung
des Spiels durchgeführt. Dabei stellte sich heraus, dass eine hohe Beschleunigung des Spiels
schlechtere Trainingsergebnisse hervorbrachte. Ein möglicher Grund für diese Verschlechterung ist,
dass durch die Echtzeitkomponente des Spiels, das Spiel bereits weiter lief, während der Agent noch
die auszuführende Action berechnete. Diese Erkenntnis wurde während der Durchführung der
Kalibrierungsexperimente (siehe ) %TODO ref
ermittelt.

\subsection{Fazit}
In der Projektarbeit wurden Grundlagen im Bereich Reinforcement Learning, theoretische Hintergründe
zu Verfahren sowie Implementierungen von Algorithmen des Reinforcement Learning betrachtet. Es
erfolgte die Beleuchtung und Abgrenzung einzelner Verfahren und die Auswertung der daraus
entstandenen Ergebnisse, wodurch Limitierungen festgestellt wurden.

Nach der Aufabreitung der theoretischen Grundlagen wurde ein Environment entwickelt, dessen
Umsetzung die von OpenAI festgelegte Interfacedefinition implementiert. Zusätzlich wurde eine
parallele Worker Architektur umgesetzt, mit welcher das Environment für weitere Verfahren
zugänglich gemacht wurde. Beide Maßnahmen erlauben es anderen Entwicklern einen Agenten für
das Environment zu entwerfen.

Mittels des entwickelten Environments wurde ein Agent mit Deep Q-learning trainiert, welcher
abschließend die Problemstellung des Environments erfolgreich löste.

\subsection{Ausblick} % TODO Philipp
Durch die Implementierung des Teeworld Environments besteht anschließend die Möglichkeit weitere Verfahren
für die Lösung der definierten Problemstellung des Environments umzusetzen. Besonders aufgrund der
Implementierung der Worker Struktur und dem daraus resultierenden parallelen Sammeln von Trainingsdaten
wird es möglich on-policy Verfahren wie die Actor-Critic Methode in der bestehenden Realisierung umzusetzen.
Des Weiteren existieren auf seiten des Environments bereits Funktionalitäten zur Veränderung der
Ausrichtung des Greifhaken sowie der aktuell ausgerüsteten Waffe. Aufbauend auf diesen Funktionen könnte
das System um die Erkennung von Lebenspunkten und Trefferpunkten erweitert werden, wodurch die
Mehrspielerkomponente des Spiels als nächster Forschungsschritt betrachtet werden könnte. Damit
einhergehend wäre die Verarbeitung von kontinuierlichen Actions, die durch Nutzung von Waffen oder dem
Greifhaken notwendig werden würden. Ein vielversprechendes Verfahren für diesen Einsatzbereich wäre
das im Paper \textit{Continuous control with deep reinforcement learning} von Lillicrap et al.
\cite{LHPHETSW2016} vorgestellte Deep Deterministic Policy Gradients Verfahren.

Ein weiterer Schritt um die Stabilität und Geschwindigkeit des Environments zu erhöhen wäre die
Realisierung eines Off-Screen Rendering im Spiel. Dadurch würden weniger Ressourcen während der
Laufzeit des Spiels durch dieses benötigt werden und ebenfalls das zyklische Aufnehmen von
Screenshots würde überflüssig werden.

Um die Komplexität der Problemstellung zu erhöhen und das bestehende Verfahren Deep Q-learning mit
sämtlichen Erweiterungen noch ausführlicher zu testen, wurde neben dem definierten Teststandard von
vier Levels, wie im \autoref{sec:results} beschrieben, die Hinzunahme von zwei weiteren Levels
realisiert. Des Weiteren erfolgte die Umsetzung des Verfahrens auf ausschließlich einem, als eher
sehr schwer einzustufenden, Level. Dieser Prozess könnte fortgesetzt werden, um die maximale
Kapazität an lernbaren Levels zu ermitteln oder mittels einer Vergrößerung der Levelstruktur, die
Anwendbarkeit im Bezug auf die Größe der Levels und deren Komplexität zu ermitteln.

%TODO mehrere Rechner als Teststrecke
%TODO weitere Erweiterungen zu DQN
%TODO grenzen von DQN herausfinden 

\newpage
%\bibliographystyle{plain}
\bibliographystyle{alpha}
\bibliography{references}
\newpage
\listoffigures 
\listoftables 

\newpage
\section{Anhang}

\subsection{Ergebnisse Pong Environment mit Deep Q-learning}
\begin{figure}[htp]
\centering
\includegraphics[scale=0.35]{res/pong_all_results.png}
\caption{Vollständige Ergebnisse mit Deep Q-learning im Pong Environment}
\label{fig:pong_all_results}
\end{figure}

\subsection{Die benutzen Level}
\label{sec:levels}
\begin{figure}[htp]
\centering
\includegraphics[width=160pt]{res/newlevel_1_full.png}
\includegraphics[width=160pt]{res/newlevel_2_full.png}
\newline
\hspace{10pt}\\
\includegraphics[width=160pt]{res/newlevel_3_full.png}
\includegraphics[width=160pt]{res/newlevel_4_full.png}

\caption{
  Die Level ''newlevel\_0'', ''newlevel\_1'',
            ''newlevel\_2'' und ''newlevel\_3''
        }
\end{figure}

\subsection{Spezifikation PC Training}
\label{sec:specs}
\begin{table}[ht]
  \begin{center}
    \begin{tabular}{l | l}
      \textbf{Hardware} & \textbf{Specification} \\
      \hline
      GPU & Nvidia GeForce GTX 970 (4034MiB VRAM)\\
      CPU & Intel(R) Xeon(R) E-2136 CPU @ 3.30GHz; 6 Kerne, 12 Threads\\
      RAM & DDR4 31.3GB; 2400MT/s
    \end{tabular}

    \caption[PC Specifikation]{Trainings PC Hardware Spezifikation}
    \label{tab:pc_specs}
  \end{center}
\end{table}


\end{document}
