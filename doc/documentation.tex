\documentclass[11pt]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[german]{babel}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{subcaption}
\newcommand{\lbparagraph}[1]{\paragraph*{#1}\mbox{}\\}
\renewcommand{\baselinestretch}{1.2}
\geometry{
  left=3cm,
  right=3cm,
  top=3cm,
  bottom=3cm,
  bindingoffset=5mm
}
\usepackage{graphicx}

\begin{document}

\begin{titlepage}
	\centering
	{\scshape\LARGE Hochschule für Technik und Wirtschaft Berlin \par}
	\vspace{2cm}
	{\Huge \scshape{Independent Coursework}\par}
	\vspace{2cm}
	{\LARGE \scshape{Reinforcement Learning}\par}
	{\scshape\Large Projekt Dokumentation\par}
	%\includegraphics[width=0.9\linewidth]{title.jpg}\par\vspace{1cm}
	\vspace{4cm}
	{\large\itshape Bruno Schilling,\\Philipp Bönsch\par}
	\vfill
	
% Bottom of the page
	{\large Betreuer: Prof. Dr.-Ing. Barthel \par}
	\vspace{1cm}
	{\large \today\par}
\end{titlepage}

\lstset{basicstyle=\ttfamily\small,breaklines=true}
\newpage
\tableofcontents
\newpage
\section{Expose}
\begin{itemize}
\item evtl hier nochmal das expose, vllt reicht Introduction aus
\end{itemize}
\section{Introduction}
\begin{itemize}
\item Zielstellung, Art des IC's, Vorwissen?
\end{itemize}
\section{Fundamentals}
\subsection{Reinforcement Learning}
\begin{itemize}
\item Agent, Environment, Reward, Observation, Action
\item Exploration vs. Exploitation
\item model-free vs. model-based
\item on- vs. off-policy
\item policy learning vs. value learning
\end{itemize}
\subsection{Markov Decision Process}
\begin{itemize}
\item Markov Process
\item Markov Reward Process
\item Markov Decision Process 
\end{itemize}
\subsection{K-Armed Bandit}
\begin{itemize}
\item Exploration
\item $\epsilon$ greedy
\item Tic Tac Toe (full greedy)
\end{itemize}
\subsection{Open AI Gym}
\begin{itemize}
\item Erklärung und Verwendung (Environment, Observation Space, Action Space) 
\end{itemize}
\section{Q-Learning}
\begin{itemize}
\item value learning* explanation
\item Bellman Equation
\item Frozen Lake
\item V(s) and Q(s,a)
\item Simple Explanation (2x2 Nonslip)
\item Vergleich von V(s) und Q(s,a) mit Charts?
\end{itemize}
\section{Deep Q-Learning}
\begin{itemize}
\item off-policy
\item again value learning but w/ network explanation
\item value network
\item replay buffer, target network
\item atari pong
\item roboschool pong environment
\item limitations
\item extensions
\end{itemize}

\section{Policy Gradients}
\begin{itemize}
\item cross entropy method, REINFORCE
\item continuous observation space \& action space
\item cart pole
\item atari pong?
\item limitations
\item high variance
\end{itemize}

\section{Actor Critic}
\begin{itemize}
\item actor and critic net
\item reducing variance $\rightarrow$ Advantage
\item extensions a3c
\end{itemize}

\section{Teeworlds}
\begin{itemize}
\item foss, teeworlds client, teeworlds server
\item custom environment (gym)
\item images as input $\rightarrow$ stack of images
\item simplified version
\item task $\rightarrow$ move to target position w/ movement and hooking?
\item implementation (explain different parts) listings
\item different algorithms?
\item many charts and discoveries
\item comparision?
\item setup, link to repos
\end{itemize}

\newpage
\bibliographystyle{plain}
\bibliography{references}
\newpage
\listoffigures 
\listoftables 
\end{document}