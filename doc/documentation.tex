% Allgemeine section Aufbau
% - Theorie
% - Einordnung (on-policy/off-policy, value-based/policy-based)
% - Implementationen
%   - Herausstellen wie Exploration und Exploitation umgesetzt ist
%   - falls vorhanden Graphs einbauen, die den Trainingsprozess veranschaulichen

\documentclass[11pt]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[german]{babel}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{listings}
\usepackage{graphicx}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\PassOptionsToPackage{hyphens}{url}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{subcaption}

\newcommand{\source}[1]{\vspace{-5pt} \caption*{\hfill \textbf{Source:} {#1}} }

\newenvironment{conditions}
  {\par\vspace{\abovedisplayskip}\noindent\begin{tabular}{>{$}l<{$} @{${}:{}$} l}}
  {\end{tabular}\par\vspace{\belowdisplayskip}}

\newcommand{\lbparagraph}[1]{\paragraph*{#1}\mbox{}\\}

\renewcommand{\baselinestretch}{1.2}
\geometry{
  left=3cm,
  right=3cm,
  top=3cm,
  bottom=3cm,
  bindingoffset=5mm
}


\usepackage{xcolor}
 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}


\begin{document}
% \sloppy

\begin{titlepage}
	\centering
	{\scshape\LARGE Hochschule für Technik und Wirtschaft Berlin \par}
	\vspace{2cm}
	{\Huge \scshape{Independent Coursework}\par}
	\vspace{2cm}
	{\LARGE \scshape{Reinforcement Learning}\par}
	{\scshape\Large Projekt Dokumentation\par}
	%\includegraphics[width=0.9\linewidth]{title.jpg}\par\vspace{1cm}
	\vspace{4cm}
	{\large\itshape Bruno Schilling,\\Philipp Bönsch\par}
	\vfill
	
% Bottom of the page
	{\large Betreuer: Prof. Dr.-Ing. Barthel \par}
	\vspace{1cm}
	{\large \today\par}
\end{titlepage}

\lstset{basicstyle=\ttfamily\small,breaklines=true}
\newpage
\tableofcontents
\newpage
\section{Introduction}

\subsection{Problemstellung}
Machine Learning Methoden werden zunehmend benutzt, um Probleme in der realen Welt zu
lösen und erhalten daher in der Zukunft einen immer höheren Stellenwert. Spiele im
speziellen stellen meist Problemstellungen dar, welche zum Teil auch schon durch Machine
Learning Verfahren absolviert werden konnten. Darunter fallen klassische Brettspiele, wie Go
oder Schach\cite{DM2018} aber auch Echtzeitspiele, wie Dota 2\cite{OA2019},
Quake\cite{DM2019} oder Starcraft\cite{DM2019_2}. Für das Supervised Research Independent Coursework
ergibt sich folgende Problemstellung: Welche Machine Learning Verfahren können für das
absolvieren eines Computerspieles verwendet werden und wie integriert man diese in die
virtuelle Umgebung?


\subsection{Zielsetzung}
Es soll innerhalb eines Supervised Research Independent Courseworks untersucht werden, wie
sich ein Agent in einer virtuellen Spielumgebung mit Hilfe von Machine Learning Methoden,
speziell Reinforcement Learning Verfahren, steuern lässt. Dafür werden initial
Untersuchungen durchgeführt, um den aktuellen Stand der Technik zu erfassen. Genauer
sollen Algorithmen mittels der von OpenAI Gym zur Verfügung gestellten Testumgebung
untersucht werden. Die dabei gesammelten Erfahrungen sollen benutzt werden, um einen
Agenten für ein Computerspiel zu entwickeln. Das Ziel des Research Projektes ist es, neue
Kenntnisse im Bereich Reinforcement Learning zu erhalten und bereits bestehendes Wissen in
den Schwerpunkten Artificial Intelligence und Machine Learning zu festigen. Weiterhin soll
dieses Wissen anschließend in einem Prototyp umgesetzt werden, bei dem ein Agent in einem
Echtzeitspiel gesteuert wird. In der ausgearbeiteten Dokumentation wird dabei sowohl auf die
Recherche und Aufarbeitung der theoretischen Teile von Reinforcement Learning als auch auf
die abschließende Implementierung der prototypischen Umsetzung eines Agenten eingegangen.
\newpage

\section{Grundlagen}
In diesem Abschnitt werden die Grundlagen des Machine Learning Bereichs Reinforcement Learning 
erläutert. Damit soll eine Grundlage für Themenbereiche in nachfolgenden Kapiteln geschaffen
werden, welche sich auf die Terminologie dieses Abschnittes beziehen.


\subsection{Reinforcement Learning}
Reinforcement Learning beschreibt eine Ansammlung von Machine Learning Verfahren, in denen die 
lernende Entität, auch \textit{Agent}, keine bezeichneten Beispiele für das Lösen eines Problemes 
erhält. Die einzige Information, welche mitgeteilt wird, ist Feedback zu gewählten Aktionen in Form
einer Belohnung oder Bestrafung. Diese Art des Feedbacks wird auch als Verstärkung bezeichnet.
Somit muss vom Lernenden selbstständig eine Strategie entwickelt werden, um das gegebene
Problem zu lösen, indem versucht wird den erhaltenen Belohnungswert zu maximieren. Die nachfolgenden
Kapitel beschreiben Bestandteile und Hauptakteure von Systemen im Bereich Reinforcement Learning.


\subsubsection{Agent}
Ein Agent nimmt seine Umgebung (\textit{Environment}) über Sensoren wahr und kann mittels Aktuatoren
Handlungen (\textit{Actions}) vollziehen um diese Umgebung zu beeinflussen oder zu verändern 
\cite[~S.60]{RN2009}. Für die ausgeführten Actions erhält der Agent einen Leitwert wie positiv oder
negativ sich diese Action auf die Umgebung oder den Agenten ausgewirkt hat. Wie bereits im vorherigen
Absatz erwähnt, kann der Agent als lernende Entität bezeichnet werden, da dieser seine Verhaltensweise
im Bereich  Reinforcement Learning selbstständig immer weiter in Richtung bestmöglicher Lösung einer 
Problemstellung optimiert. In einem Computerspiel kann ein Agent als menschlicher Spieler
repräsentiert werden. Dabei zählen die Augen und Ohren zu seinen Sensoren mit denen das Spiel
wahrgenommen wird und über seine motorischen Fähigkeiten und somit dem bedienen einer Tastatur
können Handlungen vollzogen werden. Ein im Spiel umgesetzter Nicht-Spieler-Charakter wird ebenfalls
als Agent gesehen, wobei dessen Wahrnehmung und Aktuatoren meist direkt im Spiel umgesetzt sind.


\subsubsection{Environment}
Das Environment definiert alles was den Agenten umgibt. Die Kommunikation zwischen einem Environment
und dem Agenten ist limitiert auf die \textit{Actions}, welche der Agent ausführen kann, die 
\textit{Observations}, welche der Agent nach dem Ausführen einer Action vom Environment erhält, 
sprich wahrnimmt, und durch die Bewertung der getätigten Aktionen, dem \textit{Reward} 
\cite[~S.5]{L2018}. Im Falle eines Computerspiels umfasst das Environment alle im Spiel festgelegten
Gesetzmäßigkeiten, aber auch die Netzwerkverbindung oder das Medium auf dem das Spiel ausgeführt
wird kann zum Environment gezählt werden.


\subsubsection{Action}
\label{sec:action}
Eine Action bietet die Möglichkeit für einen Agenten aktiv Einfluss auf das Geschehen im
Environment zu nehmen. Zu Actions zählen das Ausführen von Spielzügen, die durch das
Regelwerk erlaubt sind sowie die Änderung eines Zustandes in umfangreicheren Systemen. Die
Komplexität der Actions kann dabei ebenfalls variieren. Des Weiteren unterscheidet man im Bereich
Reinforcement Learning zwischen zwei Arten von Actions: diskrete und kontinuierliche Actions.
Diskrete Actions definieren eine endliche Menge an gegenseitig ausschließenden Handlungen die ein  % TODO wirklich gegenseitig ausschließend?
Agent vollziehen kann \cite[~S.8]{L2018}. Dazu zählen Actions wie sich nach links und rechts zu
bewegen oder zu springen. Unter kontinuierlichen Actions versteht man hingegen Handlungen, die
über einen zusätzlichen Wert beeinflusst werden, wie zum Beispiel das Steuern eines Fahrzeuges
mit einem Lenkrad. Dabei ist nicht ausschließlich die Lenkrichtung ausschlaggebend, sondern auch
der Einschlagswinkel des Lenkrades, welcher die Intensität der Lenkung festlegt.


\subsubsection{Observation}
Unter einer Observation versteht man die Wahrnehmung des Agenten, welche von dem Environment 
bereitgestellt wird. Dabei enthält eine Observation immer Informationen dazu, was gerade in der
Umgebung des Agenten passiert. Jedoch geben Observations für den Agenten nicht den Einblick  % TODO müssen nicht über das gesamte Environment Einblick geben, können aber
in das gesamte Environment, sondern limitieren dessen Wahrnehmung auf notwendige oder auch
ausschließlich gewollte Informationen \cite[~S.8 f.]{L2018}. Eine Observation kann aus einzelnen
Werten bestehen, welche zum Beispiel im Falle von Pong Informationen wie Position und
Geschwindigkeiten zu den Schlägern und dem Spielball zurückliefern. Zusätzlich kann eine
Observationen auch in Form eines Bildes oder einer Aneinanderreihung von Bildern umgesetzt sein.
Das Bild wird dabei von dem Agent verarbeitet und dieser führt dementsprechende Actions zum
visuell wargenommenen Zustand, sprich der Anordnung der Pixel aus. 


\subsubsection{Reward}
Im Gebiet Reinforcement Learning beschreibt der Reward einen Wert, welcher periodisch oder nach
dem Eintreffen einer definierten Gegebenheit dem Agenten vom Environment bereitgestellt wird.
Ziel des Rewards ist es dem Agenten Auskunft darüber zu geben, wie gut sich dieser verhält, sprich
diesen zu bewerten. Wie bereits beschrieben, versucht der Agent den erhaltenen Reward über den
Verlauf des Trainingsprozesses hinweg zu maximieren. Somit bildet der Reward als Feedback ein
Kernelement im Bereich Reinforcement Learning und definiert den \textit{bestärkenden} Anteil der 
Verfahren des machinellen Lernens \cite[~S.6 f.]{L2018}. Der Reward ist immer lokal zu betrachten,
dass heißt er gibt ausschließlich Auskunft über den Erfolg der aktuell ausgeführten Actions und
nicht über das bereits insgesamt gesammelte Ergebnis aller Erfolge und Misserfolge. Des Weiteren
bedeutet ein hoher lokaler Reward nicht, dass der daraus folgende Zustand global betrachet keine
noch größeren negativen Auswirkungen auf den Agenten haben wird.


\subsection{Markov Processes} % citations missing
Der Makrov Process und seine Erweiterungen bilden das theoretische Konstrukt für Reinforcement Learning
Verfahren. Durch die Beleuchtung dieser Prozesse werden weitere Terminologien geklärt, welche ebenfalls
ihren Einsatz im Reinforcement Learning finden.


\subsubsection{Markov Process}
Die einfachste Variante in der Markov Familie ist der Markov Process (Markov-Kette). Ein Markov Process
beschreibt ein System, welches ausschließlich observiert werden kann, sprich es kann nicht in die Dynamik
des Systems eingegriffen werden. Jeder Markov Process enthält eine Menge von \textit{States}, zwischen
denen das System nach den Gesetzlichkeiten der Systemdynamik hin und her wechseln kann. Die Menge aller
möglichen States wird dabei als \textit{State Space} bezeichnet. Wird das System observiert ergibt sich
durch die Wechsel von State zu State eine Folge, welche als \textit{History} bezeichnet wird. 

Damit ein Prozess als Markov Process bezeichnet werden kann muss dieser die \textit{Markov Eigenschaft}
aufweisen, welche definiert, dass die zukünftige Dynamik des Systems von jedem State nur
von diesem State abhängen darf. Die Markov Eigenschaft legt somit fest, dass ein State
differenziert von jedem anderen State des Systems betrachtet werden kann und deshalb die
History für die Vorhersage der Systemdynamik nicht notwendig ist.

Ein Prozess der die Markov Eigenschaft aufweißt muss die Fähigkeit bieten, eine Vorhersage über
die Wahrscheinlichkeit des Eintreffens eines Folgezustandes bereitzustellen. Dafür werden 
Wahrscheinlichkeiten für die Übergänge zwischen zwei States in einer \textit{Transition Matrix}
festgehalten. \autoref{fig:markov}: \nameref{fig:markov} veranschaulicht ein Wettermodell in 
der Repräsentationsform eines Markov Process mit den States \textit{Sunny} und \textit{Rainy}
und deren Übergangswahrscheinlichkeiten in zukünftige Folgezustände.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.19]{res/sunny-rainy-weather-model.png}
\caption{Sunny/Rainy Wettermodell}
\source{\cite[~S.13 - Chapter 1 - Figure 4]{L2018}}
\label{fig:markov}
\end{figure}


\subsubsection{Markov Reward Process}
\label{sec:markov_reward_process}
Die erste Erweiterung zum Markov Process ensteht durch die Hinzunahme eines weiteren Wertes zu den 
Übergängen zwischen zwei States, dem \textit{Reward} $R$. Dieser Wert definiert wie hoch die Belohnung
oder Bestrafung bei einem Wechsel der States ausfällt. Durch den Reward kann anschließend der
\textit{value of state} $V(s)$ berechnet werden. Für jeden State $s$ ist der value of state $V(s)$
der durchschnittliche (oder erwartete) Ertrag, welcher durch das Verfolgen des Markov Reward Process
entsteht und somit durch alleiniges Observieren die Bewertung von Zuständen ermöglicht.
Zusätzlich wird der \textit{discount factor} $\gamma$ eingeführt, welcher den erhaltenen Reward
für Übergänge die weiter in der Zukunft liegen verringert. Der value of state wird dabei wie
nachfolgend formuliert für eine \textit{Episode} berechnet, wobei eine Episode immer aus einer
Folge von States besteht und entweder durch das Eintreffen eines terminierenden States oder das
Erreichen einer Maximalanzahl von States beendet wird:

\begin{equation}
V(s)_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} ... = \sum_{k=0}^\infty \gamma^{k} R_{t+k+1}
\label{eq:value-of-state}
\end{equation} 

\begin{conditions}
 V(s)     	&  value of state \\
 R	     	&  Reward \\   
 \gamma 	&  discount factor für Rewards, die weiter in der Zukunft liegen
\end{conditions}


\subsubsection{Markov Decision Process}
Der Markov Decision Process erweitert den Markov Reward Process um eine endliche Menge an Actions.
Diese Menge entspricht dem \textit{Action Space} des Agenten. Dadurch kommt es nun nicht mehr
ausschließlich zur passiven Observierung, sondern der Agent kann zu jeder Zeit eine Action wählen um
somit die Wahrscheinlichkeiten des Zielzustandes selbstständig zu beeinflussen. Des Weiteren hängt 
der Reward, den der Agent erhält, nun nicht mehr ausschließlich von dem Zustand ab, in dem er endet,
sondern auch von der Action, die zu diesem Zustand führt. 

Die Verhaltensweise, die der Agent ausübt und welche für die Wahl der Actions verantwortlich ist,
wird als \textit{Policy} bezeichnet. Formal wird diese als Wahrscheinlichkeitsverteilung
über Actions für jeden möglichen Zustand definiert \cite[~S.22 f.]{L2018}.  % TODO nicht cite am Ende von Chapter


\subsection{Weiterführende Taxonomie}
In diesem Kapitel werden diverse Kategorisierungen für Reinforcement Learning Verfahren beschrieben,
welche nachfolgend bei der Erläuterung der realisierten Verfahren für deren Einordnung genutzt werden.  


\subsubsection{Value learning - policy learning}
Bei \textit{value learning} Verfahren wird jedem State oder jeder Kombination aus State und Action ein
Value/Score zugewiesen, welcher diesen bewertet. Durch den Value kann der Agent anschließend eine
Wahl zur besten Action treffen \cite[~S.NA]{L2018}. \textit{Policy learning} bezeichnet hingegen das
direkte Lernen einer Verhaltensweise, wobei anhand einer Observation eine Wahrscheinlichkeitsverteilung
der zu wählenden Actions erzeugt wird. Der Agent wählt anschließend mithilfe der jeweiligen
Wahrscheinlichkeiten die möglichen Actions \cite[~S.NA]{L2018}.


\subsubsection{On-policy - off-policy}
Die Kategorisierung \textit{off-policy} beschreibt die Fähigkeit von Reinforcement Learning Verfahren auf
Grundlage alter, bereits vor längerer Zeit aufgenommener Daten zu trainieren \cite[~S.NA]{L2018}. 
Im Kontrast dazu definiert \textit{on-policy} die Notwendigkeit von immer neu aufgezeichneten 
Trainingsdaten, welche nach einem Trainingsdurchlauf wieder verworfen und somit neu gesammelt
werden müssen.


\subsubsection{Model-free - model-based}
Der Term \textit{model-free} bezieht sich auf Verfahren, welche kein Model der Umgebung oder des Rewards
während des Trainingsprozesses aufbauen. Es werden ausschließlich durch aktuelle Observations
zugehörige Actions berechnet. Model-based Methoden versuchen im Gegenzug vorherzusagen, was 
die nächste Observation oder wie hoch der nächste Reward sein wird \cite[~S.NA]{L2018}. Aufgrund dieser
Vorhersage versucht der Agent die bestmögliche Action zu wählen, wobei in den meisten
Fällen die Vorhersage mehrfach und dabei auch mit der Betrachtung zukünftiger Schritte durchgeführt
wird \cite[~S.NA]{L2018}.


\subsection{Exploration und Exploitation}
In diesem Abschnitt wird der Konflikt zwischen Exploration und Exploitation des
Reinforcement Learnings erläutert und anhand des k-armed-Bandit Problems veranschaulicht.
Die dabei realisierten Implementierungen werden beschrieben und deren Ergebnisse
vorgestellt.


\subsubsection{Das k-armed-Bandit Problem}
% Erklärung des k-armed-Bandit Problems
Nach R. Sutton \cite[~S.26]{SB1998} besteht das k-armed-Bandit Problem darin, aus $k$
verschiedenen Actions zu wählen, wobei nachdem eine Action gewählt wurde, der Agent einen
Reward erhält, der aus einer Wahrscheinlichkeitsverteilung gezogen wird. Der
Erwartungswert der Wahrscheinlichkeitsverteilung hängt von der gewählten Action ab. Das
Ziel besteht darin mit der Ausführung von $n$ Actions die höchstmögliche Summe von Rewards
zu erhalten.

Der Erwartungswert der hinter der Verteilung einer Action steht, wird als $Value$ der Action
bezeichnet. Würden die Values der einzelnen Actions dem Agenten zur Verfügung stehen, wäre die 
Wahl der Action einfach, indem die Action mit dem höchsten Erwartungswert gewählt werden 
könnte. Allgemein ist in Reinforcement Learning Problemen der Value einer Action nicht bekannt.
Stattdessen gibt es ausschließlich Ab-schätzungen bezüglich des Values einer Action, die aus den
gesammelten Erfahrungen hervorgehen. Um den Value einer Action besser abschätzen zu können,
muss diese Action ausprobiert werden. Umso häufiger die Action ausgeführt wurde, umso besser
ist auch die Abschätzung des Values dieser Action.

Der Vorgang des Ausprobierens wird $Exploration$ genannt. Im Gegensatz dazu steht die
$Exploitation$ bei der die gesammelten Erfahrungen genutzt werden, um die Action
auszuwählen, die den höchsten Reward verspricht.

Das k-armed-Bandit Problem gilt als Vereinfachung zu allgemeinen Reinforcement Learning
Problemen, da nicht zwischen verschiedenen Ausgangszuständen unterschieden wird, sprich
es wird keine Observation durchgeführt. Weiterhin wird der Erwartungswert des
Rewards einer Action nicht über die Dauer des Spiels verändert. Dies bedeutet, dass die
Values der unterschiedlichen Actions am Anfang und Ende des Spiels identisch sind, wodurch
auch Erfahrungen, die zu Beginn des Spielens gemacht wurden, am Ende benutzt werden
können.

% TODO mehr cite
\subsubsection{Exploration-Strategien}
Im folgenden werden verschiedene Exploration-Strategien beleuchtet. Dazu werden diese erst
theoretisch erklärt und anschließend praktisch an einem k-armed-Bandit Problem verglichen.

% citation needed
\paragraph{$\epsilon$ greedy / decaying $\epsilon$ greedy}
Eine Strategie, die zu jedem Zeitpunkt die Action wählt, die nach derzeitigem Wissen den
höchsten Value hat, wird $greedy$ genannt. $\epsilon$-greedy-Strategien sind greedy
Strategien, die mit einer gewissen Wahrscheinlichkeit $\epsilon$ eine zufällige Action
wählen. Sie wählen also mit einer Wahrscheinlichkeit von $1 - \epsilon$ die Action, von
der sie sich den höchsten Reward versprechen und andernfalls eine zufällige Action.

In der Praxis werden häufig decaying-$\epsilon$-greedy-Strategien verwendet, bei denen der
$\epsilon$-Wert über den Trainingsprozess hinweg verringert wird. Dadurch werden zu Beginn
des Trainings viele neue Actions ausprobiert, während gegen Ende das Anwenden des
Gelernten stärker in den Vordergrund tritt.

% citation needed
\paragraph{Optimistic Initial Values}
Ein Agent, der ein k-armed-Bandit Problem lösen soll, besitzt eine interne Abschätzung der
Values, um Actions wählen zu können, die einen besseren Reward versprechen. Die
Bewertung der Actions wird beeinflusst durch die erfahrenen Rewards nach dem Wählen einer
Action. Beispielsweise können die erfahrenen Rewards einer Action gemittelt werden, um
zukünftige Rewards dieser Action abschätzen zu können.

Neben $\epsilon$-greedy Verfahren ist es eine weitere Möglichkeit Exploration umzusetzen,
die initiale Bewertung der Values sehr optimistisch vorzunehmen. Dadurch wird der Agent
bei der Wahl einer Action vom erhaltenen Reward \grqq enttäuscht\grqq, sodass sich die
Einschätzung dieser Action verschlechtert. Wird nun immer die Action gewählt, die am
besten eingeschätzt ist, so werden über die Zeit alle Actions versucht, bis sie sich dem
tatsächlichen Erwartungswert nähern.

% citation needed
\paragraph{Upper Confidence Bound Action Selection}
Während $\epsilon$-greedy-Methoden zufällig zwischen allen nicht greedy Actions
auswählen, wählt die Upper Confidence Bound Ac\-tion Selection Methode Actions entsprechend
dreier Parameter aus.
Der erste Parameter ist $t$ die Anzahl der Actions, die bis jetzt gespielt wurden. Der
zweite ist der bis jetzt ermittelte $Value$ der Action $a$ bezeichnet mit $Q(a)$ und der
dritte Parameter ist die Anzahl, wie oft Action $a$ schon ausprobiert wurde. Die nächste
zu spielende Action wird dann nach folgender Gleichung gewählt:

\begin{align}
A = \argmax_a\left( Q(a) + c*\sqrt\frac{\ln(t)}{N(a)} \right)
\end{align}

$A$ ist die gewählte Action. $N(a)$ ist die Anzahl, wie oft die Action $a$ schon gewählt
wurde. Existiert eine Action, die noch nie gespielt wurde, also $N(a)=0$, so wird eine der
Actions gespielt, die noch nicht gespielt wurden. Durch den Term
$c*\sqrt\frac{\ln(t)}{N(a)}$ wird sichergestellt, dass selten benutzte Actions eine höhere
Bewertung erhalten und damit wahrscheinlicher ausprobiert werden. Über den Hyperparameter
$c$ lässt sich bestimmen, wie stark selten benutzte Actions ausprobiert werden sollen.

Ein Vorteil gegenüber $\epsilon$-greedy Strategien ist, dass durch die Einbeziehung von
$Q(a)$ Actions häufiger ausprobiert werden, die besser erscheinen, während Actions, die
sehr schlecht wirkende Actions weniger oft ausprobiert werden.


\subsubsection{Implementierung}
Das k-armed-Bandit Problem sowie verschiedene Varianten der Action-Selection wurden
implementiert und sind unter \url{https://github.com/Bluemi/rl_testbed} abrufbar.

In der Klasse \lstinline!NArmedBandit! wird das k-armed-Bandit Problem umgesetzt. Es
werden zu Beginn die Values der einzelnen Actions aus einer Standardnormalverteilung
gezogen und gespeichert. Wird nun eine Action auf dem erstellten Bandit gespielt, so wird
eine Zufallszahl aus einer Normalverteilung mit dem Erwartungswert der jeweiligen Action
gezogen und als Reward zurück gegeben.
Um auf einem Bandit zu spielen wurde ein \lstinline!Solver! umgesetzt, der eine interne
Abschät\-zung der Action Values ermittelt, indem die erfahrenen Rewards dieser Action
gemittelt werden. Der Solver wählt Actions wobei unterschiedliche Exploration Strategien
ausprobiert werden können. Die Unterklasse \lstinline!EpsilonGreedySolver! implementiert
beispielsweise die $\epsilon$-greedy Strategie.

In \lstinline!src/main.py! werden unterschiedliche Solver anhand des gleichen Bandits
getestet und die Ergebnisse geplottet. Jeder Solver trainiert 5000 Actions auf einem
10-armed-Bandit. Die dabei resultierenden Rewards sind in Abbildung
\ref{fig:karmed_bandit} graphisch dargestellt. Die zu sehenden Rewards wurden über 2000
Experimente gemittelt, um ein anschaulicheres Ergebnis zu erhalten.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.3]{res/k_armed_bandit_epsilon.png}
\includegraphics[scale=0.3]{res/k_armed_bandit_optimistic_upper.png}
\caption{Ergebnisse 10-armed-Bandit}
\label{fig:karmed_bandit}
\end{figure}

\noindent
Auf der linken Seite werden 3 unterschiedliche $\epsilon$ greedy Verfahren verglichen.
Vergleicht man die Verfahren ohne decay (blau und orange), so kann man erkennen, dass
$\epsilon = 0.1$ zu Beginn besser funktioniert, da mehr Zeit für Exploration verwendet
wird und so schneller bessere Actions gefunden werden. Dafür funktioniert $\epsilon =
0.01$ nach circa 1500 Action Selections besser, da mehr Zeit verwendet wird, um die schon
gefundenen guten Actions zu exploiten.

Um die Vorteile beider Möglichkeiten zu nutzen, verwendet das dritte Experiment einen
decay, um $\epsilon$ auf $0$ zu senken. Dadurch werden zu Beginn viele zufällige Actions
ausgewählt, um die Values der einzelnen Actions besser einzuschätzen. Wenn durch den decay
$\epsilon$ gegen Ende des Experiments sinkt, werden die gefundenen Actions benutzt und ein
vergleichsweise hoher Reward entsteht.

In der rechten Abbildung sind das Verfahren Optimistic Initial Values und Upper Confidence
Bound Action Selection zu sehen. Zusätzlich ist der $\epsilon$-greedy Solver mit
$\epsilon=0.01$ aus der ersten Abbildung zum Vergleich eingezeichnet. Die obere schwarz
gestrichelte Linie deutet das Value der optimalen Action an.

Tabelle \ref{tab:explorationstrategies} zeigt die erreichten durchschnittlichen Rewards
der unterschiedlichen Verfahren.

\begin{table}[ht]
  \begin{center}
    \begin{tabular}{l | l}
      \textbf{Verfahren} & \textbf{Mean Reward} \\
      \hline
      Optimal & $1.579$\\
      Upper Confidence Bound & $1.556$ \\
      $\epsilon$-greedy with decay ($\epsilon=0.01$) & $1.523$ \\
      Optimistic Initial Values & $1.520$ \\
      $\epsilon$-greedy ($\epsilon=0.01$) & $1.439$ \\
      $\epsilon$-greedy ($\epsilon=0.1$) & $1.437$ \\
    \end{tabular}

    \caption[ExplorationStrategies]{Vergleich unterschiedlicher Exploration Strategien}
    \label{tab:explorationstrategies}
  \end{center}
\end{table}

\noindent
Im gezeigten Experiment erhielten die einfachen Versionen der $\epsilon$-greedy Solver die
kleinsten Rewards. Deutlich besser funktionierte die erweiterte Version mit decay, die
vergleichbar mit dem Verfahren Optimistic Initial Values ist. Am besten schneidet die
Upper Bound Confidence Action Selection ab.


\newpage
\subsection{Open AI Gym}
Die Python Bibliothek \textit{gym}, welche von der Firma Open AI umgesetzt wurde umfasst ein
Toolkit, welches zur Entwicklung und zum Vergleich von Reinforcement Learning Algorithmen
eingesetzt werden kann. Das Toolkit definiert keine Strukturen für die Umsetzung der Agenten,
wodurch die Wahl der Machine Learning Bibliothek dem Entwickler frei zur Verfügung steht. In
der gym Bibliothek wird hingegen eine Sammlung an Testumgebungen (Environments) zur Nutzung
bereitgestellt, welche alle eine festgelegte Problemstellung definieren \cite{OAI2016}. 
Diese Environments bieten eine einheitliche Schnittstelle, was das Implementieren von
generellen Algorithmen für nahezu jede Art von Environment ermöglicht. Da Reinforcement
Learning Verfahren in komplexen Problemstellungen immer bessere Resultate erzielten, jedoch
keine standartisierte Umsetzung für die Entwicklung und anschließende Nutzung von Environments
und somit auch keine Möglichkeit für ein Benchmarking verschiedener Verfahren bestand,
beschloss Open AI mit der gym Bibliothek genau diesen Problemen entgegenzuwirken
\cite{OAI2016}.

Nachfolgend soll der Aufbau und die Nutzung eines Environments durch die Bibliothek gym
erläutert werden und es wird auf eine Referenzimplementierung eines Environments für das
Spiel Tic-Tac-Toe eingegangen.


\subsubsection{Konzept}
Ein Environment ist eine von der Klasse gym.Env erbende Unterklasse, welche durch die
Hauptprogrammierschnittstelle des Toolkits die folgenden Methoden definiert.

\begin{itemize}
\itemsep-6pt
\item \textit{step}
\item \textit{reset}
\item \textit{render}
\item \textit{close}
\item \textit{seed}
\end{itemize}  

Mittels der \textbf{step(action)} Methode kann eine Action für einen timestamp im
Environment ausgeführt werden. Diese Action kann sowohl diskrete Werte als auch 
kontinuierliche Werte repräsentieren. Nachdem die Action im Environment ausgeführt wurde,
wird die für den Agent bereitgestellte aktuelle \textit{Observation} des Environments
aufgezeichnet. Des Weiteren wird ein, durch die Action initiierter und durch den Wechsel
in den neuen Zustand erhaltener \textit{Reward} für den Agenten ermittelt sowie ein 
\textit{done} Flag, welches Auskunft darüber gibt, ob die aktuelle Episode beendet wurde.
Weitere Informationen werden im \textit{info} Dictionary gesammelt und dienen für optionale
Analyseinformationen. Das Tuple \textit{observation, reward, done, info} wird anschließend
an den Aufrufenden zurückgegeben. \autoref{lst:step-method} veranschaulicht eine
Beispielimplementierung der \textit{step} Methode gegen die von Open AI bereitgestellte API.

\begin{lstlisting}[language=Python, caption=step method, label=lst:step-method]
def step(self, action):
	# executes the action in the environment and validates it
	valid = self._take_action(action)
	
	if not valid:
		print("invalid action: player ", self._current_player)
	
	# calculates the obtained reward
	reward = self._get_reward()
	
	# gathers current observation of the environment
	observation = (self._player_0, self._player_1)
	
	# information if the episode is finished
	is_done = self._check_if_done(reward)
    
	return observation, reward, is_done or not valid, {}
\end{lstlisting}
\noindent
Die Funktionen \textit{\_take\_action(action)}, \textit{\_get\_reward()} und
\textit{\_check\_if\_done(reward)} setzen die Kommunikation mit dem Environment um. In
diesem Bespiel besteht die Observation ausschließlich aus den Zuständen der beiden Spieler.
Nach der Änderung der Systemdynamik und dem Erfassen der Observation wird das bereits
beschriebene Tupel an Umgebungsinformationen zurückgeliefert.\\
Die Methode \textbf{reset()} dient zum vollständigen Zurücksetzen des Environments und aller
darin gespeicherten Zustände. Anschließend gibt die Methode den Ausgangszustand als Observation
an den Aufrufenden zurück. Reset kommt zum Einsatz, sobald eine Episode abgeschlossen wurde
(done Flag) damit erneut Actions darin ausgeführt werden können.
\newline
\noindent
Mit \textbf{render(mode='human')} kann der zwischenzeitliche Zustand des Environments
visualisiert werden. Dabei legt die gym Bibliothek keine explizit zu nutzenden Renderverfahren
fest, per Konvention sind aber folgende Verfahren definiert: 

\begin{itemize}
\itemsep0pt
\item \textit{human} - für Menschen veranschaulicht, 
\item \textit{rgb\_array} - ein numpy.ndarray mit RGB Pixelwerten, um dieses in einem Bild oder einem 
Video zu visualisieren und 
\item \textit{ansi} - als terminal-style Textrepräsentation.
\end{itemize}
\noindent
Abschließend kann durch \textbf{close()} das Environment und somit gegebenenfalls die 
dahinterliegende Anwendung geschlossen werden. Durch den Aufruf von 
\textbf{seed(seed)} kann ein Seed für den \textit{random number generator}
des Environments festgelegt werden.\\

Neben den beschriebenen Methoden exisiteren die Properties \textit{action\_space}, welcher die
Art und Anzahl der zu wählenden Actions vorgibt (z.B. \textit{Discrete(9)} für 9 verschiedene
diskrete Actions), \textit{observation\_space}, welcher gleiches für die Observation des
Environments angibt und \textit{reward\_range}, welcher einen Wertebereich für den zu erhaltenen
lokalen Reward definiert.\\
Für das Spiel Tic-Tac-Toe wurde ein Einspieler Environment implementiert, welches im Unterverzeichnis
\textit{preparation/05\_v\_learning/} unter \url{github.com/dephiloper/independent-coursework-rl/}
abgerufen werden kann.  % TODO overfull hbox in link


\subsubsection{Verwendung}
Das nachfolgende Listing \autoref{lst:env-usage} veranschaulicht die Nutzung eines von Open AI
bereitgestellten Environments mit der Bezeichnung CartPole-v0. Initial wird das Environment
mit \textit{gym.make('CartPole-v0')} erzeugt und daraufhin mit \textit{reset()} vollständig
zurückgesetzt, sodass alle internen Zustände wieder in die Ausgangssituation gebracht werden. 
Anschließend werden in 1000 Frames das Environment mit \textit{render()} visualisiert und es
wird eine zufällige Action aus dem action\_space des Environments gesampled und in der
\textit{step(action)} Methode ausgeführt. Durch das Ausüben der Action wird eine neue
Observation mit zugehörigem Reward vom Environment generiert und anschließend zurückgegeben.
Nach 1000 Iterationen wird das Environment geschlossen. 
\begin{lstlisting}[language=Python, caption=environment usage, label=lst:env-usage]
import gym
env = gym.make('CartPole-v0')
env.reset()
for _ in range(1000):
    env.render()
    # take a random action, gather observation and reward
    obs, reward, done, _ = env.step(env.action_space.sample())
env.close()
\end{lstlisting}

Durch den definierten Aufbau der bereitgestellten API bietet ein Environment in den meisten Fällen
die Möglichkeit Agenten mit verschiedenen Verfahren zur Ermittelung der zu tätigenden Action zu
implementieren, sodass diese nach dem Aufsetzen einer Grundstruktur auch gut ausgetauscht werden
können.


\newpage
\section{Cross-Entropy-Method}
\label{sec:cross-entropy-method}
In den nachfolgenden Kapiteln werden diverse Algorithmen in Bereich Reinforcement Learning
erläutert. Das erste zu beleuchtende Reinforcement Learning Verfahren ist die
Cross-Entropy-Method. Die Cross-Entropy-Method ist eine Monte Carlo Methode für
Stichprobenentnahme nach Wichtigkeit (importance sampling) und Optimierung (optimization)
\cite[~S.29 ff.]{R2004}. Im folgenden Kapitel wird das Verfahren erläutert und in die bereits
beschriebene Taxonomie eingeordnet. Des Weiteren erfolgt die Anwendung des Verfahrens innerhalb
von zwei Testumgebungen. 

\subsection{Einordnung}
Bei dem Verfahren handelt es sich um eine on-policy Methode, da im Trainingsprozess ausschließlich
mittels neu aufgezeichneter Trainingsdaten gelernt werden kann. Prinzipiell hat das Verfahren das
Hauptziel, den maximalen Reward innerhalb einer Episode anzusammeln. Um dieses Ziel zu erreichen
orientiert es sich an klassischen Machine Learning Verfahren und führt eine nichtlineare trainierbare
Funktion ein, die als Input die Observation des Agenten erhält und diesen auf einen Output abbildet.
Durch diese Charakteristik fällt die Cross-Entropy-Method in die Familie der policy learning Verfahren
(policy-based), da mittels neuronalen Netz eine Policy erzeugt wird, welche für jede Observation die
zu wählende Action in Form einer Wahrscheinlichkeitsverteilung über alle Actions ausgibt, sprich es
wird eine Observation über direktem Wege auf eine zu wählende Action abgebildet. Die
Methode ist nicht value-based, da der zu erwartende Reward nicht approximiert wird.

\subsection{Verfahren}
Die zu lernende Policy ist wie bereits erwähnt als Wahrscheinlichkeitsverteilung über die 
zu wählenden Actions repräsentiert und dadurch sehr ähnlich zu einem Klassifizierungsproblem,
wobei die Anzahl der Klassen hier der Anzahl von auszuführenden Actions entspricht. Durch
diese Verfahrensweise ist die Umsetzung des Agenten als eher einfach zu betrachten, da
diese ausschließlich die Observation des Environments an das Netz weitergeben muss, von
dem Netz anschließend die Wahrscheinlichkeitsverteilung erhält und anhand dieser dann eine
Zufallsstichprobe (random sampling) unter Berücksichtigung der Wahrscheinlichkeiten aller
Actions vollzieht \cite[~S.78]{L2018}. Nachdem der Agent durch das random sampling eine
Action gewählt hat, führt er diese erneut im Environment aus und erhält die nächste
Observation sowie den Reward für die zuvor ausgeführte Action.

Diese Schritte werden anschließend solange wiederholt, bis eine Ansammlung an Episoden (siehe 
\autoref{sec:markov_reward_process}: \nameref{sec:markov_reward_process}), dass heißt eine Folge
von Observations, Actions und Rewards, gesammelt wurde. Innerhalb jeder Episode wird nun der
insgesamt erwirtschaftete Reward, der \textit{total reward}, berechnet. Dadurch kann bewertet werden
wie gut eine Episode im Vergleich zu anderen Episoden ausgefallen ist. Aufgrund der Zufälligkeit
bei der Selektion von Actions kommt es dazu, dass einige Episoden besser ausfallen als andere.
Die Cross-Entropy-Method macht sich dieses Verhalten zu nutze, verwirft schlechtere Episoden und
trainiert ausschließlichen auf besseren Episoden. Das Kernprinzip des Verfahrens wird wie nachfolgend
aufgelistet in drei Schritte untergliedert \cite[~S.80 f.]{L2018}:

Initial wird eine definierte Anzahl $N$ an Episoden mit dem aktuellen Model im Environment gespielt
oder auch wahrgenommen. Anschließend erfolgt die Berechnung des total rewards für jede gespielte
Episode. Zusätzlich wird in diesem Schritt eine Obergrenze für den Reward festgelegt 
(\textit{reward boundary}). Daraus folgt, dass die besten 30 Prozent der Episoden behalten werden,
wohingegen die restlichten schlechteren Episoden verworfen werden. Anschließend wird im letzen Schritt
das Model auf den übrigen \textit{elite} Episoden trainiert, wobei eine Observation als Input in das
Netz gegeben wird und die zuvor gewählten Actions als gewünschter Output vom Netzes gefordert werden.
Während des Trainingsprozesses werden diese Schritte solange wiederholt bis ein gewünschtes
Ergebnis erzielt wurde. In \autoref{fig:cross_entropy_procedure} werden die beschriebenen Schritte
zusätzlich visuell dargestellt:

\begin{figure}[htp]
\centering
\includegraphics[scale=0.45]{res/cross-entropy-procedure.png}
\caption{Cross Entropy Verfahren}
\label{fig:cross_entropy_procedure}
\end{figure}

\subsection{Cart-Pole}
Das Cart-Pole Environment beschreibt ein eher einfach einzustufendes Environment, welches durch seine
simple Nutzung gerne als Einstiegspunkt für die Entwicklung von Reinforcement Learning Algorithmen 
dient. Das Environment besteht aus einem Stab (pole), welcher über ein Gelenk an einem Wagen (cart) 
befestigt ist \cite{OAI2016_2}. Der Agent kann den Wagen durch das Aufbringen einer horizontalen Kraft 
von -1 oder +1 steuern, wobei sich der Wagen dementsprechend nach links oder rechts bewegt. Nach 
dem Zurücksetzen des Environments startet der Stab in einer vertikalen Ausrichtung, nahezu orthogonal
zentriert auf dem Wagen. Das zu erreichende Ziel für den Agenten ist es den Stab vom Umkippen zu hindern,
wobei der Stab als umgekippt zählt sobald dessen Ausrichtung mehr als 12 Grad von einem genauen
senkrechten Winkel abweicht. Des Weiteren zählt eine Episode als beendet, wenn sich der Wagen mehr
als 2.4 Units vom Ausgangspunkt wegbewegt. Das Environment liefert dem Agenten einen Reward von +1
für jeden Frame in dem der Stab und der Wagen die beschriebenen Anforderungen erfüllen. Des Weiteren
erhält der Agent als Observation die aktuelle Position und Geschwindigkeit des Wagens, sowie den Winkel
der Ausrichtung des Stabs und die Geschwindigkeit die auf die Spitze des Stabs wirkt.

In dem Toolkit von Open AI existieren diverse Versionen von Cart-Pole, wobei sich die eben
erläuterte Beschreibung auf das Environment CartPole-v0 bezieht, des von Barto, Sutton und
Anderson beschriebenen Cart-Pole-Problems \cite[~S.838 f.]{BSA1983}. Erreicht der Agent einen
durchschnittlichen Reward von mindestens 195,0 über 100 aufeinanderfolgende Versuche hinweg gilt das
Problem als gelöst. \autoref{fig:cart-pole} veranschaulicht die beschriebene Problemstellung aus dem 
Paper\cite{BSA1983} von Barto, Sutton und Anderson:

\begin{figure}[htp]
\centering
\includegraphics[scale=0.15]{res/cart-pole.png}
\caption{Illustration des Cart-Pole Problems}
\source{\cite[~S.838]{BSA1983}}
\label{fig:cart-pole}
\end{figure}

\paragraph*{Evaluierung}
\noindent
\newline
Das Cross-Entropy-Verfahren wurde im Cart-Pole Environment mittels neuronalen Netz mit
einem Hidden Layer von 128 Nodes und einer Learning Rate von 0.01 trainiert. Als
Optimizer wurde Adam verwendet und als Aktivierungsfunktion zwischen Input Layer und
Hidden Layer die ReLu Funktion. Zur Berechnung eines Loss zwischen Vorhersage des Netzes
und den während des Spielens aufgenommenen Actions wird der Cross-Entropy-Loss verwendet.

Die nachfolgenden Graphen in \autoref{fig:cross-entropy-graph} veranschaulichen den
\textit{loss}, den stetig wachsenden \textit{reward\_bound} (Reward Obergrenze) und den
durchschnittlich erhaltenen Reward beim Spielen einer Episode (\textit{reward\_mean}).
Das Model des orange dargestellten Graphs verwendet bei der Auswahl der elite Episoden
einen Perzentil von 70, das blau dargestellte Model einen Perzentil von 80 und das
rot dargestellte Model einen Perzentil von 90. Ein Perzentil definiert eine Menge von
Werten einer Verteilung die unter oder gleich einem festgelegten Wert sind (z.B. Median 
= 50\%-Perzentil). 

\begin{figure}[htp]
\centering
\includegraphics[scale=0.385]{res/cross-entropy_loss.png}
\includegraphics[scale=0.385]{res/cross-entropy_reward-bound.png}
\includegraphics[scale=0.385]{res/cross-entropy_reward-mean.png}
\caption{Ergebnisse Cross-Entropy-Method Cart-Pole-v0}
\label{fig:cross-entropy-graph}
\end{figure}

Die Implementierung der Cross-Entropy-Method zum Cart-Pole Problem ist unter
\url{github.com/dephiloper/independent-coursework-rl} im Unterverzeichnis 
\textit{preparation/04\_cross\_entropy} in der Datei
\href{https://github.com/dephiloper/independent-coursework-rl/blob/master/preparation/04_cross_entropy/cart_pole_torch.py}{\nolinkurl{cart\_pole\_torch.py}} umgesetzt.


\subsection{Frozen Lake}
\label{sec:frozen-lake}
Bei dem zweiten Environment Frozen Lake handelt es sich um ein Environment mit diskretem action-
und diskretem observation-space, das heißt, dass sowohl die zu tätigende Action diskret ist,
aber auch die Observation als diskreter Zahlenwert von 0-15 wahrgenommen wird. Der Zahlenwert
der Observation repräsentiert dabei ausschließlich die Position des Agenten. Das Ziel ist es
einen Avatar vom Startpunkt in der linken oberen Ecke eines Spielfeldes auf die untere rechte
Seite zum Ziel zu steuern. Auf dem Weg zum Ziel befinden sich Löcher, die das Spiel beenden, wenn
sich der Avatar auf diese bewegt. Das Spielfeld ist in Abbildung \ref{fig:frozen_lake} zu sehen.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.25]{res/frozen_lake.png}
\caption{Frozen Lake Spielfeld}
\source{\cite[~S.90 - Chapter 4 - Figure 5]{L2018}}
\label{fig:frozen_lake}
\end{figure}

\noindent
Der Agent kann die Actions \grqq left\grqq, \grqq up\grqq, \grqq right\grqq und \grqq
down\grqq ausführen. Erschwert wird die Steuerung aber dadurch, dass die Oberfläche
rutschig ist, weshalb die Folgeposition des Avatars nicht deterministisch ist.
Wählt man bespielsweise die Action \grqq down\grqq, so bewegt sich der Avatar mit einer
Wahrscheinlichkeit von jeweils $1/3$ nach unten, rechts oder links. Allgemein gesprochen
ist die Richtung, in die sich der Avatar nach der Wahl einer Action bewegt, eine zufällige
Richtung, wobei jedoch nie die entgegengesetzte Action der Wahl vollzogen wird.


\paragraph*{Evaluierung}
\noindent
\newline
Anders als das Cart-Pole Environment erhält der Agent bei dem Frozen Lake Environment 
ausschließlich einen +1 Reward, sobald der gesteuerte Avatar das Ziel erreicht. Daraus folgt,
dass anhand des \textit{total reward} einer Episode nicht darauf geschlossen werden kann,
wie gut eine Episode gespielt wurde (Avatar kann lange umherwandern bevor er auf das Ziel kommt).
Somit lässt sich auch keine sinnvolle Wahl von elite Episoden treffen, da ausschließlich
durchgespielte Episoden mit einem total reward von +1 oder 0 bestehen und die Episoden mit 0
deutlich häufiger auftreten.

Aufgrund dieser Probleme werden folgende Einschränkungen der Cross-Entropy-Method 
aufgeführt \cite[~S.92 f.]{L2018}:
\begin{itemize}
\itemsep0pt
\item endliche und kurze Episoden notwendig
\item detailierte Unterscheidung des total Reward zwischen guten und schlechten Episoden
\item Reward für die Erfüllung von Zwischenschritten
\end{itemize}

Diese Einschränkungen können jedoch addressiert werden, indem mehr Episoden gespielt werden, 
um somit die Wahrscheinlichkeit zu erhöhen erfolgreiche Episoden zu erhalten. Des Weiteren
muss der discount factor $\gamma$ auf die Berechnung des total reward angewandt werden, damit
kürzere Episoden einen höheren total reward erzielen, als lange Episoden. Abschließend ist es
sinnvoll elite Episoden für mehrere Iterationen zu behalten damit auf diesen mehrmals
trainiert werden kann \cite[~S.93]{L2018}.  

Die nachfolgenden Graphen veranschaulichen die Anwendung der Cross-Entropy-Method ohne (grün)
und mit Anpassungen (grau) zur Erzielung besserer Resultate. Durch die Erweiterung des
Verfahrens kann eine Verbesserung erzielt werden, sodass der Trainingsprozess bei einer
etwa 50\% erfolgreichen Lösung des Environments stagniert.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.485]{res/frozen-lake_reward-mean_bad.png}
\includegraphics[scale=0.485]{res/frozen-lake_reward-mean.png}
\caption{Ergebnisse Cross-Entropy-Method FrozenLake-v0}
\label{fig:frozen-lake_cross-entropy}
\end{figure}

Die Implementierung der Cross-Entropy-Method zum Frozen Lake Environment ist unter
\url{github.com/dephiloper/independent-coursework-rl} im Unterverzeichnis 
\textit{preparation/04\_cross\_entropy} in der Datei 
\href{https://github.com/dephiloper/independent-coursework-rl/blob/master/preparation/04_cross_entropy/frozen_lake_torch.py}{\nolinkurl{frozen\_lake\_torch.py}} umgesetzt.
\newpage


\section{Q-Learning}
In diesem Abschnitt wird das Verfahren Q-Learning erläutert. Anschließend wird die Methode
an zwei praktischen Anwendungen getestet. Einerseits in der Testumgebung frozen-lake 
(siehe \autoref{sec:frozen-lake}) und andererseits in einer selbstständig entwickelten
Tic-Tac-Toe Anwendung, in der das Verfahren anhand eines zufällig spielenden Gegner
validiert wird.


\subsection{Einordnung}
Q-Learning Verfahren gehören zur Familie der value learning Verfahren, die
unterschiedlichen Actions unterschiedliche Values zuordnen, um daraufhin diejenigen Actions
wählen zu können, die den höchsten Value aufweisen. Der Value einer Action ist dabei der
zu erwartende Reward, welcher vom Environment zurückgegeben wird, wenn man die gewählte
Action im wahrgenommenen State ausführt.

Weiterhin gehören Q-Learning Verfahren zu den off-policy Verfahren, da auch alte
Erfahrungen zum Training benutzt werden können. Die Q-Learning Methoden, die nachfolgend
beschrieben werden sind model-free, was bedeutet, dass nicht versucht wird die nächsten
States oder die folgenden Rewards vorherzusagen.

Wie oben bereits beschrieben, basieren Q-Learning Verfahren auf einer Abschätzung des
Values eines States oder einer Action. Die Abschätzung des Values einer Action $a$ in
einem State $s$ wird als $Q(s, a)$ bezeichnet. Diese Value kann auf unterschiedliche
Weise ermittelt werden.

Eine erste Variante von Q-Learning benutzt Tabellen, in denen die Abschätzungen
für unterschiedliche States oder Actions gespeichert werden. Dieses Verfahren wird nach
\cite[~S.193]{L2018} auch als \grqq Tabular Q-Learing\grqq bezeichnet.


% Beschreibung des Verfahrens Tabular Q-Learning u. Unterscheidung zu Deep-Q-Learning
\subsection{Tabular Q-Learning}
In diesem Abschnitt wird auf das Tabular Q-Learning eingegangen. Um die
Ver\-ständ\-lich\-keit zu verbessern wird das Verfahren anhand des Frozen Lake
Environments veranschaulicht.


\subsubsection{Value-Iteration Verfahren}
Da sich das Spielfeld nicht verändert, beschränken sich die Zustände von Frozen Lake auf
die Positionen des Spielers. Durch die geringe Anzahl an Spielsituationen bietet sich das
Tabular Q-Learning Verfahren an, da es möglich ist alle erreichbaren Zustände in einer
Tabelle zu speichern und deren Values zu bestimmen. Diese Tabelle enthält für jeden State
eine Abschätzung der unterschiedlichen Actions. Tabelle \ref{tab:q_table} zeigt eine
sehr kleine beispielhafte Q-Table mit zwei Actions und zwei States.

\begin{table}[ht]
  \begin{center}
    \begin{tabular}{c | c | c}
       & $a_0$ & $a_1$ \\
      \hline
      $s_0$ & $Q(s_0, a_0)$ &$ Q(s_0, a_1)$ \\
      \hline
      $s_1$ & $Q(s_1, a_0)$ & $Q(s_1, a_1)$ \\
    \end{tabular}

    \caption[Q-Table]{Q-Table}
    \label{tab:q_table}
  \end{center}
\end{table}

\noindent
Das Ziel des Verfahren ist es die Q-Values der Tabelle zu ermitteln. Dazu werden die
Q-Values mit Null initialisiert, um diese anschließend iterativ zu bestimmen. Das Verfahren 
sieht vor, dass zufällige Actionen ausgewählt und in einem Environment ausprobiert werden.

Daraus resultieren ein State, eine Action, ein Reward und ein Folgestate, auch SARS-Tupel
genannt. Der erste State $s$ ist der Ausgangszustand, in dem man die zufällig gewählte
Action $a$ getätigt hat. Als Folge auf das Ausführen dieser Action erhält man einen
unmittelbaren Reward $r$, der vom Environment bestimmt wird, sowie den Folgestate $s'$,
also den State, den das Environment nach Ausführung von $a$ erreicht hat.

Ist das SARS-Tupel gefunden, werden die Q-Values der Tabelle angepasst, wobei der folgende
Zusammenhang verwendet wird.
\begin{align}
Q(s, a) = r_{s,a} + \gamma \max_{a' \in A}Q(s', a')
\label{aln:QValues}
\end{align}
\noindent
$Q(s, a)$ ist das Value der Action $a$ im wahrgenommenen State $s$, also der long-term
Reward, der erwartet wird, führt man in State $s$ die Action $a$ aus. Das Ziel ist es
$Q(s, a)$ so anzupassen, dass es sich dem tatsächlich zu erwartenden long-term Reward
annähert. Der unmittelbare Reward $r_{s,a}$ kann dem SARS-Tupel entnommen werden, ebenso
wie der Folgestate $s'$.

Befindet sich der Agent im State $s'$, so ist der maximal zu erwartende Reward, der 
maximal zu erwartende Reward der besten Action im State $s'$. Das bedeutet, dass
der Term $\max_{a'\in A} Q(s', a')$ der erwartete long-term Reward der besten Action des
Folgezustandes, also auch der bestmögliche long-term Reward, den man in State $s'$
erwarten kann, ist. Nach (\ref{aln:QValues}) gleicht $Q(s, a)$ dem unmittelbaren Reward
$r_{s,a}$ addiert mit dem besten zu erwartenden long-term Reward des Folgezustandes. Der
Wert des Folgezustandes wird dabei mit dem discount factor $\gamma$ multipliziert, wobei 
$\gamma$ standardmäßig zwischen $0.9$ und $0.99$ gewählt wird.

Mit Formel (\ref{aln:QValues}) erhalten wir einen neuen Wert für $Q(s, a)$, also einen
neuen Wert in unserer Q-Table. Da die Wirkung einer Action in Frozen Lake nicht
deterministisch ist und auch im Allgemeinen nicht davon ausgegangen werden kann, wäre es
nicht gut den Wert der Q-Table gleich dem ermittelten Wert zu setzen. Stattdessen wird der
Wert der Tabelle in Richtung des ermittelten Wertes angepasst. Dies geschieht durch die in
(\ref{aln:QValueAdapt}) dargestellte Zuweisung.

\begin{align}
  Q(s, a) \leftarrow (1 - \alpha)Q(s, a) +
  \alpha(r_{s, a} + \gamma \max_{a'\in A}Q(s', a'))
\label{aln:QValueAdapt}
\end{align}
\noindent

Wobei der Hyperparameter $\alpha$ zwischen 0 und 1 gesetzt wird, um den Wert $Q(s, a)$ bei
jeder Iteration schrittweise zu überblenden. Dadurch nähern sich die Q-Values der Tabelle
den tatsächlichen Q-Values an, wodurch sich das Environment erfolgreich spielen lässt.

\subsubsection{Evaluierung}
Um zu testen, wie gut die gefundenen Q-Values funktionieren, wird wie folgt vorgegangen.
Ein neues Environment wird initialisiert und der Agent wählt in jedem erfahrenen State die
Action, die das höchste Q-Value besitzt.

Das vorgestellte Experiment wird als gelöst angesehen, wenn 80\% von 20 Spielen erfolgreich
beendet wurden. Dieser Aufbau benötigt für 4x4 Frozen Lake zwischen 6000 und 42000
Iterationen, was zwischen einer und sechs Sekunden benötigt. Abbildung
\ref{fig:frozen-lake_q_learning} visualisiert den typischen Trainingsablauf, wobei der
durchschnittliche Reward aus 20 Spielen gezeigt wird.

% TODO verschiedene farben erklären
\begin{figure}[htp]
\centering
\includegraphics[scale=0.7]{res/frozen_lake_reward_tabular_q.png}
\caption{Ergebnisse Tabular-Q-Learning FrozenLake-v0}
\label{fig:frozen-lake_q_learning}
\end{figure}


\subsection{Tic Tac Toe}
Neben der Umsetzung von Frozen Lake wurde ebenfalls eine Tic Tac Toe Implementation
entwickelt, in dem ein Agent gegen einen zufällig spielenden Gegner trainiert und getestet
wurde. Unter \url{https://github.com/Bluemi/tictactoe_rl} ist diese Implementation
einsehbar. Das umgesetzte Verfahren ist eine Form der value based Verfahren unterscheidet
sich aber von Q-Learning.

Ebenfalls wurde ein Tic Tac Toe Environment entwickelt, das das Interface eines OpenAI
Gyms implementiert. Dieses kann unter
\url{https://github.com/dephiloper/independent-coursework-rl} in
\lstinline!preparation/05_v_learning/gym_tictactoe.py! aufgerufen werden.

Beim Training des Agent werden wie auch in Q-Learning Values ermittelt. Im Gegensatz zu
der im letzten Kapitel gezeigten Q-Learning Methode werden hier allerdings die Values
unterschiedlicher States und nicht die Values der Actions ermittelt. Die zu ermittelnde
Funktion ist nicht mehr $Q(s, a)$, sondern $V(s)$. Das Verfahren setzt dafür allerdings
voraus, dass aus einem gegebenen State und einer gegebenen Action der Folgezustand
berechnet werden kann, was durch das deterministische Spiel Tic Tac Toe gegeben ist.

Die Approximationen der Values der States werden in einem Dictionary gespeichert und mit
dem Startwert 0 initialisiert. Anschließend wird ein Spiel gegen einen zufällig spielenden
Gegner gespielt, wobei alle erreichten States zwischengespeichert werden. Am Ende des
Spieles wird ein Reward vergeben, der davon abhängt, ob gewonnen oder verloren wurde.

Daraufhin werden alle $V(s)$ Werte der zwischengespeicherten States in Richtung des
erhaltenen Rewards verändert. Um zu bestimmen wie stark sich der erhaltene Reward auf $V(s)$
auswirkt wird ein $step\_size$ Parameter verwendet, der wie in Zuweisung 
\ref{aln:ValueUpdate} 
benutzt wird.
\begin{align}
  V(s) \leftarrow V(s) + step\_size * (reward - V(s))
  \label{aln:ValueUpdate}
\end{align}
\noindent

Über den Trainingsprozess hinweg wird der step\_size Parameter stufenweise herabgesetzt,
um das Verfahren konvergieren zu lassen.

Damit der trainierte Agent sein gewonnenes Wissen exploiten kann, berechnet er aus den
zur Verfügung stehenden Actions die daraus resultierenden States und wählt die Action, die
zum State mit höchstem Value führt.

\subsubsection{Evaluierung}
Um das Verfahren zu testen wurden Agenten mit unterschiedlicher Anzahl an Trainingsspielen
trainiert, wobei ein Agent entweder immer das Spiel eröffnete oder den zweiten Zug
hatte. Als step\_size wurde initial 0.1 genutzt. Nach jedem Spiel wurde die step\_size
dann mit einem decay multipliziert, um die Konvergenz der State Values zu garantieren.

Tabelle \ref{tab:win_probability_tictactoe_begin} zeigt die Gewinnwahrscheinlichkeiten eines
trainierten Agenten, der gegen einen zufällig spielenden Gegner getestet wird. Jede Zelle
enthält zwei Gewinnwahrscheinlichkeiten. Die erste gibt die Gewinnwahrscheinlichkeit für
den Fall an, dass der Agent das Spiel eröffnet hat, die zweite die
Gewinnwahrscheinlichkeit, falls der Gegner das Spiel eröffnet hat.
\begin{table}[ht]
  \begin{center}
    \begin{tabular}{l | c | c | c}
      Anzahl Trainingsspiele & Gewonnen & Verloren & Unentschieden \\
      \hline
      100 & 68.9\% / 40.8\% & 14.3\% / 54.5\% & 16.9\% / 4.7\% \\
      % 500 & 80.7\% / 53.6\% & 28.6\% / 37.1\% & 14.0\% / 9.3\% \\
      1.000 & 83.6\% / 58.4\% & 10.1\% / 26.1\% & 6.4\% / 15.5\% \\
      10.000 & 94.6\% / 77.3\% & 0.5\% / 7.7\% & 4.9\% / 15.0\% \\
      100.000 & 98.1\% / 83.0\% & 0.0\% / 1.5\% & 1.9\% / 14.6\% \\
      1.000.000 & 99.1\% / 87.7\% & 0.0\% / 0.7\% & 0.9\% / 11.5\% \\
    \end{tabular}

    \caption[Gewinnwahrscheinlichkeiten Tic Tac Toe]{Gewinnwahrscheinlichkeiten Tic Tac
    Toe}
    \label{tab:win_probability_tictactoe_begin}
  \end{center}
\end{table}

\noindent
Eine weitere Steigerung der Trainingsspiele führt nicht mehr zu einer Änderung der
Gewinnwahrscheinlichkeiten. In allen gezeigten Experimenten ist die
Gewinnwahrscheinlichkeit höher, wenn der Agent beginnt und die Wahrscheinlichkeit zu
verlieren größer, wenn man nicht beginnt.

Auch ist deutlich erkennbar, dass eine erhöhte Anzahl von Trainingsspielen, die
Gewinnwahrscheinlichkeit des Agenten steigert. Während die Wahrscheinlichkeit zu gewinnen
beim beginnenden Spieler nach 100 Trainingsspielen noch bei 68.9\% liegt, liegt sie nach
100.000 Trainingsspielen bei 98.1\%. Auch sinkt die Wahrscheinlichkeit zu verlieren mit
zunehmendem Training und nach 100.000 Trainingsspielen ist der beginnende Agent nicht mehr
zu schlagen.

Interessant ist vor allem, dass auch bei einer Millionen Trainingsspielen der nicht
beginnende Agent verlieren kann. Dies kann gegen einen zufällig spielenden Gegner
vernünftig sein, da ein kleines Risiko einzugehen dazu führen kann, dass deutlich häufiger
gewonnen wird.


\subsection{Limitierungen von Tabular-Q-Learning}
Nach Lapan\cite[~S.192]{L2018} ist die Einsetzbarkeit dieser Verfahren dadurch
beschränkt, dass es in vielen Environments zu viele unterschiedliche States gibt, sodass
dessen Values oder die Values der Actions in diesen States nicht mehr ausreichend
approximiert werden können.

Dies kann durch die Betrachtung komplizierterer Environments verdeutlicht werden. Lapan
\cite[~S.192]{L2018} zeigt, dass ein Videospiel mit einer Auflösung von 210x160 Pixeln
mehr als $10^{70802}$ mögliche States besitzten kann. Daraus ergibt sich für
kompliziertere Environments, dass das Tabular-Q-Learning nicht praktikabel ist.

\newpage
\section{Deep Q-Learning}
\label{sec:dqn}
Die Limitierung von Tabular-Q-Learning wird in dem nächsten vorgestellten Verfahren \grqq
Deep-Q-Learning\grqq überwunden indem ein neuronales Netz eingeführt wird, um die Values
von Actions $Q(s, a)$ zu bestimmen. Diese werden nicht mehr in einer Tabelle gespeichert
und gelesen, sondern von einem Model berechnet, das zwischen ähnlichen States
abstrahieren kann.

In diesem Kapitel wird zuerst eine Einordnung des Verfahrens vorgenommen, um anschließend
das Verfahren aufbauend auf Tabular-Q-Learning zu beschreiben. Am Ende werden die	
Implementierung eines Agenten für das Pong Environment und die damit erzielten Ergebnisse
vorgestellt.

\subsection{Einordnung}
Deep Q-Learning funktioniert ähnlich, wie das schon vorgestellte Tabular-Q-Learning. Auch
hier werden wahrgenommene Observations abgebildet auf Values unterschiedlicher Actions, um
Actions, welche einen hohen Reward versprechen, wählen zu können. Damit ist
Deep-Q-Learning value-based.

Wie auch für Tabular-Q-Learning können vergangene Trainingsdaten verwendet werden, um die
Values der unterschiedlichen Actions zu approximieren, wodurch Deep-Q-Learning zu den
off-policy Learning Verfahren zählt.

\subsection{Verfahren}
\label{sec:deepq-procedure}
Im Grunde läuft der Trainingsprozess ähnlich ab, wie bei Tabular-Q-Learning. Zuerst wird
das neuronale Netz initialisiert, um anschließend zufällige Actions im Environment
auszuprobieren und somit ein Tupel aus wahrgenommenem State, gespielter Action, erhaltenem Reward
und neu erreichten Folgestate zu erhalten.

Daraufhin wird erneut der schon bei Tabular-Q-Learning genutzte Zusammenhang
(\ref{aln:DeepQ}) ausgenutzt, um die Ergebnisse des neuronalen Netzes anzupassen.
Die Bezeichnungen der Variablen sind identisch zu denen im Tabular-Q-Learning. Der State
$s$ ist der wahrgenommene State, $a$ ist die gewählte Action, $r$ ist der dadurch
erhaltene unmittelbare Reward und $s'$ ist der Folgestate, der aus dem Anwenden der
gewählten Action hervorgeht.
\begin{align}
  Q(s, a) \leftarrow r_{s,a} + \gamma \max_{a' \in A}Q(s', a') \label{aln:DeepQ}
\end{align}
\noindent
In Tabular-Q-Learning wurde anschließend der Wert $Q(s, a)$ der Tabelle angepasst. Da 
$Q(s, a)$ nun durch ein neuronales Netz implementiert ist, kann $Q(s, a)$ nicht dem
gefundenen Wert zugewiesen werden. Anstelle dessen wird ein Loss formuliert, auf dem
das Netz mit Stochastic Gradient Descent trainiert wird. Die Formel für diesen Loss
unterscheidet sich abhängig davon, ob es sich bei $s'$ um einen Endzustand handelt oder
nicht. Ist $s'$ ein Endzustand, so wird der Loss (\ref{aln:DeepQLoss1}) verwendet.
Andernfalls wird der Loss aus (\ref{aln:DeepQLoss2}) eingesetzt.
\begin{align}
  L & = \left(Q(s, a) - r\right)^2 \label{aln:DeepQLoss1} \\
  L & = \left(Q(s, a) - \left(r + \gamma \max_{a'} Q(s', a')\right)\right)^2 \label{aln:DeepQLoss2}
\end{align}
\noindent
Der Wert $Q(s, a)$ ist die Vorhersage des Netzes über den long-term Reward. Ist $s$ ein
Endzustand, so ist der long-term Reward gleich dem unmittelbaren Reward $r$, da nach einem
Endzustand kein Reward mehr erreicht werden kann. Daraus leitet sich der in
(\ref{aln:DeepQLoss1}) gezeigte Loss für einen Endzustand ab. Hier wird der Mean Squared
Error zwischen der Vorhersage des Netzes und dem tatsächlich zu erwartenden Reward
eingesetzt.

Handelt es sich bei $s$ nicht um einen Endzustand, so muss der long-term Reward des
Folgezustandes $s'$ hinzuaddiert werden, um den long-term Reward in $s$ zu berechnen. Aus
diesem Grund wird der Value der besten Action des Folgezustandes, skaliert mit dem
discount factor $\gamma$, auf den unmittelbaren Reward addiert. Dabei wird das Netz selbst
verwendet, um die Action Values des Folgestates zu berechnen.

Diesen Vorgang wiederholt man, bis die vom Netz vorhergesagten Q-Values den
tat\-säch\-lich\-en Values der Actions entsprechen.

Dies ist die grundlegende Funktionsweise von Deep-Q-Learning, allerdings muss nach
\cite[~S.202]{L2018} der Algorithmus, um in der Praxis zu funktionieren, noch weiter
verbessert werden. Wählt man bei der Exploration ausschließlich zufällige Actions, so
werden interessante Situationen nur sehr selten erreicht und es werden sehr viele Samples
benötigt, um zu einem zufriedenstellenden Ergebnis zu kommen. Aus diesem Grund wird eine
decaying-$\epsilon$-greedy Exploration Strategie benutzt, um gegen Ende des
Trainingsprozesses die schon gelernten Actions des Netzes zu verwenden, damit neue
Erkenntnisse gewonnen werden können.

Eine weitere Optimierung, die den Trainingsprozess verbessert, ist die Verwendung eines
Replay Buffers, in dem Erfahrungen gespeichert werden, um eine breitere Menge an
Trainingsdaten zu erhalten. Nach Lapan \cite{L2018} tritt ohne diesen Replay Buffer das
Problem auf, dass sich die gesammelten Daten aus ein Episode zu stark ähneln, sodass das
Training des Netzes suboptimal funktioniert.

Die letzte Änderung, die vollzogen werden muss, bezieht sich auf die Berechnung des Loss
Wertes aus (\ref{aln:DeepQLoss2}). Für die Berechnung des Losses, mit dem das Netz $Q$
trainiert wird, wird die Approximation des Folgestates von $Q$ selbst verwendet. Durch
diese Selbstreferenzierung kann es passieren, dass das Training instabil wird und nicht
konvergiert. Um dieses Problem zu lösen wird ein neues Netz $Q'$ eingeführt, das als
Target Netz bezeichnet wird. Dieses Netz wird für die Loss Berechnung verwendet, wie in
\ref{aln:DeepQTarget} gezeigt.
\begin{align}
  L & = \left(Q(s, a) - \left(r + \gamma \max_{a'} \textbf{Q'(s', a')}\right)\right)^2
  \label{aln:DeepQTarget}
\end{align}
\noindent
Nach einer gewissen Anzahl an Trainings werden die Gewichte des Target Netzes mit den
Gewichten des trainierten Netzes überschrieben, sodass sich die Netze synchronisieren.

\subsection{Implementierung}
Mit den eben beschriebenen Änderungen wurde ein Deep-Q-Learning Algorithmus für ein Pong
Environment implementiert und getestet. Der Quelltext kann unter
\url{https://github.com/dephiloper/independent-coursework-rl/tree/master/preparation/07_deep_q_learning}
gefunden werden.

\subsubsection{Das Roboschool Pong Environment}
Für die Umsetzung des Deep-Q-Learning Verfahrens wurde ein Pong Environment verwendet, das
unter \url{https://github.com/openai/roboschool} zu finden ist. In diesem Environment wird
das bekannte Pong Spiel trainiert. Zwei Spieler bewegen jeweils einen Balken mit dem ein
Ball auf die Seite des Gegners gestoßen wird. Verfehlt einer der Spieler den Ball, sodass
dieser auf die Seite hinter dem Spieler trifft, so hat der verfehlende Spieler verloren
und dessen Gegenspieler gewonnen. Abbildung \ref{fig:pong_env} zeigt das gerenderte
Environment.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.4]{res/pong_env.png}
\caption{Roboschool Pong Environment}
\label{fig:pong_env}
\end{figure}
\noindent
In der verwendeten Variante von Pong können die Spieler ihren Balken nicht nur parallel zu
ihrer Grundlinie bewegen, sondern auch auf ihren Gegenspieler zu oder von ihm weg. Dadurch
lässt sich der Ball stärker in die gegnerische Richtung spielen. Als Interface bietet das
Environment dabei die Möglichkeit den Balken in beliebige Richtungen mit beliebiger
Geschwindigkeit zu bewegen. Damit handelt es sich um kontinuierliche Actions, die sich mit
dem beschriebenen Verfahren nicht umsetzen lassen. Aus diesem Grund wurde die Menge der
Actions auf neun diskrete Actions beschränkt, mit denen sich der Balken in acht
verschiedene Richtungen bewegen lässt oder stehen bleibt.\\
Um zwischen den unterschiedlichen Actions unterscheiden zu können, erhält der Agent vom
Environment die Koordinaten und Geschwindigkeiten der Balken sowie des Balles. Zusätzlich
wird die Nummer des aktuellen Frames als Information geliefert.\\
Reward erhält der Agent vor allem dann, wenn er den Ball am gegnerischen Spieler vorbei
auf dessen Grundlinie stößt. Zusätzlich gibt es einen kleineren Reward, wenn der Ball mit
dem Balken getroffen wird.

\subsubsection{Agent}
% Struktur des neuronalen Netzes
Für die Lösung des Pong Environments wurde das Deep-Q-Learning Verfahren implementiert,
wobei die Q-Values von einem neuronalen Netz berechnet wurden. Es wurden unterschiedliche
Konfigurationen getestet, wobei die Anzahl der Layer zwischen drei und vier, als auch die
Anzahl der Knoten pro Layer variiert wurden. Als Aktivierungsfunktion wurde die
ReLu-Funktion verwendet. Da die Observation nicht als Bild gegeben ist, werden keine
Convolutional Layer verwendet. Die Learning Rate wurde auf $10^{-4}$ gesetzt.\\
Als discount factor wurde $\gamma = 0.99$ und eine Replaybuffergröße von 10000
Samples gewählt. Nach 1000 Frames wird das Target Net durch das trainierte Netz geupdated.

\subsubsection{Ergebnisse}
Abbildung \ref{fig:pong_results} zeigt eine Auswahl der Testläufe im Pong Environment.
Eine Abbildung aller Testläufe ist im Anhang in Abbildung \ref{fig:pong_all_results} zu
sehen.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.4]{res/pong_results.png}
\caption{Ergebnisse Pong}
\label{fig:pong_results}
\end{figure}
\noindent
Die verschiedenen Versuche unterscheiden sich in der Netzarchitektur. Hellblau zeigt die
Ergebnisse eines Agenten mit 3 Hidden Layern mit jeweils 64 Knoten. Wie zu erkennen ist,
stagniert der Prozess bei einem Reward von -7.\\
Besser funktionieren die folgenden Experimente, bei denen 4 Hidden Layer verwendet wurden.
Grau zeigt den durchschnittlichen Reward mit 64 Knoten pro Layer und Grün den Reward für
ein Netz mit 128 Knoten pro Layer.\\
Damit wurde kurzzeitig ein durchschnittlicher Reward von bis zu 6.9 erreicht. Der
trainierte Agent kann unter \url{https://www.youtube.com/watch?v=xp4XtrYNKzQ} gesehen
werden.

\newpage
\section{Policy Gradients}
Policy Gradients beschreibt eine Unterfamilie und somit Gruppe von Reinforcement Learning 
Methoden aus dem Bereich der policy learning Verfahren. Anders als die beiden zuvor
beschriebenen Verfahren Tabular-Q-Learning und Deep Q-Learning fokussieren sich policy
Verfahren auf das Erlernen einer direkten policy $\pi(s)$. Dies bringt den Vorteil mit sich,
dass Policy Gradients auf Environments mit kontinuierlichen action space, sprich Actions mit
Werterepräsentationen in Gleitkommadarstellung, angewandt werden können (siehe
\ref{sec:action} \nameref{sec:action} - Einschlagwinkel eines Lenkrads). Q-Learning Verfahren
hingegen weisen jedem State oder jeder Action immer eine Value zu, was das Optimierungsproblem
bei einem kontinuierlichen action space deutlich erschwert \cite[~S.242]{L2018}. Ein
zusätzlicher Vorteil der Policy Gradients ist die stochastische Eigenschaft von Environments.
Anders als Q-Learning wird es mit Policy Gradients möglich, die zugrundeliegende
Wahrscheinlichkeitsverteilung des Environments besser aufzugreifen, da das Verfahren, statt
anhand von Erwartungswerten des aktuellen und zukünftigen Rewards zu rechnen, mit einer
Wahrscheinlichkeitsverteilung von Actions arbeitet. In der nachfolgenden Abbildung 
(\ref{fig:prob-dist-pg}) wird genau diese Wahrscheinlichkeitsverteilung von diskreten Actions
nach Anwendung der Policy Approximation visualisiert.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.4]{res/policy_approxim.png}
\caption{Wahrscheinlichkeitsverteilung von Actions mittels Policy Approximation}
\source{\cite[~S.243 - Chapter 9 - Figure 1]{L2018}}
\label{fig:prob-dist-pg}
\end{figure}

Zusätzlich kommt, durch die Repräsentation der Actions als Wahrscheinlichkeit, der Vorteil einer
\textit{smooth representation} des Netzes zum tragen. Das heißt, würde der Output des Netzes über
diskrete Werte ausgedrückt werden, kann schon eine minimale Änderung an den Gewichten des Netzes 
zu einer komplett anderen Action bei der Vorhersage führen. Aufgrund der
Wahrscheinlichkeitsverteilung bewirken Anpassungen hingegen nur moderate Änderungen in der
resultierenden Verteilung des Output \cite[~S.243]{L2018}.


\subsection{REINFORCE Method}
Das nachfolgend beschriebene Verfahren REINFORCE oder Monte Carlo Policy Gradient, weist
deutliche Ähnlichkeiten zur bereits erläuterten \nameref{sec:cross-entropy-method} auf.
Genau wie das Cross-Entropy Verfahren, erhält das Netz als Input die Observation des
Environments und liefert eine Wahrscheinlichkeitsverteilung der Actions an den Agent
zurück. Der Unterschied kommt durch die in Formel \ref{aln:PG} festgehaltene Definition
des $PG$ (Policy Gradient) zustande.

\begin{align}
\mathcal{L}=-Q(s,a)\log\pi(a|s)
\label{aln:PG}
\end{align}

PG definiert dabei die Richtung, in welche die Gewichte des Netzes verschoben werden müssen,
um die Policy im Hinblick auf den kumulierten total reward zu verbessern. Die Skalierung des
Gradienten ist hierbei proportional zur Value der zu wählenden Action $Q(s,a)$
und der Gradient selbst entspricht in der Formel dem Gradienten der logarithmischen
Wahrscheinlichkeit der ausgewählten Action \cite[~S.244]{L2018}. Dadurch wird es möglich
die Wahrscheinlichkeit von Actions mit einem hohen total reward zu erhöhen und im Gegenzug
die Wahrscheinlichkeit der Action mit einem niedrigen oder negativen total reward zu
verringern.

Ein weiterer Unterschied zur Cross-Entropy-Method ist, dass diese als PG Methode mit einem
$Q(s,a)=1$ für Episoden mit hohem total reward und $Q(s,a)=0$ für schlechte Episoden
definiert werden kann, wohingegen REINFORCE wirkliche Berechnungen für $Q(s,a)$ durchführt
und somit eine detaillierte Unterteilung der Episoden ermöglicht.

\subsubsection{Einordnung}
Wie bereits erwähnt ist die REINFORCE Method ein policy learning Verfahren. Das Verfahren ist
on-policy, da ausschließlich anhand neu aufgezeichneter Trainingsdaten gelernt werden kann.
Demzufolge kommt das Verfahren ohne die Verwendung eines Replay Buffers (siehe \autoref{sec:deepq-procedure}) aus. Des Weiteren erfolgt die Einordnung als model-free Methode, da nicht versucht wird
Annahmen zu zukünftigen States oder Rewards zu treffen. Prinzipiell basieren alle Policy
Gradient Verfahren auf der in Formel \ref{aln:PG} festgelegten Definition, wobei
ausschließlich Anpassungen am Kernkonzept des Verfahrens vorgenommen werden
\cite[~S.244 f.]{L2018}. Anders als die bereits behandelten value-learning Verfahren ist
kein explizites Explorieren notwendig, da das REINFORCE Verfahren durch die Nutzung einer
Wahrscheinlichkeitsverteilung über alle Actions ein automatisches \grqq Erforschen\grqq 
des Environments und der zu tätigenden Actions vollzieht. Bei einer Initialisierung mit
zufälligen Gewichten gibt das Netz eine uniforme Wahrscheinlichkeitsverteilung zurück und
somit die Verhaltensweise eines random Agents.


\subsubsection{Verfahren}
Zu Beginn des Verfahrens werden die Parameter des Netzes $\theta$ als zufällige Gewichte
initialisiert. Nachfolgend werden, genau wie bei der value-based Methode Q-Learning,
multiple Episoden im Environment gespielt wobei ein Batch an SARS-Tupeln produziert wird.
Anschließend erfolgt die Berechnung des discounted total reward für die Folgeschritte eines
jeden steps, wobei ein step einem SARS-Tupel in einer Episode entspricht. Diese Berechnung
wird durch die nachfolgend aufgeführte Formel \ref{aln:reinforce-total-reward} beschrieben,
in welcher $t$ für den aktuellen step und $k$ die darin befindliche Episode des steps
definiert. 

\begin{align}
Q_{k,t}=\sum_{i=0} \gamma^{i}r_{i}
\label{aln:reinforce-total-reward}
\end{align}

Da im REINFORCE Verfahren die Berechnung des discounted total reward $Q_{k,t}$ direkt durch
Aufsummierung des aktuellen Rewards und der in zukünftigen Zuständen erhaltenen Rewards
ermittelt werden kann, ist eine Approximation von $Q$ wie in Deep-Q-Learning nicht notwendig,
was zusätzlich die Verwendung des Target Netz überflüssig macht (siehe 
\autoref{sec:deepq-procedure}). Mittels $Q_{k,t}$ kann daraufhin die Berechnung des Loss
anhand der in Formel \ref{aln:reinforce-loss} festgehaltenen Gleichung ermittelt werden.

\begin{align}
\mathcal{L}=-\sum_{k,t}Q_{k,t}\log(\pi(s_{k,t},a_{k,t}))
\label{aln:reinforce-loss}
\end{align}

Auf Grundlage dieser Berechnung wird ein Update der Gewichte mittels Stochastic
Gradient Descent durchgeführt und somit das Minimieren des Loss erzielt. Das Verfahren
wird daraufhin solange ausgeübt bis es konvergiert.

\subsubsection{Limitierungen von REINFORCE}
\label{sec:limit-reinforce}
Zu den Limitierungen des REINFORCE Verfahrens zählen laut Lapan\cite[~S.252]{L2018}, dass für den
Trainingsprozess jede Episode immer bis zum Schluss gespielt werden muss. Andernfalls kann der
discounted total reward nicht vollständig berechnet werden. Des Weiteren können bessere Resultate
erzielt werden, umso länger trainiert wird. Dies kann bei Environments mit lang andauernden Episoden
zeitaufwendig werden, da nach jedem Trainingsschritt immer neue Trainingsdaten vom Environment
generiert werden müssen. Um diesen Problem entgegen zu wirken existiert ein weiteres Verfahren, die
\textit{Actor-Critic Method}, welche im nächsten Kapitel erläutert wird. 

Zusätzlich weist die REINFORCE Method eine hohe Varianz bei der Berechnung des Gradienten auf.
Dies kommt aufgrund der Skalierung des Gradienten mittels des discounted total rewards $Q(s,a)$ zustande.
Hauptgrund dafür ist, dass der Wertebereich des zu erhaltenden Rewards von Environment zu Environment
stark variiert und in diversen Environments zu Beginn der Anteil an Punishment (negativer Reward) den
Erhalt des positiven Reward deutlich dominiert. Durch die Einführung einer Baseline, einem Wert, der
von dem total reward $Q$ subtrahiert wird, kommt es zu einer Stabilisierung des Verfahrens. Neben
diesen Erweiterungen existieren weitere, welche im nachfolgenden Kapitel analysiert werden. Aufgrund
der Instabilität des Trainingsprozesses findet das REINFORCE Verfahren selten Anwendung in komplexen
Environments.   
\newpage


\section{Actor Critic}
Das letzte zu erläuternde Verfahren reiht sich als Erweiterung in die Policy Gradients Familie ein.
Bei der \textit{Actor Critic Method} handelt es sich um ein state-of-the-art Reinforcement Learning
Verfahren mit der Bestrebung die Stabilität und den Prozess des Konvergierens im Trainingsablauf
zu verbessern. Wie bereits in \autoref{sec:limit-reinforce}: \nameref{sec:limit-reinforce} beschrieben
kann durch die Einführung einer Baseline, sprich einem Wert, welcher in der Berechnung des Loss von dem
discounted total Reward $Q(s,a)$ abgezogen wird, die Varianz des Gradienten reduziert werden, wodurch
der Trainingsprozess robuster wird. Die Baseline kann dabei eine Konstante sein, die dem Mittelwert 
aller discounted total Rewards entspricht oder als gleitender Mittelwert (moving average) realisiert
werden, welcher während des Trainingsverlauf stetig angepasst wird. Eine weitere Option ist den Baseline
Wert zustandsabhängig zu realisieren, sprich in jedem Zustand wird der Wert $V(s)$ ermittelt, der Auskunft
darüber gibt, wie gut dieser Zustand einzuschätzen ist. Anschließend kann ein \textit{advantage} Wert
$A(s,a)$ ermittelt werden, indem von dem berechneten discounted total Reward $Q(s,a)$ die \textit{Value}
des States $V(s)$ abgezogen wird \cite[~S.268 ff.]{L2018}. Hierbei ist zu erwähnen das die Berechnung von
$Q(s,a)$ genau wie im REINFORCE Verfahren (siehe \autoref{aln:reinforce-total-reward}) umgesetzt wird.
Die nachfolgende Formel \ref{aln:actor-critic-advantage} veranschaulicht den Zusammenhang zwischen der
Value des States $V(s)$ und dem discounted total Reward $Q(s,a)$.

\begin{align}
Q(s,a)=V(s)+A(s,a)
\label{aln:actor-critic-advantage}
\end{align}

Der advantage Wert $A(s,a)$ wird anschließend für die Skalierung des Gradienten verwendet und gibt Auskunft
darüber, wie viel besser oder schlechter eine gewählte Action im Kontrast zum durchschnittlichen Value des
States ausfällt. Um $A(s,a)$ zu berechnen muss initial $V(s)$ ermittelt werden. Dafür wird neben dem bereits
bestehenden ein weiteres neuronales Netz eingeführt, welches den Wert von $V(s)$ approximiert und sich damit
an der Verfahrensweise von \nameref{sec:dqn} orientiert.

In der Realisierung des Actor-Critic Verfahrens kann dieses mittels einer Zusammenführung beider Netze als
\textit{shared net} umgesetzt werden  \cite[~S.269]{L2018}. Im Hinblick darauf wird die erhaltene
Observation als Input in ein \textit{Common net} gegeben, welches im Falle von Bildern als Input aus
multiplen Convolutional Layern bestehen könnte. Anschließend werden die ermittelten Features sowohl an
ein \textit{Policy net} $\pi(a|s)$ als auch an ein \textit{Value net} $V(s)$ zur Verarbeitung
weitergegeben. Die beschriebene Architektur des Verfahrens ist in \autoref{fig:actor-critic}
visualisiert.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.5]{res/actor-critic.png}
\caption{Actor Critic - shared net Architektur}
\source{\cite[~S.268 - Chapter 10 - Figure 5]{L2018}}
\label{fig:actor-critic}
\end{figure}

\newpage
Um den Trainingsprozess umzusetzen, kann die selbe Prozedur wie bei Deep-Q-Learning verwendet werden, wobei
die Bellman Equation zur Ermittlung des discounted total rewards $Q(s,a)$ angewandt wird. Anschließend wird
der Mean-Square-Error Loss zwischen berechnetem und vorhergesagtem $Q(s,a)$ minimiert, um die Approximation
von V(s) zu bewirken.


\subsection{Einordnung}
Bei dem Actor-Critic Verfahren handelt es sich um eine policy learning Methode, da final Actions anhand der
Wahrscheinlichkeitsverteilung des Policy net ausgewählt werden. Aufgrund der parallelen Nutzung des Value
net kann das Verfahren aber ebenfalls als Mischform betrachtet werden. Genau wie bei der REINFORCE Method
kann ausschließlich aufgrund neu aufgezeichneter Daten aus dem Environment trainiert werden (on-policy).
Das Verfahren fällt des Weiteren in den Bereich der model-free Verfahren, da kein Model zu möglichen
Folgezuständen und Rewards aufgebaut wird. 

\subsection{Verfahren}
Wie bei dem Großteil aller erläuterten Verfahren werden zu Beginn die Netzwerkparameter $\theta$ mit
zufälligen Werten initialisiert. Anschließend werden $N$ Schritte im Environment anhand der aktuellen
Policy $\pi(s|a)$ gespielt, wobei jeweils ein Tupel aus State $st$, Action $at$ und Reward $rt$
gespeichert werden. Nach dem Abschluss einer Episode werden anschließend die vollzogenen Schritte in
umgekehrter Reihenfolge verarbeitet. Darauf folgend werden die Gewichte des Netzes aktualisiert, indem
die akkumulierten Gradienten beider Netze in Richtung des Policy Gradients $\partial\theta_\pi$
und entgegen der Richtung des Value Gradients $\partial\theta_v$ verschoben werden. Diese Schritte
werden anschließend solange wiederholt, bis das System konvergiert.

Das Policy net wird dabei als \textit{actor} bezeichnet, da man von diesem ableiten kann, welche
Action ausgeführt werden soll. Das Value net wird hingegen als \textit{critic} bezeichnet, da es mit
diesem möglich wird, zu bewerten wie gut die ausgeführten Actions waren, um somit bei einem hohen
advantage Wert die Wahrscheinlichkeiten der gewählten Action mehr in diese Richtung zu bewegen.

\subsection{Erweiterungen}
Obwohl das Actor Critic Verfahren bereits als Erweiterung zum klassischen Policy Gradient Ansatz
gezählt wird existieren weitere Maßnahmen zur Verbesserung und Beschleunigung der Methode.

\paragraph*{Entropy Bonus} 
\noindent
\newline
Durch die Einführung des Entropy Bonus wird der explorierende Ansatz des Verfahrens bestärkt.
Der Bonus wird als Entropy Wert zur bestehenden Loss-Funktion hinzugefügt (siehe
\ref{aln:actor-critic-entropy-bonus}):

\begin{align}
\mathcal{L}_H=\beta\sum_i\pi_\theta(s_i)\log_{\pi_\theta}(s_i)
\label{aln:actor-critic-entropy-bonus}
\end{align}

Da diese Funktion an ihrem Minimum ankommt, sobald die Wahrscheinlichkeitsverteilung uniform,
sprich gleichverteilt ist, kann durch anfügen des Ergebnisses zur Loss-Funktion der Agent davon
abgehalten werden sich \grqq zu sicher\grqq bei der Berechnung von Actions zu sein. Im
Umkehrschluss bedeutet dies, dass bei einer Wahrscheinlichkeitsverteilung bei der ausschließlich
eine Action zu 100 Prozent als wählbare Action identifiziert wurde, der berechnete Loss nicht
einem Ergebnis von 0 entspricht \cite[~S.269 f.]{L2018}.

\paragraph*{Multiple Environments} 
\noindent
\newline
Eine zusätzliche Erweiterungsmöglichkeit im Bereich der Stabilität des Verfahrens ist die Nutzung
von mehreren parallelen Environments. Dadurch können simultan mehrere Observations gesammelt werden
was aufgrund der on-policy Eigenschaft des Verfahrens den Trainingsprozess deutlich beschleunigt.
Zusätzlich erhalten die aufgezeichneten Daten eine gewisse Unabhängigkeit voneinander, da Observations
abwechselnd aus den jeweiligen Environments gesampled werden können. Diese Erweiterungsmöglichkeit wird
auch als Asynchronous Advantage Actor Critic oder in Kurzform A3C bezeichnet.

\newpage
\section{Projektarbeit}
Nach der Aufarbeitung der theoretischen Bereiche von Reinforcement Learning sowie der Umsetzung von
Algorithmen und Erweiterungen befasst sich der abschließende Teil des Independent Coursework damit,
ein eigenes Environment zu entwickeln. Anhand dieses Environments soll ein Reinforcement Learning
Verfahren implementiert werden, mit dem Ziel die Problemstellung des Environments erfolgreich zu 
lösen. In diesem Kapitel erfolgt initial die Festlegung auf ein Spiel für die Entwicklung des
Enviroments. Nachfolgend wird auf die Entwicklung des Environments und die darin enthaltenen
Funktionalitäten eingegangen. Des Weiteren wird aus den bereits vorgestellten Algorithmen ein
Verfahren gewählt und dieses in dem entwickelten Environment angewandt. 

\subsection{Mögliche Spiele}
Wie in der Zielsetzung dieser Ausarbeitung beschrieben, soll nachfolgend ein Spiel ausgewählt werden,
für welches mittels der Open AI Bibliothek ein Environment umgesetzt wird. Für die Wahl des Spieles
wurden zu Beginn folgende Spiele in Betracht gezogen:

\subsubsection{Doom}
Doom (id Software, 1993) ist ein First-Person Shooter, welcher im Dezember 1993 auf dem Betriebssystem
MS-DOS veröffentlicht wurde. Im Spiel steuert man einen Space-Marine, welcher als ''Doomguy'' bezeichnet
wird und dessen Aufgabe es ist, sich durch eindringende Dämonenhorden aus der Hölle zu kämpfen
\cite{D1993}. Es existieren bereits Realisierungen in denen Agenten mittels Reinforcement Learning
Verfahren das Spiel lernen zu spielen. Diese verwenden meist alle eine, explizit für die Entwicklung
von künstlicher Intelligenz, bereitgestellte Modifikation des Hauptspiels - ''ViZDoom'' \cite{ViZDoom}. 
Diese ermöglicht es das Spiel in beschleunigter Form (7000 Bilder pro Sekunde) auszuführen. Des
Weiteren enthält die Modifikation die Implementierung einer eigens angefertigten API sowie weitere
durch die Community bereitgestellte Anpassungen.

Die Steuerung der Kernmechaniken im Spiel umfassen das nach links und rechts Drehen des Spielers,
das Bewegen in die vier Haupthimmelsrichtungen sowie das Abfeuern einer Waffe (sieben diskrete
Actions). Die Modifikation des Hauptspiels ist unter \url{https://github.com/mwydmuch/ViZDoom}
aufrufbar. Zusätzlich wäre auch die Anfertigung eines eigenen Environments in Doom möglich. Dafür
stehen weitere Portierungen zur Verfügung wie zum Beispiel \url{https://zdoom.org/index}, welches
das Spiel auf modernen Geräten ausführbar macht. 

\subsubsection{Teeworlds}
Bei dem Spiel Teeworlds handelt es sich um ein kostenloses online Mehrspieler Spiel, welches seine
Erstveröffentlichung am 27. Mai 2007 hatte. Das Spiel wurde am Anfang ausschließlich von dem
Entwickler Magnus Auvinen programmiert, 2010 als Open Source Projekt unter der BSD-Lizenz
veröffentlicht und seitdem von mehreren Entwicklern verwaltet. In Teeworlds nimmt man die Form
eines ''Tee'' an, manövriert diesen durch die Spielwelt, weicht gegnerischen Projektilen aus und
umgeht Stacheln. Neben dem Schusswechsel mit anderen Spielern definiert eine Greifhakenmechanik den
Hauptaugenmerk des Spiels. Zusätzlich erlaubt das Spiel das Aufsammeln von spielrelevanten Objekten
wie Herzen zum Auffüllen der Lebensenergie.

Die Hauptmechaniken zur Steuerung des Avatars bestehen aus der horizontalen Bewegung, einem Befehl
zum Springen sowie einer 360 Grad Ausrichtung des Greifhaken oder der ausgerüsteten Waffe.
Das Spiel ist unter \url{https://github.com/teeworlds/teeworlds} zu finden.

\subsubsection{Starcraft II}
Im Github Repository \url{https://github.com/Blizzard/s2client-proto} wurde eine API für das Spiel
Starcraft II der Firma Blizzard umgesetzt, welche eine vollständige Kontrolle des Spiels durch
externe Enitäten ermöglicht. Starcraft II (2010) ist ein Science Fiction Echtzeit-Strategiespiel,
in dem Einheiten von jeweils drei verschiedenen Spezies gegeneinander antreten. Neben dem
Management von Militärtruppen müssen Basen gebaut und Rohstoffe verwaltet werden. Die Steuerung
des Spiels ist deutlich komplexer als die der zuvor beschriebenen Spiele. Es existiert jedoch
bereits die Umsetzung eines mit Reinforcement Learning Verfahren nutzbaren Environments der Firma
DeepMind (\url{https://github.com/deepmind/pysc2}). 

\subsection{Festlegung auf ein Spiel}
Aufgrund der hohen Komplexität und der Vielschichtigkeit der Steuerung wurde sich gegen die Umsetzung
eines Environments für das Spiel Starcraft II entschieden. Die Spiele Teeworlds und Doom bieten
deutlich einfachere Verfahren zur Steuerung durch einen Agenten. Da bereits Realisierungen für Doom
existieren und das Spiel Teeworlds durch den öffentlich verfügbaren Source Code vollständig modifizierbar
ist, wurde Teeworlds abschließend für die Realisierung eines Environments ausgewählt. Zudem erfüllt
Teeworlds die zu Beginn festgelegte Anforderung eines Spieles in Echtzeit.

Nahezu die gesamte Codebase von Teeworlds ist in C++ entwickelt worden, wobei durch das Kompilieren
des Spiels zwei ausführbare Dateien entstehen, ein Teeworlds Client und ein Teeworlds Server. Der
Client kann ausschließlich in Kombination mit einem Server genutzt werden. Über eine UDP Verbindung
werden zwischen Client und Server Informationen zur Veränderung des Game State, der Spielerpositionen
oder über eingesammelte Objekte ausgetauscht.

\subsection{Umsetzung des Environments} % TODO Bruno
\begin{itemize}
\itemsep0pt	
\item custom environment (gym)
\item vereinfachte Version des Spiels $>$ was ist das Ziel?
\item Möglichkeit für andere Environment zu nutzen
\item die Aufgabe besteht darin, Schilde und Herzen einzusammeln und Level zu bestreiten
\item spikes bei denen man stirbt
\item Umsetung Multienv/Worker
\item Bilder aufzeichnen $>$ 4 Bilder = Observation
\item erläuterung Actions, Reward, Observation
\item Bilder veranschaulichen mehrere Environments offen
\item Synchronisierung Bild $>$ Reward (Problem mit falscher Synchronisierung)
\item evtl. wichtigste Methoden erläutern (Herausforderungen bei der Implementierung)
\item Umsetzung der Level
\item Beschreibung System Setup
\end{itemize}

\subsection{Wahl des Verfahrens} % TODO Bruno
\begin{itemize}
\itemsep0pt	
\item Wahl des Verfahrens, alle behandelten Verfahren Vergleichen
\item aufgrund des Spieles Vergleich der Verfahren
\item Festlegung auf Deep-Q-Learning
\item Bilder als Input $\rightarrow$ stack of images
\item evtl. wichtigste Methoden erläutern (Herausforderungen bei der Implementierung)
\item lisitings der Umsetzung
\item Erklärung Hyperparameter, Lernprozess, Lernstats
\item Umsetzung Erweiterungen zu Deep-Q-Learning
\item Vergleich, Trainingsdauer, Auswertung
\end{itemize}

\subsubsection{Noisy Network}
Als erste Erweiterung wurde eine Maßnahme zur Verbesserung der Exploration des Environments implementiert.
In dem Paper ''Noisy Networks für Exploration'' beschreiben Fortunato et al.\cite[~S.1 f.]{FAPMOGM2017}
ein Verfahren zum Erlernen der Explorationsmerkmale während des Trainings, statt auf die Verwendung von
Explorationsstrategien zurückzugreifen, wie $\epsilon$-greedy. Durch die Hinzunahme von Rauschen (noise)
zu den Gewichten der Fully-Connected Layer und dem anschließenden Anpassen der Gewichtsparameter des
Rauschens durch Backpropagation wird ein effizientes Explorieren möglich. Das Verfahren ist laut der
Authoren des Papers verhältnismäßig simpel in dessen Umsetzung und fügt ausschließlich minimal erhöhten
Overhead zur Vanillaversion des Deep-Q-Learning Verfahrens hinzu. Als wichtigste Erkenntnis wird im Paper
beschrieben, dass eine einzige Änderung des Gewichtsvektors eine konsistente und potenziell sehr komplexe
zustandsabhängige Änderung der Policy über mehrere Zeitschritte hinweg induzieren kann, wohingegen bei
Dithering-Ansätzen\footnote{einfachste und am häufigsten verwendete Ansätze zur Erforschung von greedy
Actions durch den Einsatz von zufälligem Dithering \cite[~S.2]{R2004}}, dekorreliertes (und im Falle von
greedy Verfahren, zustandsunabhängiges) Rauschen bei jedem Schritt zur Policy, sprich der Wahl einer
Action, hinzugefügt wird \cite[~S.2]{FAPMOGM2017}. 

\paragraph*{Implementierung}
\noindent
\newline
Die nachfolgende Implementierung orientiert sich an der Vorlage nach Lapan\cite[~S.179]{L2018}.
Zur Umsetzung des Noisy Layer wird die Klasse \textit{NoisyLayer} implementiert, die von der
Superklasse \lstinline!nn.Linear! erbt. Listing \ref{lst:noisy-layer} definiert den Konstruktor
der Klasse, in dem initial eine trainierbare Gewichtsmatrix $\sigma$ erzeugt wird. Durch den
Aufruf der \lstinline!self.register_buffer! Methode wird ein Tensor für das Netz erzeugt, welcher
während des Trainings nicht durch Backpropagation aktualisiert wird. Gleiches gilt für die noisy
Biases des Netzwerks. Der Startwert von sigma wird wie im Paper empfohlen \cite[~S.6]{FAPMOGM2017}
auf $\sigma=0.017$ gesetzt.

\begin{lstlisting}[language=Python, caption=Konstruktor Noisy Layer, label=lst:noisy-layer]
class NoisyLinear(nn.Linear):
	def __init__(self, in_features, out_features, sigma_init=0.017, 
	bias=True):
		
		super(NoisyLinear, self).__init__(in_features, out_features,
		bias=bias)
		
		self.sigma_weight = nn.Parameter(torch.full((out_features,
		in_features), sigma_init))
		
		self.register_buffer("epsilon_weight", 
		torch.zeros(out_features, in_features))
		
		if bias:
    		self.sigma_bias = nn.Parameter(torch.full((out_features,)
    		,sigma_init))
    		
    		self.register_buffer("epsilon_bias", 
    		torch.zeros(out_features))
    		
		self.reset_parameters()
\end{lstlisting}

Durch den Aufruf von \lstinline!self.reset_parameter! (\ref{lst:noisy_reset-params} werden die
Gewichte und Biases nach der im Paper beschriebenen Vorhergehensweise initialisiert. Dabei wird
jedem Gewicht ein Wert einer unabhängigen Gleichverteilung
$\mathcal{U}[-\sqrt{\frac{3}{p}},+\sqrt{\frac{3}{p}}]$ zugewiesen, wobei $p$ der Anzahl der
Input Nodes des Linear Layers entspricht.
 
\begin{lstlisting}[language=Python, caption=Reinitialisierung der Gewichte und Biases, 
label=lst:noisy_reset-params]
	def reset_parameters(self):
		std = np.math.sqrt(3 / self.in_features)
		self.weight.data.uniform_(-std, std)
		self.bias.data.uniform_(-std, std)
\end{lstlisting}

Die \lstinline!forward(self, input)! Funktion wird jedes mal aufgerufen, wenn mit der Instanz
des Netzes die Berechnung des Outputs vollzogen wird. Durch den Aufruf von 
\lstinline!normal_()! der Gewichte und Biases werden für beide Parametermengen 
\textit{Random Noise} Werte gesampled. Anschließend erfolgt mittels \lstinline!F.linear()!
die lineare Transformation der Input Daten, wie in der \lstinline!nn.Linear! Klasse 
\cite[~S.179 ff.]{L2018}.

\begin{lstlisting}[language=Python, caption=\textit{forward()} Funktion Noisy Layer, 
label=lst:noisy_forward]
	def forward(self, input):
	self.epsilon_weight.normal_()
	bias = self.bias
	if bias is not None:
	    self.epsilon_bias.normal_()
	    bias = bias + self.sigma_bias * self.epsilon_bias.data
		
	return F.linear(input, self.weight + self.sigma_weight *
	self.epsilon_weight.data, bias)
\end{lstlisting}

Die Verwendung der Layer funktioniert anschließend genau wie bei der Erstellung des Netzes mit
Linear Layer, wobei diese durch die \textit{NoisyLayer} Klasse ausgetauscht werden (Vergleich
). %TODO zitiere Erstellung Netz ohne Noisy Layer

\paragraph*{Auswertung}
\noindent
\newline


\subsubsection{Double DQN} % TODO Philipp
Beschreibung, Implementierungen, Graphen
\subsubsection{Prioritized Replay Buffer} % TODO Bruno
Beschreibung, Implementierungen, Graphen
\subsubsection{Dueling DQN} % TODO Bruno
Beschreibung, Implementierungen, Graphen
\subsubsection{N-step DQN} % TODO Philipp
Beschreibung, Grund des nicht nutzens

\subsection{Ergebnisse} % TODO Philipp	
\begin{itemize}
\itemsep0pt	
\item evtl. noch mehr Graphen mit Erläuterungen
\item Auswertung der Ergebnisse, Annahmen: lernt nur auswendig? spielt besser durch.. kann mehrere maps spielen
\item IC erfolgreich durchgeführt.. lol guck mal ein ^^
\end{itemize}

\subsection{Ausblick} % TODO Philipp
\begin{itemize}
\itemsep0pt	
\item andere Verfahren Umsetzen A2C, PPO, TRPO
\item continuous actions
\item schießen, grabling hook
\item alternative Ansätze Genetic Algorithms
\item env alleinstehend von vielen nutzbar
\end{itemize}

\newpage
%\bibliographystyle{plain}
\bibliographystyle{alpha}
\bibliography{references}
\newpage
\listoffigures 
\listoftables 

\newpage
\section{Anhang}

\begin{figure}[htp]
\centering
\includegraphics[scale=0.35]{res/pong_all_results.png}
\caption{Vollständige Ergebnisse mit Deep-Q-Learning im Pong Environment}
\label{fig:pong_all_results}
\end{figure}

\end{document}
