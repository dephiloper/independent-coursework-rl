% Allgemeine section Aufbau
% - Theorie
% - Einordnung (on-policy/off-policy, value-based/policy-based)
% - Implementationen
%   - Herausstellen wie Exploration und Exploitation umgesetzt ist
%   - falls vorhanden Graphs einbauen, die den Trainingsprozess veranschaulichen

\documentclass[11pt]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[german]{babel}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{listings}
\usepackage{graphicx}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\PassOptionsToPackage{hyphens}{url}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{subcaption}

\newcommand{\source}[1]{\vspace{-5pt} \caption*{\hfill \textbf{Source:} {#1}} }

\newenvironment{conditions}
  {\par\vspace{\abovedisplayskip}\noindent\begin{tabular}{>{$}l<{$} @{${}:{}$} l}}
  {\end{tabular}\par\vspace{\belowdisplayskip}}

\newcommand{\lbparagraph}[1]{\paragraph*{#1}\mbox{}\\}

\renewcommand{\baselinestretch}{1.2}
\geometry{
  left=3cm,
  right=3cm,
  top=3cm,
  bottom=3cm,
  bindingoffset=5mm
}


\usepackage{xcolor}
 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}


\begin{document}
% \sloppy

\begin{titlepage}
	\centering
	{\scshape\LARGE Hochschule für Technik und Wirtschaft Berlin \par}
	\vspace{2cm}
	{\Huge \scshape{Independent Coursework}\par}
	\vspace{2cm}
	{\LARGE \scshape{Reinforcement Learning}\par}
	{\scshape\Large Projekt Dokumentation\par}
	%\includegraphics[width=0.9\linewidth]{title.jpg}\par\vspace{1cm}
	\vspace{4cm}
	{\large\itshape Bruno Schilling,\\Philipp Bönsch\par}
	\vfill
	
% Bottom of the page
	{\large Betreuer: Prof. Dr.-Ing. Barthel \par}
	\vspace{1cm}
	{\large \today\par}
\end{titlepage}

\lstset{basicstyle=\ttfamily\small,breaklines=true}
\newpage
\tableofcontents
\newpage
\section{Introduction}

\subsection{Problem Case}
Machine Learning Methoden werden zunehmend benutzt, um Probleme in der realen Welt zu
lösen und erhalten daher in der Zukunft einen immer höheren Stellenwert. Spiele im
speziellen stellen meist Problemstellungen dar, welche zum Teil auch schon durch Machine
Learning Verfahren absolviert werden konnten. Darunter fallen klassische Brettspiele, wie Go
oder Schach\cite{DM2018} aber auch echtzeit Spiele, wie Dota 2\cite{OA2019},
Quake\cite{DM2019} oder Starcraft\cite{DM2019_2}. Für das Supervised Research Independent Coursework
ergibt sich folgende Problemstellung: Welche Machine Learning Verfahren können für das
absolvieren eines Computerspieles verwendet werden und wie integriert man diese in die
virtuelle Umgebung?

\subsection{Objectives}
Es soll innerhalb eines Supervised Research Independent Courseworks untersucht werden, wie
sich ein Agent in einer virtuellen Spielumgebung mit Hilfe von Machine Learning Methoden,
speziell Reinforcement Learning Verfahren, steuern lässt. Dafür werden initial
Untersuchungen durchgeführt, um den aktuellen Stand der Technik zu erfassen. Genauer
sollen Algorithmen mittels der von OpenAI Gym zur Verfügung gestellten Testumgebung
untersucht werden. Die dabei gesammelten Erfahrungen sollen benutzt werden, um einen
Agenten für ein Computerspiel zu entwickeln. Das Ziel des Research Projektes ist es, neue
Kenntnisse im Bereich Reinforcement Learning zu erhalten und bereits bestehendes Wissen in
den Schwerpunkten Artificial Intelligence und Machine Learning zu festigen. Weiterhin soll
dieses Wissen anschließend in einem Prototyp umgesetzt werden, bei dem ein Agent in einem
Echtzeitspiel gesteuert wird. In der ausgearbeiteten Dokumentation wird dabei sowohl auf die
Recherche und Aufarbeitung der theoretischen Teile von Reinforcement Learning als auch auf
die abschließende Implementierung der prototypischen Umsetzung eines Agenten eingegangen.


\newpage
\section{Fundamentals}
In diesem Abschnitt werden die Grundlagen des Machine Learning Bereichs Reinforcement Learning 
erläutert. Damit soll eine Grundlage für Themenbereiche in nachfolgenden Kapiteln geschaffen
werden, welche sich auf die Terminologie dieses Abschnittes beziehen.

\subsection{Reinforcement Learning}
Reinforcement Learning beschreibt eine Ansammlung von Machine Learning Verfahren, in denen die 
lernende Entität, auch \textit{Agent}, keine bezeichneten Beispiele für das Lösen eines Problemes 
erhält. Die einzige Information, welche mitgeteilt wird, ist Feedback zu gewählten Aktionen in Form
einer Belohnung oder Bestrafung. Diese Art des Feedbacks wird auch als Verstärkung bezeichnet.
Somit muss vom Lernenden selbstständig eine Strategie entwickelt werden, um das gegebene
Problem zu lösen, indem versucht wird den erhaltenen Belohnungswert zu maximieren. Die nachfolgenden
Kapitel beschreiben Bestandteile und Hauptakteure von Systemen im Bereich Reinforcement Learning.

\subsubsection{Agent}
Ein Agent nimmt seine Umgebung (\textit{Environment}) über Sensoren wahr und kann mittels Aktuatoren
Handlungen (\textit{Actions}) vollziehen um diese Umgebung zu beeinflussen oder zu verändern 
\cite[~S.60]{RN2009}. Für die ausgeführten Aktionen erhält der Agent einen Leitwert wie positiv oder
negativ sich diese Aktion auf die Umgebung oder den Agenten ausgewirkt hat. Wie bereits im vorherigen
Absatz erwähnt, kann der Agent als lernende Entität bezeichnet werden, da dieser seine Verhaltensweise
im Bereich  Reinforcement Learning selbstständig immer weiter in Richtung bestmöglicher Lösung einer 
Problemstellung optimiert. In einem Computerspiel kann ein Agent als menschlicher Spieler
repräsentiert werden. Dabei zählen die Augen und Ohren zu seinen Sensoren mit denen das Spiel
wahrgenommen wird und über seine motorischen Fähigkeiten und somit dem bedienen einer Tastatur
können Handlungen vollzogen werden. Ein im Spiel umgesetzter Nicht-Spieler-Charakter wird ebenfalls
als Agent gesehen, wobei dessen Wahrnehmung und Aktuatoren meist direkt im Spiel umgesetzt sind.


\subsubsection{Environment}
Das Environment definiert alles was den Agenten umgibt. Die Kommunikation zwischen einem Environment
und dem Agenten ist limitiert auf die \textit{Actions}, welche der Agent ausführen kann, die 
\textit{Observations}, welche der Agent nach dem Ausführen einer Action vom Environment erhält, 
sprich wahrnimmt, und durch die Bewertung der getätigten Aktionen, dem \textit{Reward} 
\cite[~S.5]{L2018}. Im Falle eines Computerspiels umfasst das Environment alle im Spiel festgelegten
Gesetzmäßigkeiten, aber auch die Netzwerkverbindung oder das Medium auf dem das Spiel ausgeführt
wird kann zum Environment gezählt werden.

\subsubsection{Action}
\label{sec:action}
Eine Action bietet die Möglichkeit für einen Agenten aktiv Einfluss auf das Geschehen im
Environment zu nehmen. Zu Actions zählen das Ausführen von Spielzügen, die durch das
Regelwerk erlaubt sind sowie die Änderung eines Zustandes in umfangreicheren Systemen. Die
Komplexität der Actions kann dabei ebenfalls variieren. Des Weiteren unterscheidet man im Bereich
Reinforcement Learning zwischen zwei Arten von Actions: diskrete und kontinuierliche Actions.
Diskrete Actions definieren eine endliche Menge an gegenseitig ausschließenden Handlungen die ein
Agent vollziehen kann \cite[~S.8]{L2018}. Dazu zählen Actions wie sich nach links und rechts zu
bewegen oder zu springen. Unter kontinuierlichen Actions versteht man hingegen Handlungen, die
über einen zusätzlichen Wert beeinflusst werden, wie zum Beispiel das Steuern eines Fahrzeuges
mit einem Lenkrad. Dabei ist nich ausschließlich die Lenkrichtung ausschlaggebend, sondern auch
der Einschlagswinkel des Lenkrades, welcher die Intensität der Lenkung festlegt.

\subsubsection{Observation}
Unter einer Observation versteht man die Wahrnehmung des Agenten, welche von dem Environment 
bereitgestellt wird. Dabei enthält eine Observation immer Informationen dazu, was gerade in der
Umgebung des Agenten passiert. Jedoch geben Observations für den Agenten nicht den Einblick
in das gesamte Environment, sondern limitieren dessen Wahrnehmung auf notwendige oder auch
ausschließlich gewollte Informationen \cite[~S.8 f.]{L2018}. Eine Observation kann aus einzelnen
Werten bestehen, welche zum Beispiel im Falle von Pong Informationen wie Position und
Geschwindigkeiten zu den Schlägern und dem Spielball zurückliefern. Zusätzlich kann eine
Observationen auch in Form eines Bildes oder einer Aneinanderreihung von Bildern umgesetzt sein.
Das Bild wird dabei von dem Agent verarbeitet und dieser führt dementsprechende Actions zum
visuell wargenommenen Zustand, sprich der Anordnung der Pixel aus. 


\subsubsection{Reward}
Im Gebiet Reinforcement Learning beschreibt der Reward einen Wert, welcher periodisch oder nach
dem Eintreffen einer definierten Gegebenheit dem Agenten vom Environment bereitgestellt wird.
Ziel des Rewards ist es dem Agenten Auskunft darüber zu geben, wie gut sich dieser verhält, sprich
diesen zu bewerten. Wie bereits beschrieben, versucht der Agent den erhaltenen Reward über den
Verlauf des Trainingsprozesses hinweg zu maximieren. Somit bildet der Reward als Feedback ein
Kernelement im Bereich Reinforcement Learning und definiert den \textit{bestärkenden} Anteil der 
Verfahren des machinellen Lernens \cite[~S.6 f.]{L2018}. Der Reward ist immer lokal zu betrachten,
dass heißt er gibt ausschließlich Auskunft über den Erfolg der aktuell ausgeführten Actions und
nicht über das bereits insgesamt gesammelte Ergebnis aller Erfolge und Misserfolge. Des Weiteren
bedeutet ein hoher lokaler Reward nicht, dass der daraus folgende Zustand global betrachet keine
noch größeren negativen Auswirkungen auf den Agenten haben wird.

\subsection{Markov Processes}
Der Makrov Prozess und seine Erweiterungen bilden das theoretische Konstrukt für Reinforcement Learning
Verfahren. Durch die Beleuchtung dieser Prozesse werden weitere Terminologien geklärt, welche ebenfalls
ihren Einsatz im Reinforcement Learning finden.

\subsubsection{Markov Process}
Die einfachste Variante in der Markov Familie ist der Markov Prozess (Markov-Kette). Ein Markov Prozess
beschreibt ein System, welches ausschließlich observiert werden kann, sprich es kann nicht in die Dynamik
des Systems eingegriffen werden. Jeder Markov Prozess enthält eine Menge von \textit{States}, zwischen
denen das System nach den Gesetzlichkeiten der Systemdynamik hin und her wechseln kann. Die Menge aller
möglichen States wird dabei als \textit{State Space} bezeichnet. Wird das System observiert ergibt sich
durch die Wechsel von State zu State eine Folge, welche als \textit{History} bezeichnet wird. 

Damit ein Prozess als Markov Prozess bezeichnet werden kann muss dieser die \textit{Markov Eigenschaft}
aufweisen, welche definiert, dass die zukünftige Dynamik des Systems von jedem State nur von diesem State abhängen darf. Die Markov Eigenschaft legt somit fest, dass ein State differenziert von jedem anderen State 
des Systems betrachtet werden kann und deshalb die History für die Vorhersage der Systemdynamik nicht
notwendig ist.

Ein Prozess der die Markov Eigenschaft aufweißt muss die Fähigkeit bieten, eine Vorhersage über
die Wahrscheinlichkeit des Eintreffens eines Folgezustandes bereitzustellen. Dafür werden 
Wahrscheinlichkeiten für die Übergänge zwischen zwei States in einer \textit{Transition Matrix}
festgehalten. \autoref{fig:markov}: \nameref{fig:markov} veranschaulicht ein Wettermodell in 
der Repräsentationsform eines Markov Prozesses mit den States \textit{Sunny} und \textit{Rainy}
und deren Übergangswahrscheinlichkeiten in zukünftige Folgezustände.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.2]{res/sunny-rainy-weather-model.png}
\caption{Sunny/Rainy Wettermodell}
\source{\cite[~S.13 - Chapter 1 - Figure 4]{L2018}}
\label{fig:markov}
\end{figure}

\subsubsection{Markov Reward Process}
\label{sec:markov_reward_process}
Die erste Erweiterung zum Markov Process ensteht durch die Hinzunahme eines weiteren Wertes zu den 
Übergängen zwischen zwei States, dem \textit{Reward} $R$. Dieser Wert definiert wie hoch die Belohnung
oder Bestrafung bei einem Wechsel der States ausfällt. Durch den Reward kann anschließend der
\textit{value of state} $V(s)$ berechnet werden. Für jeden State $s$ ist der value of state $V(s)$
der durchschnittliche (oder erwartete) Ertrag, welcher durch das Verfolgen des Markov-Reward-Prozess
entsteht und somit durch alleiniges Observieren die Bewertung von Zuständen ermöglicht.
Zusätzlich wird der \textit{discount factor} $\gamma$ eingeführt, welcher den erhaltenen Reward
für Übergänge die weiter in der Zukunft liegen verringert. Der value of state wird dabei wie
nachfolgend formuliert für eine \textit{Episode} berechnet, wobei eine Episode immer aus einer
Folge von States besteht und entweder durch das Eintreffen eines terminierenden States oder das
Erreichen einer Maximalanzahl von States beendet wird:

\begin{equation}
V(s)_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} ... = \sum_{k=0}^\infty \gamma^{k} R_{t+k+1}
\label{eq:value-of-state}
\end{equation} 

\begin{conditions}
 V(s)     	&  value of state \\
 R	     	&  Reward \\   
 \gamma 	&  discount factor für Rewards, die weiter in der Zukunft liegen
\end{conditions}

\subsubsection{Markov Decision Process}
Der Markov Decision Process erweitert den Markov Reward Process um eine endliche Menge an Actions.
Diese Menge entspricht dem \textit{Action Space} des Agenten. Dadurch kommt es nun nicht mehr
ausschließlich zur passiven Observierung, sondern der Agent kann zu jeder Zeit eine Action wählen um
somit die Wahrscheinlichkeiten des Zielzustandes selbstständig zu beeinflussen. Des Weiteren hängt 
der Reward, den der Agent erhält, nun nicht mehr ausschließlich von dem Zustand ab, in dem er endet,
sondern auch von der Action, die zu diesem Zustand führt. 

Die Verhaltensweise, die der Agent ausübt und welche für die Wahl der Actions verantwortlich ist,
wird als \textit{Policy} bezeichnet. Formal wird diese als Wahrscheinlichkeitsverteilung
über Actions für jeden möglichen Zustand definiert \cite[~S.22 f.]{L2018}. 

\subsection{Further Taxonomy} % TODO Überschrift gefällt mir irgendwie nicht so richtig
In diesem Kapitel werden diverse Kategorisierungen für Reinforcement Learning Verfahren beschrieben,
welche nachfolgend bei der Erläuterung der realisierten Verfahren für deren Einordnung genutzt werden.  


\subsubsection{Value learning vs. policy learning}
Bei \textit{value learning} Verfahren wird jedem State oder jeder Kombination aus State und Action ein
Value/Score zugewiesen, welcher diesen bewertet. Durch den Value kann der Agent anschließend eine
Wahl zur besten Action treffen \cite[~S.NA]{L2018}. \textit{Policy learning} bezeichnet hingegen das
direkte Lernen einer Verhaltensweise, wobei anhand einer Observation eine Wahrscheinlichkeitsverteilung
der zu wählenden Actions erzeugt wird. Der Agent wählt anschließend mithilfe der jeweiligen
Wahrscheinlichkeiten die möglichen Actions \cite[~S.NA]{L2018}.


\subsubsection{On-policy vs. off-policy}
Die Kategorisierung \textit{off-policy} beschreibt die Fähigkeit von Reinforcement Learning Verfahren auf
Grundlage alter, bereits vor längerer Zeit aufgenommener Daten zu trainieren \cite[~S.NA]{L2018}. 
Im Kontrast dazu definiert \textit{on-policy} die Notwendigkeit von immer neu aufgezeichneten 
Trainingsdaten, welche nach einem Trainingsdurchlauf wieder verworfen und somit neu gesammelt
werden müssen.


\subsubsection{Model-free vs. model-based}
Der Term \textit{model-free} bezieht sich auf Verfahren, welche kein Modell der Umgebung oder des Rewards
während des Trainingsprozesses aufbauen. Es werden ausschließlich durch aktuelle Observations
zugehörige Actions berechnet. Model-based Methoden versuchen im Gegenzug vorherzusagen, was 
die nächste Observation oder wie hoch der nächste Reward sein wird \cite[~S.NA]{L2018}. Aufgrund dieser
Vorhersage versucht der Agent die bestmögliche Action zu wählen, wobei in den meisten
Fällen die Vorhersage mehrfach und dabei auch mit der Betrachtung zukünftiger Schritte durchgeführt
wird \cite[~S.NA]{L2018}.

\subsection{Exploration und Exploitation}
In diesem Abschnitt wird der Konflikt zwischen Exploration und Exploitation des
Reinforcement Learnings erläutert und anhand des k-armed-Bandit Problems veranschaulicht.
Die dabei realisierten Implementierungen werden beschrieben und deren Ergebnisse
vorgestellt.

\subsubsection{Das k-armed-Bandit Problem}
% Erklärung des k-armed-Bandit Problems
Das k-armed-Bandit Problem besteht darin aus $k$ verschiedenen Actions zu wählen.
Nachdem eine Action gewählt wurde, erhält der Agent einen Reward, der aus einer
Wahrscheinlichkeitsverteilung gezogen wird. Der Erwartungswert der
Wahrscheinlichkeitsverteilung hängt von der gewählten Action ab. Das Ziel besteht darin
mit der Ausführung von $n$ Actions die höchstmögliche Summe von Rewards zu erhalten.

Der Erwartungswert der hinter der Verteilung einer Action steht als $Value$ der Action
bezeichnet. Würden die Values der einzelnen Actions den Agenten zur Verfügung stehen, wäre die 
Wahl der Action einfach, indem die Action mit dem höchsten Erwartungswert gewählt werden 
könnte. Allgemein ist in Reinforcement Learning Problemen der Value einer Action nicht bekannt.
Stattdessen gibt es ausschließlich Abschätzungen bezüglich des Values einer Action, die aus den
vorhergehenden Erfahrungen hervorgehen. Um den Value einer Action besser abschätzen zu können,
muss diese Action ausprobiert werden. Umso häufiger die Action ausgeführt wurde, umso besser
ist auch die Abschätzung des Values dieser Action.

Der Vorgang des Ausprobierens wird $Exploration$ genannt. Im Gegensatz dazu steht die
$Exploitation$ bei der die gesammelten Erfahrungen genutzt werden, um die Action
auszuwählen, die den höchsten Reward verspricht.

Das k-armed-Bandit Problem gilt als Vereinfachung zu allgemeinen Reinforcement Learning
Problemen, da nicht zwischen unterschiedlichen Ausgangszuständen unterschieden wird. Mit
anderen Worten wird keine Observation durchgeführt. Weiterhin wird der Erwartungswert des
Rewards einer Action nicht über die Dauer des Spiels verändert. Dies bedeutet, dass die
Values der unterschiedlichen Actions am Anfang und Ende des Spiels identisch sind, wodurch
auch Erfahrungen, die zu Beginn des Spielens gemacht wurden, am Ende benutzt werden
können.

\subsubsection{Exploration-Strategien}
Im folgenden werden verschiedene Exploration-Strategien beleuchtet. Dazu werden diese erst
theoretisch erklärt und anschließend praktisch an einem k-armed-Bandit Problem verglichen.

\paragraph{$\epsilon$ greedy / decaying $\epsilon$ greedy}
Eine Strategie, die zu jedem Zeitpunkt die Action wählt, die nach derzeitigem Wissen den
höchsten Value hat wird $greedy$ genannt. $\epsilon$-greedy-Strategien sind greedy
Strategien, die mit einer gewissen Wahrscheinlichkeit $\epsilon$ eine zufällige Action
wählen. Sie wählen also mit einer Wahrscheinlichkeit von $1 - \epsilon$ die Action, von
der sie sich den höchsten Reward versprechen und andernfalls eine zufällige Action.\\
In der Praxis werden häufig decaying-$\epsilon$-greedy-Strategien verwendet, bei denen der
$\epsilon$-Wert über den Trainingsprozess hinweg verringert wird. Dadurch werden zu Beginn
des Trainings viele neue Actions ausprobiert, während gegen Ende das Anwenden des
Gelernten stärker in den Vordergrund rückt.

\paragraph{Optimistic Initial Values}
Ein Agent, der ein k-armed-Bandit Problem lösen soll, besitzt eine interne Abschätzung der
Values, um Actions wählen zu können, die einen besseren Reward versprechen. Die
Bewertung der Actions wird beeinflusst durch die erfahrenen Rewards nach dem Wählen einer
Action.\\
Neben $\epsilon$-greedy Verfahren ist es eine weitere Möglichkeit Exploration umzusetzen,
die initiale Bewertung der Values sehr optimistisch vorzunehmen. Dadurch wird der Agent
bei der Wahl einer Action vom erhaltenen Reward \grqq enttäuscht\grqq, sodass sich die
Einschätzung dieser Action verschlechtert. Wird nun immer die Action gewählt, die am
besten eingeschätzt ist, so werden über die Zeit alle Actions versucht, bis sie sich dem
tatsächlichen Erwartungswert nähern.

\paragraph{Upper Confidence Bound Action Selection}
Während $\epsilon$-greedy-Methoden zufällig zwischen allen nicht greedy Actions
auswählen, wählt die Upper Confidence Bound Ac\-tion Selection Methode Actions entsprechend
dreier Parameter aus.\\
Der erste Parameter ist $t$ die Anzahl der Actions, die bis jetzt gespielt wurden. Der
zweite ist der bis jetzt ermittelte $Value$ der Action $a$ bezeichnet mit $Q(a)$ und der
dritte Parameter ist die Anzahl, wie oft Action $a$ schon ausprobiert wurde. Die nächste
zu spielende Action wird dann nach folgender Gleichung gewählt:

\[ A = \argmax_a\left( Q(a) + c*\sqrt\frac{\ln(t)}{N(a)} \right) \]

\noindent
$A$ ist die gewählte Action. $N(a)$ ist die Anzahl, wie oft die Action $a$ schon gewählt
wurde. Existiert eine Action, die noch nie gespielt wurde, also $N(a)=0$, so wird eine der
Actions gespielt, die noch nicht gespielt wurden. Durch den Term
$c*\sqrt\frac{\ln(t)}{N(a)}$ wird sichergestellt, dass selten benutzte Actions eine höhere
Bewertung erhalten und damit wahrscheinlicher ausprobiert werden. Über den Hyperparameter
$c$ lässt sich bestimmen, wie stark selten benutzte Actions ausprobiert werden sollen.\\
Ein Vorteil gegenüber $\epsilon$-greedy Strategien ist, dass durch die Einbeziehung von
$Q(a)$ Actions häufiger ausprobiert werden, die besser wirken, während Actions, die sehr
schlecht wirken weniger oft ausprobiert werden.

\subsubsection{Implementation}
% Repo
% Es wurde implementiert:
%   - NArmedBandit
%   - Solver
%   - main
Das k-armed-Bandit Problem sowie verschiedene Varianten der Action-Selection wurden
implementiert und sind unter \url{https://github.com/Bluemi/rl_testbed} abrufbar.\\
In der Klasse \lstinline!NArmedBandit! wird das k-armed-Bandit Problem umgesetzt. Es
werden zu Beginn die Values der einzelnen Actions aus einer Standardnormalverteilung
gezogen und gespeichert. Wird nun eine Action auf dem erstellten Bandit gespielt, so wird
eine Zufallszahl aus einer Normalverteilung mit dem Erwartungswert der jeweiligen Action
gezogen und als Reward zurück gegeben.\\
Um auf einem Bandit zu spielen wurde ein \lstinline!Solver! umgesetzt, der eine interne
Abschät\-zung der Action Values ermittelt, indem die erfahrenen Rewards dieser Action
gemittelt werden. Der Solver wählt Actions wobei unterschiedliche Exploration Strategien
ausprobiert werden können. Die Unterklasse \lstinline!EpsilonGreedySolver! implementiert
beispielsweise die $\epsilon$-greedy Strategie.\\
In \lstinline!src/main.py! werden unterschiedliche Solver anhand des gleichen Bandits
getestet und die Ergebnisse geplottet. Jeder Solver trainiert 5000 Actions auf einem
10-armed-Bandit. Die dabei resultierenden Rewards sind in Abbildung
\ref{fig:karmed_bandit} graphisch dargestellt. Die zu sehenden Rewards wurden über 2000
Experimente gemittelt, um ein anschaulicheres Ergebnis zu erhalten.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.3]{res/k_armed_bandit_epsilon.png}
\includegraphics[scale=0.3]{res/k_armed_bandit_optimistic_upper.png}
\caption{Ergebnisse 10-armed-Bandit}
\label{fig:karmed_bandit}
\end{figure}

\noindent
Auf der linken Seite werden 3 unterschiedliche $\epsilon$ greedy Verfahren verglichen.
Vergleicht man die Verfahren ohne decay (blau und orange), so kann man erkennen, dass
$\epsilon = 0.1$ zu Beginn besser funktioniert, da mehr Zeit für Exploration verwendet
wird und so schneller bessere Actions gefunden werden. Dafür funktioniert $\epsilon =
0.01$ nach circa 1500 Action Selections besser, da mehr Zeit verwendet wird, um die schon
gefundenen guten Actions zu exploiten.\\
Um die Vorteile beider Möglichkeiten zu nutzen, verwendet das dritte Experiment einen
decay, um $\epsilon$ auf $0$ zu senken. Dadurch werden zu Beginn viele zufällige Actions
ausgewählt, um die Values der einzelnen Actions besser einzuschätzen. Wenn durch den decay
$\epsilon$ gegen Ende des Experiments $\epsilon$ sinkt, werden die gefundenen Actions
benutzt und ein vergleichsweise hoher Reward entsteht.\\
In der rechten Abbildung sind das Verfahren Optimistic Initial Values und Upper Confidence
Bound Action Selection zu sehen. Zusätzlich ist der $\epsilon$-greedy Solver mit
$\epsilon=0.01$ aus der ersten Abbildung zum Vergleich eingezeichnet. Die obere schwarz
gestrichelte Linie deutet das Value der optimalen Action an.\\
Tabelle \ref{tab:explorationstrategies} zeigt die erreichten durchschnittlichen Rewards
der unterschiedlichen Verfahren.

\begin{table}[h]
  \begin{center}
    \begin{tabular}{l | l}
      \textbf{Verfahren} & \textbf{Mean Reward} \\
      \hline
      Optimal & $1.579$\\
      Upper Confidence Bound & $1.556$ \\
      $\epsilon$-greedy with decay ($\epsilon=0.01$) & $1.523$ \\
      Optimistic Initial Values & $1.520$ \\
      $\epsilon$-greedy ($\epsilon=0.01$) & $1.439$ \\
      $\epsilon$-greedy ($\epsilon=0.1$) & $1.437$ \\
    \end{tabular}

    \caption[ExplorationStrategies]{Vergleich unterschiedlicher Exploration Strategien}
    \label{tab:explorationstrategies}
  \end{center}
\end{table}

\noindent
Im gezeigten Experiment erhielten die einfachen Versionen der $\epsilon$-greedy Solver die
kleinsten Rewards. Deutlich besser funktionierte die erweiterte Version mit decay, die
ungefähr genau so viel Reward erreichte, wie das Optimistic Initial Values Verfahren. Am
besten schneidet die Upper Bound Confidence Action Selection ab.

\newpage
\subsection{Open AI Gym}
Die Python Bibliothek \textit{gym}, welche von der Firma Open AI umgesetzt wurde umfasst ein
Toolkit, welches zur Entwicklung und zum Vergleich von Reinforcement Learning Algorithmen
eingesetzt werden kann. Das Toolkit definiert keine Strukturen für die Umsetzung der Agenten,
wodurch die Wahl der Machine Learning Bibliothek dem Entwickler frei zur Verfügung steht. In
der gym Bibliothek wird hingegen eine Sammlung an Testumgebungen (Environments) zur Nutzung
bereit gestellt, welche alle eine festgelegte Problemstellung definieren \cite{OAI2016}. 
Diese Environments bieten eine einheitliche Schnittstelle, was das implementieren von
generellen Algorithmen für nahezu jede Art von Environment ermöglicht. Da Reinforcement
Learning Verfahren in komplexen Problemstellungen immer bessere Resultate erzielten, jedoch
keine standartisierte Umsetzung für die Entwicklung und anschließende Nutzung von Environments
und somit auch keine Möglichkeit für ein Benchmarking verschiedener Verfahren bestand,
beschloss Open AI mit der gym Bibliothek genau diesen Problemen entgegenzuwirken
\cite{OAI2016}.

Nachfolgend soll der Aufbau und die Nutzung eines Environments durch die Bibliothek gym
erläutert werden und es wird auf eine Referenzimplementierung eines Environments für das
Spiel Tic-Tac-Toe eingegangen.

\subsubsection{Concept}
Ein Environment ist eine von der Klasse gym.Env erbende Unterklasse, welche durch die
Hauptprogrammierschnittstelle des Toolkits die folgenden Methoden definiert.

\begin{itemize}
\itemsep-6pt
\item \textit{step}
\item \textit{reset}
\item \textit{render}
\item \textit{close}
\item \textit{seed}
\end{itemize}  

Mittels der \textbf{step(action)} Methode kann eine Action für einen timestamp im
Environment ausgeführt werden. Diese Action kann sowohl diskrete Werte als auch 
kontinuierliche Werte repräsentieren. Nachdem die Action im Environment ausgeführt wurde,
wird die für den Agent bereitgestellte aktuelle \textit{Observation} des Environments
aufgezeichnet. Des Weiteren wird ein, durch die Action initiierter und durch den Wechsel
in den neuen Zustand erhaltener \textit{Reward} für den Agenten ermittelt sowie ein 
\textit{done} Flag, welches Auskunft darüber gibt, ob die aktuelle Episode beendet wurde.
Weitere Informationen werden im \textit{info} Dictionary gesammelt und dienen für optionale
Analyseinformationen. Das Tuple \textit{observation, reward, done, info} wird anschließend
an den Aufrufenden zurückgegeben. \autoref{lst:step-method} veranschaulicht eine
Beispielimplementierung der \textit{step} Methode gegen die von Open AI bereitgestellte API.

\begin{lstlisting}[language=Python, caption=step method, label=lst:step-method]
def step(self, action):
	# executes the action in the environment and validates it
	valid = self._take_action(action)
	
	if not valid:
		print("invalid action: player ", self._current_player)
	
	# calculates the obtained reward
	reward = self._get_reward()
	
	# gathers current observation of the environment
	observation = (self._player_0, self._player_1)
	
	# information if the episode is finished
	is_done = self._check_if_done(reward)
    
	return observation, reward, is_done or not valid, {}
\end{lstlisting}

Die Funktionen \textit{\_take\_action(action)}, \textit{\_get\_reward()} und 
\textit{\_check\_if\_done(reward)} setzen die Kommunikation mit dem Environment um. In
diesem Bespiel besteht die Observation ausschließlich aus den Zuständen der beiden Spieler.
Nach der Änderung der Systemdynamik und dem Erfassen der Observation wird das bereits
beschriebene Tupel an Umgebungsinformationen zurückgeliefert.
\newline

\noindent
Die Methode \textbf{reset()} dient zum vollständigen Zurücksetzen des Environments und aller
darin gespeicherten Zustände. Anschließend gibt die Methode den Ausgangszustand als Observation
an den Aufrufenden zurück. Reset kommt zum Einsatz, sobald eine Episode abgeschlossen wurde
(done Flag) damit erneut Actions darin ausgeführt werden können.
\newline

\noindent
Mit \textbf{render(mode='human')} kann der zwischenzeitliche Zustand des Environments
visualisiert werden. Dabei legt die gym Bibliothek keine explizit zu nutzenden Renderverfahren
fest, per Konvention sind aber folgende Verfahren definiert: 

\begin{itemize}
\itemsep0pt
\item \textit{human} - für Menschen veranschaulicht, 
\item \textit{rgb\_array} - ein numpy.ndarray mit RGB Pixelwerten, um dieses in einem Bild oder einem 
Video zu visualisieren und 
\item \textit{ansi} - als terminal-style Textrepräsentation.
\end{itemize}

\noindent
Abschließend kann durch \textbf{close()} das Environment und somit gegebenenfalls die 
dahinterliegende Anwendung geschlossen werden und durch den Aufruf von 
\textbf{seed(seed)} kann ein Seed für den \textit{random number generator}
des Environments festgelegt werden.
\newline
%TODO explanation rng

\noindent
Neben den beschriebenen Methoden exisiteren die Properties \textit{action\_space}, welcher die
Art und Anzahl der zu wählenden Actions vorgibt (z.B. \textit{Discrete(9)} für 9 verschiedene
diskrete Actions), \textit{observation\_space}, welcher gleiches für die Observation des
Environments angibt und \textit{reward\_range}, welcher einen Wertebereich für den zu erhaltenen
lokalen Reward definiert.
\newline

\noindent
Für das Spiel Tic-Tac-Toe wurde ein Einspieler Environment implementiert, welches im Unterverzeichnis
\textit{preparation/05\_v\_learning/} unter \url{github.com/dephiloper/independent-coursework-rl/}
abgerufen werden kann.

\subsubsection{Usage}
Das nachfolgende Listing \autoref{lst:env-usage} veranschaulicht die Nutzung eines von Open AI
bereitgestellten Environments mit der Bezeichnung CartPole-v0. Initial wird das Environment
mit \textit{gym.make('CartPole-v0')} erzeugt und daraufhin mit \textit{reset()} vollständig
zurückgesetzt, sodass alle internen Zustände wieder in die Ausgangssituation gebracht werden. 
Anschließend werden in 1000 Frames das Environment mit \textit{render()} visualisiert und es
wird eine zufällige Action aus dem action\_space des Environments gesampled und in der
\textit{step(action)} Methode ausgeführt. Durch das Ausüben der Action wird eine neue
Observation mit zugehörigem Reward vom Environment generiert und anschließend zurückgegeben.
Nach 1000 Iterationen wird das Environment geschlossen. 
\begin{lstlisting}[language=Python, caption=environment usage, label=lst:env-usage]
import gym
env = gym.make('CartPole-v0')
env.reset()
for _ in range(1000):
    env.render()
    # take a random action, gather observation and reward
    obs, reward, done, _ = env.step(env.action_space.sample())
env.close()
\end{lstlisting}

\noindent
Durch den definierten Aufbau der bereitgestellten API bietet ein Environment in den meisten Fällen
die Möglichkeit Agenten mit verschiedenen Verfahren zur Ermittelung der zu tätigenden Action zu
implementieren, sodass diese nach dem Aufsetzen einer Grundstruktur auch gut ausgetauscht werden
können.


\newpage
\section{Cross-Entropy-Method}
Das erste zu beleuchtende Reinforcement Learning Verfahren ist die Cross-Entropy-Method. Die
Cross-Entropy-Method ist eine Monte Carlo Methode für Stichprobenentnahme nach Wichtigkeit
(importance sampling) und Optimierung (optimization) \cite[~S.29 ff.]{R2004}. Im folgenden
Kapitel wird das Verfahren erläutert und in die bereits beschriebene Taxonomie eingeordnet.
Des Weiteren erfolgt die Anwendung des Verfahrens innerhalb von zwei Testumgebungen. 

\subsection{Categorisation}
Bei dem Verfahren handelt es sich um eine on-policy Methode, da im Trainingsprozess ausschließlich
mittels neu aufgezeichneter Trainingsdaten gelernt werden kann. Prinzipiell hat das Verfahren das
Hauptziel, den maximalen Reward innerhalb einer Episode anzusammeln. Um dieses Ziel zu erreichen
orientiert es sich an klassischen Machine Learning Verfahren und führt eine nichtlineare trainierbare
Funktion ein, die als Input die Observation des Agenten erhält und diesen auf einen Output abbildet.
Durch diese Charakteristik fällt die Cross-Entropy-Method in die Familie der policy learning Verfahren
(policy-based), da mittels neuronalen Netz eine Policy erzeugt wird, welche für jede Observation die
zu wählende Action in Form einer Wahrscheinlichkeitsverteilung über alle Actions ausgibt, sprich es
wird eine Observation über direktem Wege auf eine zu wählende Action abgebildet. Die
Methode ist nicht value-based, da der zu erwartende Reward nicht approximiert wird.

\subsection{Procedure} % TODO Überschrift gefällt mir irgendwie nicht so richtig
Die zu lernende Policy ist wie bereits erwähnt als Wahrscheinlichkeitsverteilung der
über die Actions repräsentiert und dadurch sehr ähnlich zu einem Klassifizierungsproblem,
wobei die Anzahl der Klassen hier der Anzahl von auszuführenden Actions entspricht. Durch
diese Verfahrensweise ist die Umsetzung des Agenten als eher einfach zu betrachten, da
diese ausschließlich die Observation des Environments an das Netz weitergeben muss, von
dem Netz anschließend die Wahrscheinlichkeitsverteilung erhält und anhand dieser dann eine
Zufallsstichprobe (random sampling) unter Berücksichtigung der Wahrscheinlichkeiten aller
Actions vollzieht \cite[~S.78]{L2018}. Nachdem der Agent durch das random sampling eine
Action gewählt hat führt er diese erneut im Environment aus und erhält die nächste
Observation sowie den Reward für die zuvor ausgeführte Action.

Diese Schritte werden anschließend solange wiederholt, bis eine Ansammlung an Episoden (siehe 
\autoref{sec:markov_reward_process}: \nameref{sec:markov_reward_process}), dass heißt eine Folge
von Observations, Actions und Rewards, gesammelt wurde. Innerhalb jeder Episode wird nun der
insgesamt erwirtschaftete Reward, der \textit{total reward}, berechnet. Dadurch kann bewertet werden
wie gut eine Episode im Vergleich zu anderen Episoden ausgefallen ist. Aufgrund der Zufälligkeit
bei der Selektion von Actions kommt es dazu, dass einige Episoden besser ausfallen als andere.
Die Cross-Entropy-Method macht sich dieses Verhalten zu nutze, verwirft schlechtere Episoden und
trainiert ausschließlichen auf besseren Episoden. Das Kernprinzip des Verfahrens wird wie nachfolgend
aufgelistet in drei Schritte untergliedert \cite[~S.80 f.]{L2018}:

Initial wird eine definierte Anzahl $N$ an Episoden mit dem aktuellen Model im Enviroment gespielt
oder auch wahrgenommen. Anschließend erfolgt die Berechnung des total rewards für jede gespielte
Episode. Zusätzlich wird in diesem Schritt eine Obergrenze für den Reward festgelegt 
(\textit{reward boundary}). Daraus folgt, dass die besten 30 Prozent der Episoden behalten werden,
wohingegen die restlichten schlechteren Episoden verworfen werden. Anschließend wird im letzen Schritt
das Model auf den übrigen \textit{elite} Episoden trainiert, wobei eine Observation als Input in das
Netz gegeben wird und die zuvor gewählten Actions als gewünschter Output vom Netzes gefordert werden.
Während des Trainingsprozesses werden diese Schritte solange wiederholt bis ein gewünschtes
Ergebnis erzielt wurde. In \autoref{fig:cross_entropy_procedure} werden die beschriebenen Schritte
zusätzlich visuell dargestellt:

\begin{figure}[htp]
\centering
\includegraphics[scale=0.45]{res/cross-entropy-procedure.png}
\caption{Cross Entropy Verfahren}
\label{fig:cross_entropy_procedure}
\end{figure}

\subsection{Cart-Pole}
Das Cart-Pole Environment beschreibt ein eher einfach einzustufendes Environment, welches durch seine
simple Nutzung gerne als Einstiegspunkt für die Entwicklung von Reinforcement Learning Algorithmen 
dient. Das Environment besteht aus einem Stab (pole), welcher über ein Gelenk an einem Wagen (cart) 
befestigt ist \cite{OAI2016_2}. Der Agent kann den Wagen durch das Aufbringen einer horizontalen Kraft 
von -1 oder +1 steuern, wobei sich der Wagen dementsprechend nach links oder rechts bewegt. Nach 
dem Zurücksetzen des Enviroments startet der Stab in einer vertikalen Ausrichtung, nahezu orthogonal
zentriert auf dem Wagen. Das zu erreichende Ziel für den Agenten ist es den Stab vom Umkippen zu hindern,
wobei der Stab als umgekippt zählt sobald dessen Ausrichtung mehr als 12 Grad von einem genauen
senkrechten Winkel abweicht. Des Weiteren zählt eine Episode als beendet, wenn sich der Wagen mehr
als 2.4 Units vom Ausgangspunkt wegbewegt. Das Environment liefert dem Agenten einen Reward von +1
für jeden Frame in dem der Stab und der Wagen die beschriebenen Anforderungen erfüllen. Des Weiteren
erhält der Agent als Observation die aktuelle Position und Geschwindigkeit des Wagens, sowie den Winkel
der Ausrichtung des Stabs und die Geschwindigkeit die auf die Spitze des Stabs wirkt.

In dem Toolkit von Open AI existieren diverse Versionen von Cart-Pole, wobei sich die eben
erläuterte Beschreibung auf das Environment CartPole-v0 bezieht, des von Barto, Sutton und
Anderson beschriebenen Cart-Pole-Problems \cite[~S.838 f.]{BSA1983}. Erreicht der Agent einen
durchschnittlichen Reward von mindestens 195,0 über 100 aufeinanderfolgende Versuche hinweg gilt das
Problem als gelöst. \autoref{fig:cart-pole} veranschaulicht die beschriebene Problemstellung aus dem 
Paper\cite{BSA1983} von Barto, Sutton und Anderson:

\begin{figure}[htp]
\centering
\includegraphics[scale=0.15]{res/cart-pole.png}
\caption{Illustration des Cart-Pole Problems \cite[~S.838 f.]{BSA1983}}
\label{fig:cart-pole}
\end{figure}

\paragraph*{Evaluation}
\noindent
\newline
Das Cross-Entropy-Verfahren wurde im Cart-Pole Environment mittels neuronalen Netz mit
einem Hidden Layer mit 128 Nodes und einer Learning Rate von 0.01 trainiert. Als
Optimizer wurde Adam verwendet und als Aktivierungsfunktion zwischen Input Layer und
Hidden Layer die ReLu Funktion. Zur Berechnung eines Loss zwischen Vorhersage des Netzes
und den während des Spielens aufgenommenen Actions wird der Cross-Entropy-Loss verwendet.

Die nachfolgenden Graphen in \autoref{fig:cross-entropy-graph} veranschaulichen den
\textit{loss}, den stetig wachsenden \textit{reward\_bound} (Reward Obergrenze) und den
durchschnittlich erhaltenen Reward beim Spielen einer Episode (\textit{reward\_mean}).
Das Model des orange dargestellten Graphs verwendet bei der Auswahl der elite Episoden
einen Perzentil von 70, das blau dargestellte Model einen Perzentil von 80 und das
rot dargestellte Model einen Perzentil von 90. Ein Perzentil definiert eine Menge von
Werten einer Verteilung die unter oder gleich einem festgelegten Wert sind (z.B. Median 
= 50\%-Perzentil). 

\begin{figure}[htp]
\centering
\includegraphics[scale=0.385]{res/cross-entropy_loss.png}
\includegraphics[scale=0.385]{res/cross-entropy_reward-bound.png}
\includegraphics[scale=0.385]{res/cross-entropy_reward-mean.png}
\caption{Ergebnisse Cross-Entropy-Method Cart-Pole-v0}
\label{fig:cross-entropy-graph}
\end{figure}

Die Implementierung der Cross-Entropy-Method zum Cart-Pole Problem ist unter
\url{github.com/dephiloper/independent-coursework-rl} im Unterverzeichnis 
\textit{preparation/04\_cross\_entropy} in der Datei
\href{https://github.com/dephiloper/independent-coursework-rl/blob/master/preparation/04_cross_entropy/cart_pole_torch.py}{\nolinkurl{forzen\_lake\_torch.py}} umgesetzt.

\subsubsection{Frozen Lake}
\label{sec:frozen-lake}
Bei dem zweiten Environment Frozen Lake handelt es sich um ein Environment mit diskretem action-
und diskretem observation-space, das heißt, dass sowohl die zu tätigende Action diskret ist,
aber auch die Observation als diskreter Zahlenwert von 0-15 wahrgenommen wird. Der Zahlenwert
der Observation repräsentiert dabei ausschließlich die Position des Agenten. Das Ziel ist es
einen Avatar vom Startpunkt in der linken oberen Ecke eines Spielfeldes auf die untere rechte
Seite zum Ziel zu steuern. Auf dem Weg zum Ziel befinden sich Löcher, die das Spiel beenden, wenn
sich der Avatar auf diese bewegt. Das Spielfeld ist in Abbildung \ref{fig:frozen_lake} zu sehen.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.25]{res/frozen_lake.png}
\caption{Frozen Lake Spielfeld}
\source{\cite[~S.90 - Chapter 4 - Figure 5]{L2018}}
\label{fig:frozen_lake}
\end{figure}


\noindent
Der Agent kann die Actions \grqq left\grqq, \grqq up\grqq, \grqq right\grqq und \grqq
down\grqq ausführen. Erschwert wird die Steuerung aber dadurch, dass die Oberfläche des
Sees rutschig ist, weshalb die Folgeposition des Avatars nicht deterministisch ist.
Wählt man bespielsweise die Action \grqq down\grqq, so bewegt sich der Avatar mit einer
Wahrscheinlichkeit von jeweils $1/3$ nach unten, rechts oder links. Allgemein gesprochen
ist die Richtung, in die sich der Avatar nach der Wahl einer Action bewegt, eine zufällige
Richtung, wobei jedoch nie die entgegengesetzte Action der Wahl vollzogen wird.

\paragraph*{Evaluation}
\noindent
\newline
Anders als das Cart-Pole Environment erhält der Agent bei dem Frozen Lake Environment 
ausschließlich einen +1 Reward, sobald der gesteuerte Avatar das Ziel erreicht. Daraus folgt,
dass anhand des \textit{total reward} einer Episode nicht darauf geschlossen werden kann,
wie gut eine Episode gespielt wurde (Avatar kann lange umherwandern bevor er auf das Ziel kommt).
Somit lässt sich das auch keine sinnvolle Wahl von elite Episoden treffen, da ausschließlich
durchgespielte Episoden mit einem total reward von +1 oder 0 bestehen und die Episoden mit 0
deutlich häufiger auftreten.

Aufgrund dieser Probleme werden folgende Einschränkungen der Cross-Entropy-Method 
aufgeführt \cite[~S.92 f.]{L2018}:
\begin{itemize}
\itemsep0pt
\item endliche und kurze Episoden
\item detailierte Unterscheidung des total reward zwischen guten und schlechten Episoden
\item Reward für die Erfüllung von Zwischenschritten
\end{itemize}

Diese Einschränkungen können jedoch addressiert werden, indem mehr Episoden gespielt werden, 
um somit die Wahrscheinlichkeit zu erhöhen erfolgreiche Episoden zu erhalten. Des Weiteren
muss der discount factor $\gamma$ auf die Berechnung des total reward angewandt werden, damit
kürzere Episoden einen höheren total reward erzielen, als lange Episoden. Abschließend ist es
sinnvoll elite Episoden für mehrere Iterationen zu behalten damit auf diesen trainiert werden
kann \cite[~S.93]{L2018}.  

Die nachfolgenden Graphen veranschaulichen die Anwendung der Cross-Entropy-Method ohne (grün)
und mit Anpassungen (grau) zur Erzielung besserer Resultate. Durch die Erweiterung des
Verfahrens kann eine Verbesserung erzielt werden, sodass der Trainingsprozess bei einer
etwa 50\% erfolgreichen Lösung des Environments stagniert.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.485]{res/frozen-lake_reward-mean_bad.png}
\includegraphics[scale=0.485]{res/frozen-lake_reward-mean.png}
\caption{Ergebnisse Cross-Entropy-Method FrozenLake-v0}
\label{fig:frozen-lake_cross-entropy}
\end{figure}

Die Implementierung der Cross-Entropy-Method zum Frozen Lake Environment ist unter
\url{github.com/dephiloper/independent-coursework-rl} im Unterverzeichnis 
\textit{preparation/04\_cross\_entropy} in der Datei 
\href{https://github.com/dephiloper/independent-coursework-rl/blob/master/preparation/04_cross_entropy/frozen_lake_torch.py}{\nolinkurl{forzen\_lake\_torch.py}} umgesetzt.


\newpage
% TODO Bruno
\section{Q-Learning}
In diesem Abschnitt wird das Verfahren Q-Learning erläutert. Anschließend wird die Methode
an zwei praktischen Anwendungen getestet. Einerseits in der Testumgebung frozen-lake 
(siehe \autoref{sec:frozen-lake}) und andererseits in einer selbstständig entwickelten
Tic-Tac-Toe Anwendung, in der das Verfahren anhand eines zufällig spielenden Gegner
validiert wird.

\subsection{Categorisation}
Q-Learning Verfahren gehören zur Familie der value learning Verfahren, die
unterschiedlichen Actions unterschiedliche Values zuordnen, um daraufhin diejenigen Actions
wählen zu können, die den höchsten Value aufweisen. Der Value einer Action ist dabei der
zu erwartende Reward, welcher vom Environment zurückgegeben wird, wenn man die gewählte
Action im wahrgenommenen State ausführt.\\
Weiterhin gehören Q-Learning Verfahren zu den off-policy Verfahren, da auch alte
Erfahrungen zum Training benutzt werden können. Die Q-Learning Methoden, die nachfolgend
beschrieben werden sind model-free, was bedeutet, dass nicht versucht wird die nächsten
States oder die folgenden Rewards vorherzusagen.\\
Wie oben bereits beschrieben, basieren Q-Learning Verfahren auf einer Abschätzung des
Values eines States oder einer Action. Die Abschätzung des Values einer Action $a$ in
einem State $s$ wird als $Q(s, a)$ bezeichnet. Diese Value kann auf unterschiedliche
Weise ermittelt werden.\\
Eine erste Variante von Q-Learning benutzt Tabellen, in denen die Abschätzungen
für unterschiedliche States oder Actions gespeichert werden. Dieses Verfahren wird daher
auch als \grqq Tabular Q-Learing\grqq (TODO cite) bezeichnet.


% Beschreibung des Verfahrens Tabular Q-Learning u. Unterscheidung zu Deep-Q-Learning
\subsection{Tabular Q-Learning}
In diesem Abschnitt wird auf das Tabular Q-Learning eingegangen. Um die
Ver\-ständ\-lich\-keit zu verbessern wird das Verfahren anhand des Frozen Lake
Environments veranschaulicht.

\subsubsection{Das Value-Iteration Verfahren}
Da sich das Spielfeld nicht ändert, beschränken sich die Zustände von Frozen Lake auf die
Positionen des Spielers. Aus diesem Grund besteht die Möglichkeit zur Lösung der Problemstellung
mittels des Tabular Q-Learning Verfahrens, da es möglich ist alle erreichbaren Zustände in einer
Tabelle zu speichern. Diese Tabelle enthält für jeden State eine Abschätzung der unterschiedlichen
Actions. Tabelle \ref{tab:q_table} zeigt eine beispielhafte Q-Table mit zwei Actions und zwei States.

\begin{table}[h]
  \begin{center}
    \begin{tabular}{c | c | c}
       & $a_0$ & $a_1$ \\
      \hline
      $s_0$ & $Q(s_0, a_0)$ &$ Q(s_0, a_1)$ \\
      \hline
      $s_1$ & $Q(s_1, a_0)$ & $Q(s_1, a_1)$ \\
    \end{tabular}

    \caption[Q-Table]{Q-Table}
    \label{tab:q_table}
  \end{center}
\end{table}

% Beschreibung des Verfahrens an Frozen Lake
%  - Es werden random Actions getätigt
%  - Daraus wird eine SARS Tupel geholt
%  - Value Update
%  - Testen, ob Ziel erreicht ist

\noindent
Das Ziel des Verfahren ist es die Q-Values der Tabelle zu ermitteln. Dazu werden die
Q-Values mit Null initialisiert, um diese anschließend iterativ zu bestimmen. Das Verfahren 
sieht vor, dass zufällige Actionen ausgewählt und in einem Environment ausprobiert werden.\\
Daraus resultieren ein State, eine Action, ein Reward und ein Folgestate, auch SARS-Tupel
genannt. Der erste State $s$ ist der Ausgangszustand, in dem man die zufällig gewählte
Action $a$ getätigt hat. Als Folge auf das Ausführen dieser Action erhält man einen
unmittelbaren Reward $r$, der vom Environment bestimmt wird, sowie den Folgestate $s'$,
also den State, den das Environment nach Ausführung von $a$ erreicht hat.\\
Ist das SARS-Tupel gefunden, werden die Q-Values der Tabelle angepasst, wobei der folgende
Zusammenhang verwendet wird.
\begin{align}
Q(s, a) = r_{s,a} + \gamma \max_{a' \in A}Q(s', a')
\label{aln:QValues}
\end{align}

\noindent
$Q(s, a)$ ist das Value der Action $a$ im wahrgenommenen State $s$, also der long-term
Reward, der erwartet wird, führt man in State $s$ die Action $a$ aus. Das Ziel ist es
$Q(s, a)$ so anzupassen, dass es sich dem tatsächlich zu erwartenden long-term reward
annähert. Der unmittelbare Reward $r_{s,a}$ wird aus dem SARS-Tupel erhalten, ebenso wie
der State $s'$.\\ % TODO verstehe nicht ganz die Aussage des Satzes
Befindet sich der Agent im State $s'$, so ist der maximal zu erwartende Reward, der 
maximal zu erwartende Reward der besten Action im State $s'$. Das bedeutet, dass
der Term $\max_{a'\in A} Q(s', a')$ der erwartete long-term Reward der besten Action des
Folgezustandes, also auch der bestmögliche long-term Reward, den man in State $s'$
erwarten kann, ist. Nach (\ref{aln:QValues}) gleicht $Q(s, a)$ dem unmittelbaren Reward
$r_{s,a}$ addiert mit dem besten zu erwartenden long-term Reward des Folgezustandes. Der
Wert des Folgezustandes wird dabei mit dem Hyperparameter $\gamma$ multipliziert, wobei % discount factor statt hyperparam
$\gamma$ standardmäßig zwischen $0.9$ und $0.99$ gewählt wird.\\
Mit Formel (\ref{aln:QValues}) erhalten wir einen neuen Wert für $Q(s, a)$, also einen
neuen Wert in unserer Q-Table. Da die Wirkung einer Action in Frozen Lake nicht immer die
selbe ist und wir daher auch nicht im Allgemeinen davon ausgehen dürfen, wäre es nicht gut % bitte nicht wir schreiben
den Wert der Q-Table gleich dem ermittelten Wert zu setzen. Stattdessen wird der Wert der
Tabelle in Richtung des ermittelten Wertes angepasst. Dies geschieht durch die in
(\ref{aln:QValueAdapt}) dargestellte Zuweisung.

\begin{align}
  Q(s, a) \leftarrow (1 - \alpha)Q(s, a) +
  \alpha(r_{s, a} + \gamma \max_{a'\in A}Q(s', a'))
\label{aln:QValueAdapt}
\end{align}

Wobei der Hyperparameter $\alpha$ zwischen 0 und 1 gesetzt wird, um den Wert $Q(s, a)$ bei
jeder Iteration schrittweise zu überblenden. Dadurch nähern sich die Q-Values der Tabelle
den tatsächlichen Q-Values an, wodurch sich das Environment erfolgreich spielen lässt.\\
Um zu testen, wie gut unsere gefundenen Q-Values funktionieren, wird wie folgt % bitte nicht unsere
vorgegangen. Man initialisiert ein Environment und lässt den Agenten in jedem erfahrenen % bitte nicht Man initialisert 
State die Action wählen, die das höchste Q-Value besitzt.

\subsection{Ergebnisse}
In unserem Experiment wird als gelöst angesehen, wenn 80\% von 20 Spielen erfolgreich % bitte nicht unserem
beendet wurden. Dieser Aufbau benötigt für 4x4 Frozen Lake zwischen 6000 und 42000
Iterationen, was zwischen einer und sechs Sekunden benötigt. Abbildung
\ref{fig:frozen-lake_q_learning} visualisiert den typischen Trainingsablauf, wobei der
durchschnittliche Reward aus 20 Spielen gezeigt wird.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.7]{res/frozen_lake_reward_tabular_q.png}
\caption{Ergebnisse Tabular-Q-Learning FrozenLake-v0}
\label{fig:frozen-lake_q_learning}
\end{figure}


\subsection{Tic Tac Toe}

% tic tac toe


\subsection{Grenzen} % TODO: Andere Überschrift
Die Einsetzbarkeit dieses Verfahrens wird dadurch beschränkt, dass es in vielen
Problemstellungen viele unterschiedliche States gibt, welche nicht mehr in einer Q-Table
gespeichert werden könnten. Um dieses Problem zu umgehen gibt es die Abwandlung \grqq
Deep-Q-Learning\grqq, bei der ein neuronales Netz benutzt wird, um die Values von Actions
zu bestimmen. Diese werden dann nicht mehr in einer Tabelle gespeichert und gelesen,
sondern von einem Modell berechnet.



\newpage
% Bruno TODO
\section{Deep Q-Learning}
Erweitung des Q-learning Verfahrens um neuronales Netz, was die Nutzung von Spielen und
Simulationen mit kontinuierlichem Spielbereich (Observation-Space) ermöglicht. Umsetzung
mehrerer Environements (Atari Pong, Roboschool Environments) und Beschreibung deren
Funktionsweise sowie folgender Terminologien und Verfahren mit diversen Implementierungen:
% \url{https://github.com/dephiloper/independent-coursework-rl/tree/master/preparation/07_deep_q_learning}

\begin{itemize}
\itemsep0pt
\item off-policy
\item Erklärung value learning mit Netz
\item value network
\item replay buffer, target network
\item atari pong (\url{https://gym.openai.com/envs/#atari})
\item roboschool pong environment (\url{https://github.com/openai/roboschool}
\item limitations
\item extensions: duelling dqn, categorical dqn, n-step dqn
\end{itemize}

% Philipp TODO
\newpage
\section{Policy Gradients}
Policy Gradients beschreibt eine Unterfamilie von Reinforcement Learning Methoden aus dem
Bereich der policy learning Verfahren. Anders als die beiden zuvor beschriebenen Verfahren
Tabular-Q-Learning und Deep Q-Learning fokussieren sich policy Verfahren auf das Erlernen
einer direkten policy. Dies bringt den Vorteil mit sich, dass Policy Gradients auf
Environments mit kontinuierlichen action space, sprich Actions mit Werterepräsentationen in 
Gleitkommadarstellung, angewandt werden können (siehe \ref{sec:action} \nameref{sec:action}
- Einschlagwinkel eines Lenkrads). Q-Learning Verfahren hingegen weisen jedem State oder
jeder Action immer eine Value zu, was das Optimierungsproblem bei einem kontinuierlichen
action space deutlich erschwert \cite[~S.242]{L2018}. Ein zusätzlicher Vorteil der Policy
Gradients ist die stochastische Eigenschaft von Environments. Anders als Q-Learning wird es
mit Policy Gradients möglich, die zugrundeliegende Wahrscheinlichkeitsverteilung des
Environments besser aufzugreifen, da das Verfahren, statt mittels Erwartungswerten von aktuellen
und zukünftigen Rewards zu rechnen, mit einer Wahrscheinlichkeitsverteilung von Actions arbeitet.
In der nachfolgenden Abbildung wird genau diese Wahrscheinlichkeitsverteilung von diskreten 
Actions nach Anwendung der Policy Approximation visualisiert.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.4]{res/policy_approxim.png}
\caption{Wahrscheinlichkeitsverteilung von Actions mittels Policy Approximation}
\source{\cite[~S.243 - Chapter 9 - Figure 1]{L2018}}
\label{fig:markov}
\end{figure}




% Philipp TODO
\newpage
\section{Actor Critic}
Beschreibung einer state-of-the-art RL Methode Actor Critic. Kombination aus policy learning und value learning. Ansatz mit mehreren Environments parallel zur Beschleunigung und Stabilisierung des Lernprozesses.
\begin{itemize}
\itemsep0pt
\item actor and critic net
\item reducing variance $\rightarrow$ Advantage
\item extensions a3c
\end{itemize}

\newpage
\section{Teeworlds}
Wie in der Einleitung erwähnt Erfolg die Umsetzung eines eigenen Environments mittels des
Spiels Teeworlds (\url{https://teeworlds.com/}). Initial wird das Spiel erläutert und
dessen grundlegende Struktur. Anschließend erfolgt eine Beschreibung zur Umsetzung des
Environments, wobei folgende Themen kategorisiert werden:
\begin{itemize}
\itemsep0pt	
\item Warum wir uns dafür entschieden haben
\item Wahl des Verfahrens
\item FOSS, teeworlds client, teeworlds server
\item custom environment (gym)
\item Bilder als Input $\rightarrow$ stack of images
\item Umsetzung einer vereinfachten Version des Spiels
\item die Aufgabe besteht darin, Schilde und Herzen einzusammeln und Level zu bestreiten
\item Implementierung erläutert mittels Listings
\item Herausforderungen bei der Implementierung und wie diese bewältigt wurden
\item Erklärung Hyperparameter, Lernprozess, Lernstats
\item Vergleich, Trainingsdauer, Auswertung, Erweiterungen
\item Beschreibung System Setup
\end{itemize}

\newpage
%\bibliographystyle{plain}
\bibliographystyle{alpha}
\bibliography{references}
\newpage
\listoffigures 
\listoftables 
\end{document}
