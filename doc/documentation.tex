% Allgemeine section Aufbau
% - Theorie
% - Einordnung (on-policy/off-policy, value-based/policy-based)
% - Implementationen
%   - Herausstellen wie Exploration und Exploitation umgesetzt ist
%   - falls vorhanden Graphs einbauen, die den Trainingsprozess veranschaulichen

\documentclass[11pt]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[german]{babel}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\PassOptionsToPackage{hyphens}{url}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{subcaption}

\newcommand{\source}[1]{\vspace{-5pt} \caption*{\hfill \textbf{Source:} {#1}} }

\newenvironment{conditions}
  {\par\vspace{\abovedisplayskip}\noindent\begin{tabular}{>{$}l<{$} @{${}:{}$} l}}
  {\end{tabular}\par\vspace{\belowdisplayskip}}

\newcommand{\lbparagraph}[1]{\paragraph*{#1}\mbox{}\\}

\renewcommand{\baselinestretch}{1.2}
\geometry{
  left=3cm,
  right=3cm,
  top=3cm,
  bottom=3cm,
  bindingoffset=5mm
}
\usepackage{graphicx}

\begin{document}
% \sloppy

\begin{titlepage}
	\centering
	{\scshape\LARGE Hochschule für Technik und Wirtschaft Berlin \par}
	\vspace{2cm}
	{\Huge \scshape{Independent Coursework}\par}
	\vspace{2cm}
	{\LARGE \scshape{Reinforcement Learning}\par}
	{\scshape\Large Projekt Dokumentation\par}
	%\includegraphics[width=0.9\linewidth]{title.jpg}\par\vspace{1cm}
	\vspace{4cm}
	{\large\itshape Bruno Schilling,\\Philipp Bönsch\par}
	\vfill
	
% Bottom of the page
	{\large Betreuer: Prof. Dr.-Ing. Barthel \par}
	\vspace{1cm}
	{\large \today\par}
\end{titlepage}

\lstset{basicstyle=\ttfamily\small,breaklines=true}
\newpage
\tableofcontents
\newpage
\section{Introduction}

\subsection{Problem Case}
Machine Learning Methoden werden zunehmend benutzt, um Probleme in der realen Welt zu
lösen und erhalten daher in der Zukunft einen immer höheren Stellenwert. Spiele im
speziellen stellen meist Problemstellungen dar, welche zum Teil auch schon durch Machine
Learning Verfahren absolviert werden konnten. Darunter zählen klassische Brettspiele, wie Go
oder Schach\cite{DM2018} aber auch echtzeit Spiele, wie Dota 2\cite{OA2019},
Quake\cite{DM2019} oder Starcraft\cite{DM2019_2}. Für das Supervised Research Independent Coursework
ergibt sich folgende Problemstellung: Welche Machine Learning Verfahren können für das
absolvieren eines Computerspieles verwendet werden und wie integriert man diese in die
virtuelle Umgebung?

\subsection{Objectives}
Es soll innerhalb eines Supervised Research Independent Courseworks untersucht werden, wie
sich ein Agent in einer virtuellen Spielumgebung mit Hilfe von Machine Learning Methoden,
speziell Reinforcement Learning Verfahren, steuern lässt. Dafür werden initial
Untersuchungen durchgeführt, um den aktuellen Stand der Technik zu erfassen. Genauer
sollen Algorithmen mittels der von OpenAI Gym zur Verfügung gestellten Testumgebung
untersucht werden. Die dabei gesammelten Erfahrungen sollen benutzt werden, um einen
Agenten für ein Computerspiel zu entwickeln. Das Ziel des Research Projektes ist es, neue
Kenntnisse im Bereich Reinforcement Learning zu erhalten und bereits bestehendes Wissen in
den Schwerpunkten Artificial Intelligence und Machine Learning zu festigen. Weiterhin soll
dieses Wissen anschließend in einem Prototyp umgesetzt werden, bei dem ein Agent in einem
Echtzeitspiel gesteuert wird.

% Philipp TODO: - Arbeit beinhaltet:
%                 - Recherche/Aufarbeitung (wichtiger Teil). Deshalb auch theoretische
%                   Bereiche ausgeführt
%                 - Implementierungen

\newpage
\section{Fundamentals}
In diesem Abschnitt werden die Grundlagen des Machine Learning Bereichs Reinforcement Learning 
erläutert. Damit soll eine Grundlage für Themenbereiche in nachfolgenden Kapiteln geschaffen
werden, welche sich auf die Terminologie dieses Abschnittes beziehen.

\subsection{Reinforcement Learning}
Reinforcement Learning beschreibt eine Ansammlung von Machine Learning Verfahren, in denen die 
lernende Entität, auch \textit{Agent}, keine bezeichneten Beispiele für das Lösen eines Problemes 
erhält. Die einzige Information, welche mitgeteilt wird, ist Feedback zu gewählten Aktionen in Form
einer Belohnung oder Bestrafung. Diese Art des Feedbacks wird auch als Verstärkung bezeichnet.
Somit muss vom Lernenden selbstständig eine Strategie entwickelt werden, um das gegebene
Problem zu lösen, indem versucht wird den erhaltenen Belohnungswert zu maximieren. Anschließend
werden die Terminologien im Bereich Reinforcement Learning geklärt sowie Kategorisierungen zu 
nachfolgend beschriebenen Verfahren erläutert.

\subsubsection{Agent}
Ein Agent nimmt seine Umgebung (\textit{Environment}) über Sensoren wahr und kann mittels Aktuatoren
Handlungen (\textit{Actions}) vollziehen um diese Umgebung zu beeinflussen oder zu verändern 
\cite[~S.60]{RN2009}. Für die ausgeführten Aktionen erhält der Agent einen Leitwert wie positiv oder
negativ sich diese Aktion auf die Umgebung oder den Agenten ausgewirkt hat. Wie bereits im vorherigen
Absatz erwähnt, kann der Agent als lernende Entität bezeichnet werden, da dieser seine Verhaltensweise
im Bereich  Reinforcement Learning selbstständig immer weiter in Richtung bestmöglicher Lösung einer 
Problemstellung optimiert. In einem Computerspiel kann ein Agent als menschlicher Spieler
repräsentiert werden. Dabei zählen die Augen und Ohren zu seinen Sensoren mit denen das Spiel
wahrgenommen wird und über seine motorischen Fähigkeiten und somit dem bedienen einer Tastatur
können Handlungen vollzogen werden. Ein im Spiel umgesetzter Nicht-Spieler-Charakter wird ebenfalls
als Agent gesehen, wobei dessen Wahrnehmung und Aktuatoren meist direkt im Spiel umgesetzt sind.


\subsubsection{Environment}
Das Environment definiert alles was den Agenten umgibt. Die Kommunikation zwischen einem Environment
und dem Agenten ist limitiert auf die \textit{Actions}, welche der Agent ausführen kann, die 
\textit{Observations}, welche der Agent nach dem Ausführen einer Action vom Environment erhält, 
sprich wahrnimmt, und durch die Bewertung der getätigten Aktionen, dem \textit{Reward} 
\cite[~S.5]{L2018}. Im Falle eines Computerspiels umfasst das Environment alle im Spiel festgelegten
Gesetzmäßigkeiten, aber auch die Netzwerkverbindung oder das Medium auf dem das Spiel ausgeführt
wird kann zum Environment gezählt werden.

\subsubsection{Action}
Eine Action bietet die Möglichkeit für einen Agenten aktiv Einfluss auf das Geschehen im
Environment zu nehmen. Zu Actions zählen das Ausführen von Spielzügen, die durch das
Regelwerk erlaubt sind sowie die Änderung eines Zustandes in umfangreicheren Systemen. Die
Komplexität der Actions kann dabei ebenfalls variieren. Des Weiteren unterscheidet man im Bereich
Reinforcement Learning zwischen zwei Arten von Actions: diskrete und kontinuierliche Actions.
Diskrete Actions definieren eine endliche Menge an gegenseitig ausschließenden Handlungen die ein
Agent vollziehen kann \cite[~S.8]{L2018}. Dazu zählen Actions wie sich nach links und rechts zu
bewegen oder zu springen. Unter kontinuierlichen Actions versteht man hingegen Handlungen, die
über einen zusätzlichen Wert beeinflusst werden, wie zum Beispiel das Steuern eines Fahrzeuges
mit einem Lenkrad. Dabei ist nich ausschließlich die Lenkrichtung ausschlaggebend, sondern auch
der Einschlagswinkel des Lenkrades, welcher die Intensität der Lenkung festlegt.

\subsubsection{Observation}
Unter einer Observation versteht man die Wahrnehmung des Agenten, welche von dem Environment 
bereitgestellt wird. Dabei enthält eine Observation immer Informationen dazu, was gerade in der
Umgebung des Agenten passiert. Jedoch geben Observations für den Agenten nicht den Einblick
in das gesamte Environment, sondern limitieren dessen Wahrnehmung auf notwendige bzw. ausschließlich
gewollte Informationen \cite[~S.8 f.]{L2018}. Eine Observation kann aus einzelnen Werten bestehen, 
welche zum Beispiel im Falle von Pong Informationen wie Position und Geschwindigkeiten zu den
Schlägern und dem Spielball zurückliefern. Zusätzlich kann eine Observationen auch in Form eines 
Bildes oder einer Aneinanderreihung von Bildern umgesetzt sein. Das Bild wird dabei von 
dem Agent verarbeitet und dieser führt dementsprechende Actions zum visuell wargenommenen Zustand
bzw. der Anordnung von Pixeln aus. 


\subsubsection{Reward}
Im Gebiet Reinforcement Learning beschreibt der Reward einen Wert, welcher periodisch oder nach
dem Eintreffen einer definierten Gegebenheit dem Agenten vom Environment bereitgestellt wird.
Ziel des Rewards ist es dem Agenten Auskunft darüber zu geben, wie gut sich dieser Verhält, sprich
diesen zu bewerten. Wie bereits beschrieben, versucht der Agent den erhaltenen Reward zu
maximieren. Somit bildet der Reward als Feedback ein Kernelement im Bereich Reinforcement
Learning und definiert den \textit{bestärkenden} Anteil der Verfahren des machinellen Lernens 
\cite[~S.6 f.]{L2018}. Der Reward ist immer lokal zu betrachten, dass heißt er gibt ausschließlich 
Auskunft über den Erfolg der aktuell ausgeführten Actions und nicht über das bereits insgesamt
gesammelte Ergebnis aller Erfolge und Misserfolge. Des Weiteren bedeutet ein hoher lokaler Reward 
nicht, dass der daraus folgende Zustand global betrachet keine noch größeren negativen Auswirkungen 
auf den Agenten haben wird.

\subsection{Markov Processes}
Der Makrov Prozess und seine Erweiterungen bilden das theoretische Konstrukt für Reinforcement Learning
Verfahren. Durch die Beleuchtung dieser Prozesse werden weitere Terminologien geklärt, welche ebenfalls
ihren Einsatz im Reinforcement Learning finden.

\subsubsection{Markov Process}
Die einfachste Variante in der Markov Familie ist der Markov Prozess (Markov-Kette). Ein Markov Prozess
beschreibt ein System, welches ausschließlich observiert werden kann, sprich es kann nicht in die Dynamik
des Systems eingegriffen werden. Jeder Markov Prozess enthält eine Menge von \textit{States}, zwischen
denen das System nach den Gesetzlichkeiten der Systemdynamik hin und her wechseln kann. Die Menge aller
möglichen States wird dabei als \textit{State Space} bezeichnet. Wird das System observiert ergibt sich
durch die Wechsel von State zu State eine Folge, welche als \textit{History} bezeichnet wird. 

Damit ein Prozess als Markov Prozess bezeichnet werden kann muss dieser die \textit{Markov Eigenschaft}
aufweisen, welche definiert, dass die zukünftige Dynamik des Systems von jedem State nur von diesem State abhängen darf. Die Markov Eigenschaft legt somit fest, dass ein State differenziert von jedem anderen State 
des Systems betrachtet werden kann und deshalb die History für die Vorhersage der Systemdynamik nicht
notwendig ist.

Ein Prozess der die Markov Eigenschaft aufweißt muss die Fähigkeit bieten, eine Vorhersage über
die Wahrscheinlichkeit des Eintreffens eines Folgezustandes, ausgehend von einem Ausgangszustand, zu 
ermitteln. Dafür werden Wahrscheinlichkeiten für die Übergänge zwischen zwei States in einer 
\textit{Transition Matrix} festgehalten. \autoref{fig:markov}: \nameref{fig:markov} veranschaulicht
ein Wettermodell in der Repräsentationsform eines Markov Prozesses mit den States \textit{Sunny} und
\textit{Rainy} und deren Übergangswahrscheinlichkeiten in zukünftige Folgezustände.

\begin{figure}[htp]
\centering
\includegraphics[scale=0.2]{/home/phil/Pictures/srwm.png}
\caption{Sunny/Rainy weather model}
\source{Figure 4: Sunny/Rainy weather model \cite[~S.13]{L2018}}
\label{fig:markov}
\end{figure}

\subsubsection{Markov Reward Process}
Die erste Erweiterung zum Markov Process ensteht durch die Hinzunahme eines weiteren Wertes zu den 
Übergängen zwischen zwei States, dem \textit{Reward} $R$. Dieser Wert definiert wie hoch die Belohnung
oder Bestrafung bei einem Wechsel der States ausfällt. Durch den Reward kann anschließend der
\textit{value of state} $V(s)$ berechnet werden. Für jeden State $s$ ist der value of state $V(s)$
der durchschnittliche (oder erwartete) Ertrag, welcher durch das Verfolgen des Markov-Reward-Prozess
entsteht und somit durch alleiniges Observieren die Bewertung von Zuständen ermöglicht.
Zusätzlich wird der \textit{discount factor} $\gamma$ eingeführt, welcher den erhaltenen Reward
für Übergänge die weiter in der Zukunft liegen verringert. Der value of state wird dabei wie
nachfolgend formuliert für eine \textit{Episode} berechnet, wobei eine Episode immer aus einer
Folge von States besteht und entweder durch das Eintreffen eines terminierenden States oder das
Erreichen einer Maximalanzahl von States beendet wird:

\begin{equation}
V(s)_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} ... = \sum_{k=0}^\infty \gamma^{k} R_{t+k+1}
\label{eq:value-of-state}
\end{equation} 

\begin{conditions}
 V(s)     	&  value of state \\
 R	     	&  Reward \\   
 \gamma 	&  discount factor für Rewards, die weiter in der Zukunft liegen
\end{conditions}

\subsubsection{Markov Decision Process}
Der Markov Decision Process erweitert den Markov Reward Process um eine endliche Menge an Actions.
Diese Menge entspricht dem \textit{Action Space} des Agenten. Dadurch kommt es nun nicht mehr
ausschließlich zur passiven Observierung, sondern der Agent kann zu jeder Zeit eine Action wählen um
somit die Wahrscheinlichkeiten des Zielzustandes selbstständig zu beeinflussen. Des Weiteren hängt 
der Reward, den der Agent erhält, nun nicht mehr ausschließlich von dem Zustand ab, in dem er endet,
sondern auch von der Action, die zu diesem Zustand führt. 

Die Verhaltensweise, die der Agent ausübt und welche für die Wahl der Actions verantwortlich ist,
wird als \textit{Policy} bezeichnet. Formal wird diese als Wahrscheinlichkeitsverteilung
über Actions für jeden möglichen Zustand definiert \cite[~S.22 f.]{L2018}. 

\subsection{Further Taxonomy} % Überschrift gefällt mir irgendwie nicht so richtig
\subsubsection{Policy Learning vs. Value Learning}
\subsubsection{On-Policy vs. Off-Policy}
\subsubsection{Model-Free vs. Model-Based}

\newpage
% Bruno TODO: - subsection name: Exploration vs Exploitation
%             - k-armed bandit as example
\subsection{Exploration und Exploitation}
In diesem Abschnitt wird der Konflikt zwischen Exploration vs Exploitation des
Reinforcement Learnings erläutert und anhand des k-armed-Bandit Problems veranschaulicht.
Die dabei realisierten Implementierungen werden beschrieben und deren Ergebnisse
vorgestellt.

\subsubsection{Das k-armed-Bandit Problem}
% Erklärung des k-armed-Bandit Problems
Das k-armed-Bandit Problem besteht darin aus $k$ verschiedenen Actions zu wählen.
Nachdem eine Action gewählt wurde, erhält der Agent einen Reward, der aus einer
Wahrscheinlichkeitsverteilung gezogen wird. Der Erwartungswert der
Wahrscheinlichkeitsverteilung hängt von der gewählten Action ab. Das Ziel besteht darin
mit der Ausführung von $n$ Actions die höchstmögliche Summe von Rewards zu erhalten.\\
Der Erwartungswert der hinter einer Verteilung einer Action steht wird $Value$ der Action
genannt. Würden wir die Values der einzelnen Actions kennen, so wäre die Wahl der Action
einfach, indem die Action mit dem höchsten Erwartungswert gewählt werden würde.
Allgemein ist in Reinforcement Learning Problemen der Value einer Action nicht bekannt.
Stattdessen gibt es nur Abschätzungen bezüglich des Values einer Action, die aus den
vorhergehenden Erfahrungen hervorgehen.\\
Um den Value einer Action besser abschätzen zu können, muss diese Action ausprobiert
werden. Umso häufiger die Action ausgeführt wurde, umso besser ist auch die Abschätzung
des Values dieser Action.\\
Der Vorgang des Ausprobierens wird $Exploration$ genannt. Im Gegensatz dazu steht die
$Exploitation$ bei der die gesammelten Erfahrungen genutzt werden, um die Action
auszuwählen, die den höchsten Reward verspricht.\\
Das k-armed-Bandit Problem gilt als Vereinfachung zu allgemeinen Reinforcement Learning
Problemen, da nicht zwischen unterschiedlichen Ausgangszuständen unterschieden wird. Mit
anderen Worten wird keine Observation durchgeführt. Weiterhin wird der Erwartungswert des
Rewards einer Action nicht über die Dauer des Spiels verändert. Dies bedeutet, dass die
Values der unterschiedlichen Actions am Anfang und Ende des Spiels identisch sind, wodurch
auch Erfahrungen, die zu Beginn des Spielens gemacht wurden, am Ende benutzt werden
können.

\subsubsection{Exploration Strategien}
Im folgenden werden verschiedene Exploration-Strategien beleuchtet. Dazu werden diese erst
theoretisch erklärt, um diese anschließend praktisch an einem k-armed-Bandit Problem zu
vergleichen.
% Die Implementationen sind unter \url{https://github.com/Bluemi/rl_testbed} zu finden.

\paragraph{$\epsilon$ greedy / decaying $\epsilon$ greedy}
Eine Strategie, die zu jedem Zeitpunkt die Action wählt, die nach derzeitigem Wissen den
höchsten Value hat wird $greedy$ genannt. Epsilon-greedy Strategien sind greedy
Strategien, die mit einer gewissen Wahrscheinlichkeit $\epsilon$ eine zufällige Action
wählen. Sie wählen also mit einer Wahrscheinlichkeit von $1 - \epsilon$ die Action, von
der sie sich den höchsten Reward versprechen und andernfalls eine zufällige Action.\\
In der Praxis werden häufig decaying $\epsilon$ greedy Strategien verwendet, bei denen der
$\epsilon$ Wert über den Trainingsprozess hinweg verringert wird. Dadurch werden zu Beginn
des Trainings viele neue Actions ausprobiert, während gegen Ende das Anwenden des
gelernten stärker in den Vordergrund rückt.

\paragraph{Optimistic Initial Values}
Ein Agent, der ein k-armed-Bandit Problem lösen soll, besitzt eine interne Abschätzung der
Values, um Actions wählen zu können, die einen besseren Reward versprechen. Die
Bewertung der Actions wird beeinflusst durch die erfahrenen Rewards nach dem wählen einer
Action.\\
Neben $\epsilon$-greedy Verfahren ist es eine weitere Möglichkeit Exploration umzusetzen,
die initiale Bewertung der Values sehr optimistisch vorzunehmen. Dadurch wird der Agent
bei der Wahl einer Action vom erhaltenen Reward "enttäuscht", sodass sich die Einschätzung
dieser Action verschlechtert. Wird nun immer die Action gewählt, die am besten
eingeschätzt ist, so werden über die Zeit alle Actions versucht, bis sie sich dem
tatsächlichen Erwartungswert nähern.

\paragraph{Upper Confidence Bound Action Selection}
Während $\epsilon$ greedy Methoden zufällig zwischen allen nicht greedy Actions
auswählen, wählt die Upper Confidence Bound Ac\-tion Selection Methode Actions entsprechend
dreier Parameter aus.\\
Der erste Parameter ist $t$ die Anzahl der Actions, die bis jetzt gespielt wurden, der
zweite ist der bis jetzt ermittelte $Value$ der Action $a$ bezeichnet mit $Q(a)$ und der
dritte Parameter ist die Anzahl, wie oft Action $a$ schon ausprobiert wurde. Die nächste
zu spielende Action wird dann nach folgender Gleichung gewählt:

\[ A = \argmax_a\left( Q(a) + c*\sqrt\frac{N(a)}{\ln(t)} \right) \]

\noindent
$A$ ist die gewählte Aktion. $N(a)$ ist die Anzahl, wie oft die Aktion $a$ schon gewählt
wurde. Durch den Term $c*\sqrt\frac{N(a)}{\ln(t)}$ wird sichergestellt, dass selten
benutzte Aktionen eine höhere Bewertung erhalten und damit wahrscheinlicher Ausprobiert
werden. Über den Hyperparameter $c$ lässt sich bestimmen, wie stark selten benutzte
Actions ausprobiert werden sollen.\\
Ein Vorteil gegenüber $\epsilon$-greedy Strategien ist, dass durch die Einbeziehung von
$Q(a)$ Actions häufiger ausprobiert werden, die besser wirken, während Actions, die sehr
schlecht wirken weniger oft ausprobiert werden.

\subsubsection{Implementation}

\newpage
\subsection{Open AI Gym}
Ein von der Firma Open AI (\url{https://gym.openai.com/}) bereitgestelltes Toolkit,
welches es ermöglicht Verfahren im Bereich Reinforcement Learning algorithmisch umzusetzen
und anschließend miteinander zu vergleichen. Dabei werden Umgebungen wie Spiele oder
Simulationen anhand einer definierten API zur Verfügung gestellt. Hierbei wird erneut auf
die dabei zu beachtende Terminologie eingegangen und der Aufbau und die eigene
Realisierung eines Gym Systems thematisiert.

\begin{itemize}
\itemsep0pt
\item Erklärung und Verwendung: Environment, Observation Space, Action Space
\item Realisierung Environment Tic Tac Toe:
  % \url{https://github.com/dephiloper/independent-coursework-rl/blob/master/preparation/05_v_learning/tictactoe_v_learning.py}
\end{itemize}

\section{Cross-Entropy-Method}
Erläuterung der Cross-Entropy-Method und der Verwendung eines neuronalen Netzes zum Lernen
von Cart-Pole
% (\url{https://github.com/dephiloper/independent-coursework-rl/tree/master/preparation/04_cross_entropy}).
\\
Einordnung des Verfahrens im Bereich policy learning.

\section{Q-Learning}
Erläuterung des Q-Learning Verfahrens und deren Erweiterungen, dessen Einordnung im
Bereich value learning sowie Begriffserklärung und Implementierung von Q-learning anhand
der Verwendung eines bereits existierenden Gyms. Erläuterung folgender Terminologien:

% Bruno TODO: - write section
%             - tictactoe implementation example
%             - frozen lake
%             - frozen lake 2x2 explanation

\begin{itemize}
\itemsep0pt
\item value learning Familie
\item Bellman Equation und Proof of Bellman Optimality Equation 
\item Frozen Lake Environement
\item V(s) and Q(s,a)
\item einfache Erläuterung der Abwandlung von Frozen Lake: 2x2 Nonslip Lake
\item Vergleich von V(s) und Q(s,a) und deren Einsatz
\item Implementierung q-learning und v-learning Frozen Lake:
  % \url{https://github.com/dephiloper/independent-coursework-rl/tree/master/preparation/05_v_learning}
\end{itemize}

% Bruno TODO
\section{Deep Q-Learning}
Erweitung des Q-learning Verfahrens um neuronales Netz, was die Nutzung von Spielen und
Simulationen mit kontinuierlichem Spielbereich (Observation-Space) ermöglicht. Umsetzung
mehrerer Environements (Atari Pong, Roboschool Environments) und Beschreibung deren
Funktionsweise sowie folgender Terminologien und Verfahren mit diversen Implementierungen:
% \url{https://github.com/dephiloper/independent-coursework-rl/tree/master/preparation/07_deep_q_learning}

\begin{itemize}
\itemsep0pt
\item off-policy
\item Erklärung value learning mit Netz
\item value network
\item replay buffer, target network
\item atari pong (\url{https://gym.openai.com/envs/#atari})
\item roboschool pong environment (\url{https://github.com/openai/roboschool}
\item limitations
\item extensions: duelling dqn, categorical dqn, n-step dqn
\end{itemize}

% Philipp TODO
\section{Policy Gradients}
Erneute Beschreibung von Verfahren im Bereich policy learning. Dabei fokussieren der
Policy Gradients Verfahren und der REINFORCE Methode. Thematisierung und Vergleich von
policy und value sowie Erweiterungen.
\begin{itemize}
\itemsep0pt
\item cross entropy method (bereits erläutert) dazu im Kontrast: REINFORCE
\item continuous observation space \& action space
\item Umsetzung von cart pole
\item Umsetzung von atari pong
\item limitations
\item high variance, Einführung einer Baseline
\end{itemize}

% Philipp TODO
\section{Actor Critic}
Beschreibung einer state-of-the-art RL Methode Actor Critic. Kombination aus policy learning und value learning. Ansatz mit mehreren Environments parallel zur Beschleunigung und Stabilisierung des Lernprozesses.
\begin{itemize}
\itemsep0pt
\item actor and critic net
\item reducing variance $\rightarrow$ Advantage
\item extensions a3c
\end{itemize}

\section{Teeworlds}
Wie in der Einleitung erwähnt Erfolg die Umsetzung eines eigenen Environments mittels des
Spiels Teeworlds (\url{https://teeworlds.com/}). Initial wird das Spiel erläutert und
dessen grundlegende Struktur. Anschließend erfolgt eine Beschreibung zur Umsetzung des
Environments, wobei folgende Themen kategorisiert werden:
\begin{itemize}
\itemsep0pt	
\item Warum wir uns dafür entschieden haben
\item Wahl des Verfahrens
\item FOSS, teeworlds client, teeworlds server
\item custom environment (gym)
\item Bilder als Input $\rightarrow$ stack of images
\item Umsetzung einer vereinfachten Version des Spiels
\item die Aufgabe besteht darin, Schilde und Herzen einzusammeln und Level zu bestreiten
\item Implementierung erläutert mittels Listings
\item Herausforderungen bei der Implementierung und wie diese bewältigt wurden
\item Erklärung Hyperparameter, Lernprozess, Lernstats
\item Vergleich, Trainingsdauer, Auswertung, Erweiterungen
\item Beschreibung System Setup
\end{itemize}

\newpage
\bibliographystyle{plain}
\bibliography{references}
\newpage
\listoffigures 
\listoftables 
\end{document}
