% Allgemeine section Aufbau
% - Theorie
% - Einordnung (on-policy/off-policy, value-based/policy-based)
% - Implementationen
%   - Herausstellen wie Exploration und Exploitation umgesetzt ist
%   - falls vorhanden Graphs einbauen, die den Trainingsprozess veranschaulichen

\documentclass[11pt]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[german]{babel}
\usepackage{geometry}

\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\PassOptionsToPackage{hyphens}{url}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{subcaption}

\newcommand{\lbparagraph}[1]{\paragraph*{#1}\mbox{}\\}
\renewcommand{\baselinestretch}{1.2}
\geometry{
  left=3cm,
  right=3cm,
  top=3cm,
  bottom=3cm,
  bindingoffset=5mm
}
\usepackage{graphicx}

\begin{document}
% \sloppy

\begin{titlepage}
	\centering
	{\scshape\LARGE Hochschule für Technik und Wirtschaft Berlin \par}
	\vspace{2cm}
	{\Huge \scshape{Independent Coursework}\par}
	\vspace{2cm}
	{\LARGE \scshape{Reinforcement Learning}\par}
	{\scshape\Large Projekt Dokumentation\par}
	%\includegraphics[width=0.9\linewidth]{title.jpg}\par\vspace{1cm}
	\vspace{4cm}
	{\large\itshape Bruno Schilling,\\Philipp Bönsch\par}
	\vfill
	
% Bottom of the page
	{\large Betreuer: Prof. Dr.-Ing. Barthel \par}
	\vspace{1cm}
	{\large \today\par}
\end{titlepage}

\lstset{basicstyle=\ttfamily\small,breaklines=true}
\newpage
\tableofcontents
\newpage
\section{Introduction}

\subsection{Problem Case}
Machine Learning Methoden werden zunehmend benutzt, um Probleme in der realen Welt zu
lösen und erhalten daher in der Zukunft einen immer höheren Stellenwert. Spiele im
speziellen stellen meist Problemstellungen dar, welche zum Teil auch schon durch Machine
Learning Verfahren absolviert werden konnten. Darunter zählen klassische Brettspiele, wie Go
oder Schach\cite{DM2018} aber auch echtzeit Spiele, wie Dota 2\cite{OA2019},
Quake\cite{DM2019} oder Starcraft\cite{DM2019_2}. Für das Supervised Research Independent Coursework
ergibt sich folgende Problemstellung: Welche Machine Learning Verfahren können für das
absolvieren eines Computerspieles verwendet werden und wie integriert man diese in die
virtuelle Umgebung?

\subsection{Objectives}
Es soll innerhalb eines Supervised Research Independent Courseworks untersucht werden, wie
sich ein Agent in einer virtuellen Spielumgebung mit Hilfe von Machine Learning Methoden,
speziell Reinforcement Learning Verfahren, steuern lässt. Dafür werden initial
Untersuchungen durchgeführt, um den aktuellen Stand der Technik zu erfassen. Genauer
sollen Algorithmen mittels der von OpenAI Gym zur Verfügung gestellten Testumgebung
untersucht werden. Die dabei gesammelten Erfahrungen sollen benutzt werden, um einen
Agenten für ein Computerspiel zu entwickeln. Das Ziel des Research Projektes ist es, neue
Kenntnisse im Bereich Reinforcement Learning zu erhalten und bereits bestehendes Wissen in
den Schwerpunkten Artificial Intelligence und Machine Learning zu festigen. Weiterhin soll
dieses Wissen anschließend in einem Prototyp umgesetzt werden, bei dem ein Agent in einem
Echtzeitspiel gesteuert wird.

% Philipp TODO: - Arbeit beinhaltet:
%                 - Recherche/Aufarbeitung (wichtiger Teil). Deshalb auch theoretische
%                   Bereiche ausgeführt
%                 - Implementierungen

\newpage
\section{Fundamentals}
In diesem Abschnitt werden die Grundlagen des Machine Learning Bereichs Reinforcement Learning 
erläutert. Damit soll eine Grundlage für Themenbereiche in nachfolgenden Kapiteln geschaffen
werden, welche sich auf die Terminologie dieses Abschnittes beziehen.

\subsection{Reinforcement Learning}
Reinforcement Learning beschreibt eine Ansammlung von Machine Learning Verfahren, in denen die 
lernende Entität, auch \textit{Agent}, keine bezeichneten Beispiele für das Lösen eines Problemes 
erhält. Die einzige Information, welche mitgeteilt wird, ist Feedback zu gewählten Aktionen in Form
einer Belohnung oder Bestrafung. Diese Art des Feedbacks wird auch als Verstärkung bezeichnet.
Somit muss vom Lernenden selbstständig eine Strategie entwickelt werden, um das gegebene
Problem zu lösen, indem versucht wird den erhaltenen Belohnungswert zu maximieren. Anschließend
werden die Terminologien im Bereich Reinforcement Learning geklärt sowie Kategorisierungen zu 
nachfolgend beschriebenen Verfahren erläutert.

\subsubsection{Agent}
Ein Agent nimmt seine Umgebung (\textit{Environment}) über Sensoren wahr und kann mittels Aktuatoren
Handlungen (\textit{Actions}) vollziehen um diese Umgebung zu beeinflussen oder zu verändern 
\cite[~S.60]{RN2009}. Für die ausgeführten Aktionen erhält der Agent einen Leitwert wie positiv oder
negativ sich diese Aktion auf die Umgebung oder den Agenten ausgewirkt hat. Wie bereits im vorherigen
Absatz erwähnt, kann der Agent als lernende Entität bezeichnet werden, da dieser seine Verhaltensweise
im Bereich  Reinforcement Learning selbstständig immer weiter in Richtung bestmöglicher Lösung einer 
Problemstellung optimiert. In einem Computerspiel kann ein Agent als menschlicher Spieler
repräsentiert werden. Dabei zählen die Augen und Ohren zu seinen Sensoren mit denen das Spiel
wahrgenommen wird und über seine motorischen Fähigkeiten und somit dem bedienen einer Tastatur
können Handlungen vollzogen werden. Ein im Spiel umgesetzter Nicht-Spieler-Charakter wird ebenfalls
als Agent gesehen, wobei dessen Wahrnehmung und Aktuatoren meist direkt im Spiel umgesetzt sind.


\subsubsection{Environment}
Das Environment definiert alles was den Agenten umgibt. Die Kommunikation zwischen einem Environment
und dem Agenten ist limitiert auf die \textit{Actions}, welche der Agent ausführen kann, die 
\textit{Observations}, welche der Agent nach dem Ausführen einer Action vom Environment erhält, 
sprich wahrnimmt, und durch die Bewertung der getätigten Aktionen, dem \textit{Reward} 
\cite[~S.5]{L2018}. Im Falle eines Computerspiels umfasst das Environment alle im Spiel festgelegten
Gesetzmäßigkeiten, aber auch die Netzwerkverbindung oder das Medium auf dem das Spiel ausgeführt
wird kann zum Environment gezählt werden.

\subsubsection{Action}
Eine Action bietet die Möglichkeit für einen Agenten aktiv Einfluss auf das Geschehen im
Environment zu nehmen. Zu Actions zählen das Ausführen von Spielzügen, die durch das
Regelwerk erlaubt sind sowie die Änderung eines Zustandes in umfangreicheren Systemen. Die
Komplexität der Actions kann dabei ebenfalls variieren. Des Weiteren unterscheidet man im Bereich
Reinforcement Learning zwischen zwei Arten von Actions: diskrete und kontinuierliche Actions.
Diskrete Actions definieren eine endliche Menge an gegenseitig ausschließenden Handlungen die ein
Agent vollziehen kann \cite[~S.8]{L2018}. Dazu zählen Actions wie sich nach links und rechts zu
bewegen oder zu springen. Unter kontinuierlichen Actions versteht man hingegen Handlungen, die
über einen zusätzlichen Wert beeinflusst werden, wie zum Beispiel das Steuern eines Fahrzeuges
mit einem Lenkrad. Dabei ist nich ausschließlich die Lenkrichtung ausschlaggebend, sondern auch
der Einschlagswinkel des Lenkrades, welcher die Intensität der Lenkung festlegt.

\subsubsection{Observation}
Unter einer Observation versteht man die Wahrnehmung des Agenten, welche von dem Environment 
bereitgestellt wird. Dabei enthält eine Observation immer Informationen dazu, was gerade in der
Umgebung des Agenten passiert. Jedoch geben Observations für den Agenten nicht den Einblick
in das gesamte Environment, sondern limitieren dessen Wahrnehmung auf notwendige bzw. ausschließlich
gewollte Informationen \cite[~S.8 f.]{L2018}. Eine Observation kann aus einzelnen Werten bestehen, 
welche zum Beispiel im Falle von Pong Informationen wie Position und Geschwindigkeiten zu den
Schlägern und dem Spielball zurückliefern. Zusätzlich kann eine Observationen auch in Form eines 
Bildes oder einer Aneinanderreihung von Bildern umgesetzt sein. Das Bild wird dabei von 
dem Agent verarbeitet und dieser führt dementsprechende Actions zum visuell wargenommenen Zustand
bzw. der Anordnung von Pixeln aus. 


\subsubsection{Reward}
Im Gebiet Reinforcement Learning beschreibt der Reward einen Wert, welcher periodisch oder nach
dem Eintreffen einer definierten Gegebenheit dem Agenten vom Environment bereitgestellt wird.
Ziel des Rewards ist es dem Agenten Auskunft darüber zu geben, wie gut sich dieser Verhält, sprich
diesen zu bewerten. Wie bereits beschrieben, versucht der Agent den erhaltenen Reward zu
maximieren. Somit bildet der Reward als Feedback ein Kernelement im Bereich Reinforcement
Learning und definiert den \textit{bestärkenden} Anteil der Verfahren des machinellen Lernens 
\cite[~S.6 f.]{L2018}. Der Reward ist immer lokal zu betrachten, dass heißt er gibt ausschließlich 
Auskunft über den Erfolg der aktuell ausgeführten Actions und nicht über das bereits insgesamt
gesammelte Ergebnis aller Erfolge und Misserfolge. Des Weiteren bedeutet ein hoher lokaler Reward 
nicht, dass der daraus folgende Zustand global betrachet keine noch größeren negativen Auswirkungen 
auf den Agenten haben wird.

\subsection{Markov Decision Process}
Der Makrov Prozess und seine Erweiterungen bilden das theoretische Konstrukt für Reinforcement Learning
Verfahren. Durch die Beleuchtung dieser Prozesse werden weitere Terminologien geklärt, welche ebenfalls
ihren Einsatz im Reinforcement Learning finden. Die einfachste Variante der Markov Familie ist der 
Markov Prozess (Markov-Kette). Ein Markov Prozess beschreibt ein System, welches ausschließlich
observiert werden kann, sprich es kann nicht in die Dynamik des Systems eingegriffen werden. Jeder
Markov Prozess enthält eine Menge von \textit{States}, zwischen denen das System nach den
Gesetzlichkeiten der Systemdynamik hin und her wechseln kann. Die Menge aller möglichen States wird
dabei als State Space bezeichnet. Wird das System observiert ergibt sich durch die Wechsel von State zu 
State eine Folge, welche als \textit{History} bezeichnet wird. 

Damit ein Prozess als Markov Prozess bezeichnet werden kann muss dieser die Markov Eigenschaft aufweisen,
welche definiert, dass die zukünftige Dynamik des Systems von jedem State nur von diesem State abhängen 
darf. Die Markov Eigenschaft legt somit fest, dass ein State differenziert von jedem anderen State des
Systems betrachtet werden kann und deshalb die History für die Vorhersage der Systemdynamik nicht
notwendig ist.

% Philipp TODO
\begin{itemize}
\itemsep0pt
\item Exploration vs. Exploitation
\item model-free vs. model-based
\item on- vs. off-policy
\item policy learning vs. value learning
\end{itemize}
\subsection{Markov Decision Process}
Der Makrov Decision Process dient als Fundament von Umsetzungen im Bereich Reinforcement Learning. In diesem Kapitel sollen dabei die Zusammenhänge und Unterschiede der einzelnen Markov Prozesse erläutert werden. Dabei wird unterschieden in:
\begin{itemize}
\itemsep0pt
\item Markov Process
\item Markov Reward Process
\item Markov Decision Process 
\end{itemize}

\newpage
% Bruno TODO: - subsection name: Exploration vs Exploitation
%             - k-armed bandit as example
\subsection{k-armed Bandit}
Hierbei soll das k-armed Bandit Problem der Wahrscheinlichkeitstheorie erläutert werden
und dabei auf  Schwerpunkte Exploration vs. Exploitation, einem Kernproblem des
Reinforcement Learning, eingegangen werden. Die dabei realisierten Implementierungen
sollen genauer erläutert und deren Bewandtnis in allgemeinen RL genauer beschrieben
werden.

\subsubsection{Das k-armed-Bandit Problem}
% Erklärung des k-armed-Bandit Problems
Das k-armed-Bandit Problem beschreibt auf vereinfachte Weise eine Grundproblemstellung vor
der ein Agent steht. Das Problem besteht darin aus $k$ verschiedenen $Aktionen$ zu wählen.
Nachdem eine Aktion gewählt wurde, erhält der Agent einen $Reward$, der aus einer
Wahrscheinlichkeitsverteilung gezogen wird. Der Erwartungswert der
Wahrscheinlichkeitsverteilung hängt von der gewählten Aktion ab. Das Ziel besteht darin
mit $n$ Aktions den höchstmöglichen Reward zu erhalten.\\
Würden wir den Erwartungswert der einzelnen Verteilungen kennen, so wäre die Wahl der
Aktion einfach, indem die Aktion mit dem höchsten Erwartungswert gewählt werden würde. Der
Erwartungswert der hinter einer Verteilung einer Aktion steht wird $Value$ der Aktion
genannt.\\
Allgemein ist in Reinforcement Learning Problemen der Erwartungswert der Aktionen nicht
bekannt, sodass dieser durch versuchen ermittelt werden muss. Stattdessen gibt es nur
Abschätzungen bezüglich des Values einer Aktion. Das ermitteln des erwarteten Rewards wird
$Exploration$ genannt. Im Gegensatz dazu steht die $Exploitation$ bei der die gesammelten
Erfahrungen genutzt werden, um die Aktion auszuwählen, die den höchsten Reward verspricht.

\subsubsection{Exploration Strategien}
Im folgenden werden verschiedene Exploration-Strategien beleuchtet. Dazu werden diese erst
theoretisch erklärt, um diese anschließend praktisch an einem k-armed-Bandit Problem zu
vergleichen.
% Die Implementationen sind unter \url{https://github.com/Bluemi/rl_testbed} zu finden.

\paragraph{$\epsilon$ greedy / decaying $\epsilon$ greedy}
Eine Strategie, die zu jedem Zeitpunkt die Aktion wählt, die nach derzeitigem Wissen den
höchsten Reward verspricht wird $greedy$ genannt. Epsilon-greedy Strategien sind greedy
Strategien, die mit einer gewissen Wahrscheinlichkeit $\epsilon$ eine zufällige Aktion
wählen. Sie wählen also mit einer Wahrscheinlichkeit von $1 - \epsilon$ die Aktion, von
der sie sich den höchsten Reward versprechen und andernfalls eine zufällige Aktion.\\
In der Praxis werden häufig decaying $\epsilon$ greedy Strategien verwendet, bei denen der
$\epsilon$ Wert über den Trainingsprozess hinweg verringert wird. Dadurch werden zu Beginn
des Trainings viele neue Aktionen ausprobiert, während gegen Ende das Anwenden des
gelernten stärker in den Vordergrund rückt.

\paragraph{Optimistic Initial Values}
Ein Agent, der ein k-armed-Bandit Problem lösen soll, bewertet die unterschiedlichen
Aktionen, um Aktionen wählen zu können, die einen besseren Reward versprechen. Die
Bewertung der Aktionen wird beeinflusst durch die erfahrenen Rewards nach dem wählen einer
Aktion.\\
Neben $\epsilon$-greedy Verfahren ist es eine weitere Möglichkeit Exploration umzusetzen,
die initiale Bewertung der Aktionen sehr optimistisch vorzunehmen. Dadurch wird der Agent
bei der Wahl einer Aktion vom erhaltenen Reward "enttäuscht", sodass sich die Einschätzung
dieser Aktion verschlechtert. Wird nun immer die Aktion gewählt, die am besten
eingeschätzt ist, so werden über die Zeit alle Aktionen versucht, bis sie sich dem
tatsächlichen Erwartungswert nähern.

\paragraph{Upper Confidence Bound Action Selection}
Während $\epsilon$ greedy Methoden zufällig zwischen allen nicht greedy Aktionen
auswählen, wählt die Upper Confidence Bound Ac\-tion Selection Methode Aktionen entsprechend
dreier Parameter aus.\\
Der erste Parameter ist $t$ die Anzahl der Aktionen, die bis jetzt gespielt wurden, der
zweite ist der bis jetzt ermittelte $Value$ der Aktion $a$ bezeichnet mit $Q(a)$ und der
dritte Parameter ist die Anzahl, wie oft $a$ schon ausprobiert wurde. Die nächste zu
spielende Aktion wird dann nach folgender Gleichung gewählt:

\[ A = \argmax_a\left( Q(a) + c*\sqrt\frac{N(a)}{\ln(t)} \right) \]

\noindent
$A$ ist die zu wählende Aktion. $N(a)$ ist die Anzahl, wie oft die Aktion $a$ schon
gewählt wurde. Durch den Term $c*\sqrt\frac{N(a)}{\ln(t)}$ wird sichergestellt, dass
selten benutzte Aktionen eine höhere Bewertung erhalten.

% \subsubsection{Umsetzung Tic-Tac-Toe}

\newpage
\subsection{Open AI Gym}
Ein von der Firma Open AI (\url{https://gym.openai.com/}) bereitgestelltes Toolkit,
welches es ermöglicht Verfahren im Bereich Reinforcement Learning algorithmisch umzusetzen
und anschließend miteinander zu vergleichen. Dabei werden Umgebungen wie Spiele oder
Simulationen anhand einer definierten API zur Verfügung gestellt. Hierbei wird erneut auf
die dabei zu beachtende Terminologie eingegangen und der Aufbau und die eigene
Realisierung eines Gym Systems thematisiert.

\begin{itemize}
\itemsep0pt
\item Erklärung und Verwendung: Environment, Observation Space, Action Space
\item Realisierung Environment Tic Tac Toe:
  % \url{https://github.com/dephiloper/independent-coursework-rl/blob/master/preparation/05_v_learning/tictactoe_v_learning.py}
\end{itemize}

\section{Cross-Entropy-Method}
Erläuterung der Cross-Entropy-Method und der Verwendung eines neuronalen Netzes zum Lernen
von Cart-Pole
% (\url{https://github.com/dephiloper/independent-coursework-rl/tree/master/preparation/04_cross_entropy}).
\\
Einordnung des Verfahrens im Bereich policy learning.

\section{Q-Learning}
Erläuterung des Q-Learning Verfahrens und deren Erweiterungen, dessen Einordnung im
Bereich value learning sowie Begriffserklärung und Implementierung von Q-learning anhand
der Verwendung eines bereits existierenden Gyms. Erläuterung folgender Terminologien:

% Bruno TODO: - write section
%             - tictactoe implementation example
%             - frozen lake
%             - frozen lake 2x2 explanation

\begin{itemize}
\itemsep0pt
\item value learning Familie
\item Bellman Equation und Proof of Bellman Optimality Equation 
\item Frozen Lake Environement
\item V(s) and Q(s,a)
\item einfache Erläuterung der Abwandlung von Frozen Lake: 2x2 Nonslip Lake
\item Vergleich von V(s) und Q(s,a) und deren Einsatz
\item Implementierung q-learning und v-learning Frozen Lake:
  % \url{https://github.com/dephiloper/independent-coursework-rl/tree/master/preparation/05_v_learning}
\end{itemize}

% Bruno TODO
\section{Deep Q-Learning}
Erweitung des Q-learning Verfahrens um neuronales Netz, was die Nutzung von Spielen und
Simulationen mit kontinuierlichem Spielbereich (Observation-Space) ermöglicht. Umsetzung
mehrerer Environements (Atari Pong, Roboschool Environments) und Beschreibung deren
Funktionsweise sowie folgender Terminologien und Verfahren mit diversen Implementierungen:
% \url{https://github.com/dephiloper/independent-coursework-rl/tree/master/preparation/07_deep_q_learning}

\begin{itemize}
\itemsep0pt
\item off-policy
\item Erklärung value learning mit Netz
\item value network
\item replay buffer, target network
\item atari pong (\url{https://gym.openai.com/envs/#atari})
\item roboschool pong environment (\url{https://github.com/openai/roboschool}
\item limitations
\item extensions: duelling dqn, categorical dqn, n-step dqn
\end{itemize}

% Philipp TODO
\section{Policy Gradients}
Erneute Beschreibung von Verfahren im Bereich policy learning. Dabei fokussieren der
Policy Gradients Verfahren und der REINFORCE Methode. Thematisierung und Vergleich von
policy und value sowie Erweiterungen.
\begin{itemize}
\itemsep0pt
\item cross entropy method (bereits erläutert) dazu im Kontrast: REINFORCE
\item continuous observation space \& action space
\item Umsetzung von cart pole
\item Umsetzung von atari pong
\item limitations
\item high variance, Einführung einer Baseline
\end{itemize}

% Philipp TODO
\section{Actor Critic}
Beschreibung einer state-of-the-art RL Methode Actor Critic. Kombination aus policy learning und value learning. Ansatz mit mehreren Environments parallel zur Beschleunigung und Stabilisierung des Lernprozesses.
\begin{itemize}
\itemsep0pt
\item actor and critic net
\item reducing variance $\rightarrow$ Advantage
\item extensions a3c
\end{itemize}

\section{Teeworlds}
Wie in der Einleitung erwähnt Erfolg die Umsetzung eines eigenen Environments mittels des
Spiels Teeworlds (\url{https://teeworlds.com/}). Initial wird das Spiel erläutert und
dessen grundlegende Struktur. Anschließend erfolgt eine Beschreibung zur Umsetzung des
Environments, wobei folgende Themen kategorisiert werden:
\begin{itemize}
\itemsep0pt	
\item Warum wir uns dafür entschieden haben
\item Wahl des Verfahrens
\item FOSS, teeworlds client, teeworlds server
\item custom environment (gym)
\item Bilder als Input $\rightarrow$ stack of images
\item Umsetzung einer vereinfachten Version des Spiels
\item die Aufgabe besteht darin, Schilde und Herzen einzusammeln und Level zu bestreiten
\item Implementierung erläutert mittels Listings
\item Herausforderungen bei der Implementierung und wie diese bewältigt wurden
\item Erklärung Hyperparameter, Lernprozess, Lernstats
\item Vergleich, Trainingsdauer, Auswertung, Erweiterungen
\item Beschreibung System Setup
\end{itemize}

\newpage
\bibliographystyle{plain}
\bibliography{references}
\newpage
\listoffigures 
\listoftables 
\end{document}
