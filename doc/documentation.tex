\documentclass[11pt]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage[german]{babel}
\usepackage{geometry}

\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\PassOptionsToPackage{hyphens}{url}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{subcaption}

\newcommand{\lbparagraph}[1]{\paragraph*{#1}\mbox{}\\}
\renewcommand{\baselinestretch}{1.2}
\geometry{
  left=3cm,
  right=3cm,
  top=3cm,
  bottom=3cm,
  bindingoffset=5mm
}
\usepackage{graphicx}

\begin{document}
% \sloppy

\begin{titlepage}
	\centering
	{\scshape\LARGE Hochschule für Technik und Wirtschaft Berlin \par}
	\vspace{2cm}
	{\Huge \scshape{Independent Coursework}\par}
	\vspace{2cm}
	{\LARGE \scshape{Reinforcement Learning}\par}
	{\scshape\Large Projekt Dokumentation\par}
	%\includegraphics[width=0.9\linewidth]{title.jpg}\par\vspace{1cm}
	\vspace{4cm}
	{\large\itshape Bruno Schilling,\\Philipp Bönsch\par}
	\vfill
	
% Bottom of the page
	{\large Betreuer: Prof. Dr.-Ing. Barthel \par}
	\vspace{1cm}
	{\large \today\par}
\end{titlepage}

\lstset{basicstyle=\ttfamily\small,breaklines=true}
\newpage
\tableofcontents
\newpage
\section{Introduction}

\subsection{Problem Case}
Machine Learning Methoden werden zunehmend benutzt, um Probleme in der realen Welt zu
lösen und erhalten daher in der Zukunft einen immer höheren Stellenwert. Spiele im
speziellen stellen meist Problemstellungen dar, welche zum Teil auch schon durch Machine
Learning Verfahren absolviert werden konnten. Darunter zählen klassische Spiele, wie Go
oder Schach (siehe Google Deep Mind, Alpha Zero) aber auch echtzeit Spiele, wie Dota 2,
Quake oder Starcraft (siehe OpenAI). Für das Supervised Research Independent Coursework
ergibt sich folgende Problemstellung: Welche Machine Learning Verfahren können für das
absolvieren eines Computerspieles verwendet werden und wie integriert man diese in die
virtuelle Umgebung?

\subsection{Objectives}
Es soll innerhalb eines Supervised Research Independent Courseworks untersucht werden, wie
sich ein Agent in einer virtuellen Spielumgebung mit Hilfe von Machine Learning Methoden,
speziell Reinforcement Learning Verfahren, steuern lässt. Dafür werden initial
Untersuchungen durchgeführt, um den aktuellen Stand der Technik zu erfassen. Genauer
sollen Algorithmen mittels der von OpenAI Gym zur Verfügung gestellten Testumgebung
untersucht werden. Die dabei gesammelten Erfahrungen sollen benutzt werden, um einen
Agenten für ein Computerspiel zu entwickeln. Das Ziel des Research Projektes ist es, neue
Kenntnisse im Bereich Reinforcement Learning zu erhalten und bereits bestehendes Wissen in
den Schwerpunkten Artificial Intelligence und Machine Learning zu festigen. Weiterhin soll
dieses Wissen anschließend in einem Prototyp umgesetzt werden, bei dem ein Agent in einem
Echtzeitspiel gesteuert wird.

\newpage
\section{Fundamentals}
In diesem Abschnitt sollen die Grundlagen des Reinforcement Learning Bereichs erläu\-tert
werden. Damit soll die Grundlage für Erläuterungen in nachfolgenden Kapiteln geschaffen
werden, welche sich auf die Terminologie dieses Abschnittes beziehen.
\subsection{Reinforcement Learning}
Erläuterung der Einordnung von Reinforcement Learning im Bereich Machine Learning sowie Beschreibung der nachfolgenden Terminologien:
\begin{itemize}
\itemsep0pt
\item Agent, Environment, Reward, Observation, Action
\item Exploration vs. Exploitation
\item model-free vs. model-based
\item on- vs. off-policy
\item policy learning vs. value learning
\end{itemize}
\subsection{Markov Decision Process}
Der Makrov Decision Process dient als Fundament von Umsetzungen im Bereich Reinforcement Learning. In diesem Kapitel sollen dabei die Zusammenhänge und Unterschiede der einzelnen Markov Prozesse erläutert werden. Dabei wird unterschieden in:
\begin{itemize}
\itemsep0pt
\item Markov Process
\item Markov Reward Process
\item Markov Decision Process 
\end{itemize}

\newpage
\subsection{k-armed Bandit}
Hierbei soll das k-armed Bandit Problem der Wahrscheinlichkeitstheorie erläutert werden
und dabei auf  Schwerpunkte Exploration vs. Exploitation, einem Kernproblem des
Reinforcement Learning, eingegangen werden. Die dabei realisierten Implementierungen
sollen genauer erläutert und deren Bewandtnis in allgemeinen RL genauer beschrieben
werden.

\subsubsection{Das k-armed-Bandit Problem}
% Erklärung des k-armed-Bandit Problems
Das k-armed-Bandit Problem beschreibt auf vereinfachte Weise eine Grundproblemstellung vor
der ein Agent steht. Das Problem besteht darin aus $k$ verschiedenen $Aktionen$ zu wählen.
Nachdem eine Aktion gewählt wurde, erhält der Agent einen $Reward$, der aus einer
Wahrscheinlichkeitsverteilung gezogen wird. Der Erwartungswert der
Wahrscheinlichkeitsverteilung hängt von der gewählten Aktion ab. Das Ziel besteht darin
mit $n$ Aktions den höchstmöglichen Reward zu erhalten.\\
Würden wir den Erwartungswert der einzelnen Verteilungen kennen, so wäre die Wahl der
Aktion einfach, indem die Aktion mit dem höchsten Erwartungswert gewählt werden würde. Der
Erwartungswert der hinter einer Verteilung einer Aktion steht wird $Value$ der Aktion
genannt.\\
Allgemein ist in Reinforcement Learning Problemen der Erwartungswert der Aktionen nicht
bekannt, sodass dieser durch versuchen ermittelt werden muss. Stattdessen gibt es nur
Abschätzungen bezüglich des Values einer Aktion. Das ermitteln des erwarteten Rewards wird
$Exploration$ genannt. Im Gegensatz dazu steht die $Exploitation$ bei der die gesammelten
Erfahrungen genutzt werden, um die Aktion auszuwählen, die den höchsten Reward verspricht.

\subsubsection{Exploration Strategien}
Im folgenden werden verschiedene Exploration-Strategien beleuchtet. Dazu werden diese erst
theoretisch erklärt, um diese anschließend praktisch an einem k-armed-Bandit Problem zu
vergleichen.
% Die Implementationen sind unter \url{https://github.com/Bluemi/rl_testbed} zu finden.

\paragraph{$\epsilon$ greedy / decaying $\epsilon$ greedy}
Eine Strategie, die zu jedem Zeitpunkt die Aktion wählt, die nach derzeitigem Wissen den
höchsten Reward verspricht wird $greedy$ genannt. Epsilon-greedy Strategien sind greedy
Strategien, die mit einer gewissen Wahrscheinlichkeit $\epsilon$ eine zufällige Aktion
wählen. Sie wählen also mit einer Wahrscheinlichkeit von $1 - \epsilon$ die Aktion, von
der sie sich den höchsten Reward versprechen und andernfalls eine zufällige Aktion.\\
In der Praxis werden häufig decaying $\epsilon$ greedy Strategien verwendet, bei denen der
$\epsilon$ Wert über den Trainingsprozess hinweg verringert wird. Dadurch werden zu Beginn
des Trainings viele neue Aktionen ausprobiert, während gegen Ende das Anwenden des
gelernten stärker in den Vordergrund rückt.

\paragraph{Optimistic Initial Values}
Ein Agent, der ein k-armed-Bandit Problem lösen soll, bewertet die unterschiedlichen
Aktionen, um Aktionen wählen zu können, die einen besseren Reward versprechen. Die
Bewertung der Aktionen wird beeinflusst durch die erfahrenen Rewards nach dem wählen einer
Aktion.\\
Neben $\epsilon$-greedy Verfahren ist es eine weitere Möglichkeit Exploration umzusetzen,
die initiale Bewertung der Aktionen sehr optimistisch vorzunehmen. Dadurch wird der Agent
bei der Wahl einer Aktion vom erhaltenen Reward "enttäuscht", sodass sich die Einschätzung
dieser Aktion verschlechtert. Wird nun immer die Aktion gewählt, die am besten
eingeschätzt ist, so werden über die Zeit alle Aktionen versucht, bis sie sich dem
tatsächlichen Erwartungswert nähern.

\paragraph{Upper Confidence Bound Action Selection}
Während $\epsilon$ greedy Methoden zufällig zwischen allen nicht greedy Aktionen
auswählen, wählt die Upper Confidence Bound Ac\-tion Selection Methode Aktionen entsprechend
dreier Parameter aus.\\
Der erste Parameter ist $t$ die Anzahl der Aktionen, die bis jetzt gespielt wurden, der
zweite ist der bis jetzt ermittelte $Value$ der Aktion $a$ bezeichnet mit $Q(a)$ und der
dritte Parameter ist die Anzahl, wie oft $a$ schon ausprobiert wurde. Die nächste zu
spielende Aktion wird dann nach folgender Gleichung gewählt:

\[ A = \argmax_a\left( Q(a) + c*\sqrt\frac{N(a)}{\ln(t)} \right) \]

\noindent
$A$ ist die zu wählende Aktion. $N(a)$ ist die Anzahl, wie oft die Aktion $a$ schon
gewählt wurde. Durch den Term $c*\sqrt\frac{N(a)}{\ln(t)}$ wird sichergestellt, dass
selten benutzte Aktionen eine höhere Bewertung erhalten.

% \subsubsection{Umsetzung Tic-Tac-Toe}

\newpage
\subsection{Open AI Gym}
Ein von der Firma Open AI (\url{https://gym.openai.com/}) bereitgestelltes Toolkit,
welches es ermöglicht Verfahren im Bereich Reinforcement Learning algorithmisch umzusetzen
und anschließend miteinander zu vergleichen. Dabei werden Umgebungen wie Spiele oder
Simulationen anhand einer definierten API zur Verfügung gestellt. Hierbei wird erneut auf
die dabei zu beachtende Terminologie eingegangen und der Aufbau und die eigene
Realisierung eines Gym Systems thematisiert.

\begin{itemize}
\itemsep0pt
\item Erklärung und Verwendung: Environment, Observation Space, Action Space
\item Realisierung Environment Tic Tac Toe:
  % \url{https://github.com/dephiloper/independent-coursework-rl/blob/master/preparation/05_v_learning/tictactoe_v_learning.py}
\end{itemize}

\section{Cross-Entropy-Method}
Erläuterung der Cross-Entropy-Method und der Verwendung eines neuronalen Netzes zum Lernen
von Cart-Pole
% (\url{https://github.com/dephiloper/independent-coursework-rl/tree/master/preparation/04_cross_entropy}).
\\
Einordnung des Verfahrens im Bereich policy learning.

\section{Q-Learning}
Erläuterung des Q-Learning Verfahrens und deren Erweiterungen, dessen Einordnung im
Bereich value learning sowie Begriffserklärung und Implementierung von Q-learning anhand
der Verwendung eines bereits existierenden Gyms. Erläuterung folgender Terminologien:

\begin{itemize}
\itemsep0pt
\item value learning Familie
\item Bellman Equation und Proof of Bellman Optimality Equation 
\item Frozen Lake Environement
\item V(s) and Q(s,a)
\item einfache Erläuterung der Abwandlung von Frozen Lake: 2x2 Nonslip Lake
\item Vergleich von V(s) und Q(s,a) und deren Einsatz
\item Implementierung q-learning und v-learning Frozen Lake:
  % \url{https://github.com/dephiloper/independent-coursework-rl/tree/master/preparation/05_v_learning}
\end{itemize}

\section{Deep Q-Learning}
Erweitung des Q-learning Verfahrens um neuronales Netz, was die Nutzung von Spielen und
Simulationen mit kontinuierlichem Spielbereich (Observation-Space) ermöglicht. Umsetzung
mehrerer Environements (Atari Pong, Roboschool Environments) und Beschreibung deren
Funktionsweise sowie folgender Terminologien und Verfahren mit diversen Implementierungen:
% \url{https://github.com/dephiloper/independent-coursework-rl/tree/master/preparation/07_deep_q_learning}

\begin{itemize}
\itemsep0pt
\item off-policy
\item Erklärung value learning mit Netz
\item value network
\item replay buffer, target network
\item atari pong (\url{https://gym.openai.com/envs/#atari})
\item roboschool pong environment (\url{https://github.com/openai/roboschool}
\item limitations
\item extensions: duelling dqn, categorical dqn, n-step dqn
\end{itemize}

\section{Policy Gradients}
Erneute Beschreibung von Verfahren im Bereich policy learning. Dabei fokussieren der
Policy Gradients Verfahren und der REINFORCE Methode. Thematisierung und Vergleich von
policy und value sowie Erweiterungen.
\begin{itemize}
\itemsep0pt
\item cross entropy method (bereits erläutert) dazu im Kontrast: REINFORCE
\item continuous observation space \& action space
\item Umsetzung von cart pole
\item Umsetzung von atari pong
\item limitations
\item high variance, Einführung einer Baseline
\end{itemize}

\section{Actor Critic}
Beschreibung einer state-of-the-art RL Methode Actor Critic. Kombination aus policy learning und value learning. Ansatz mit mehreren Environments parallel zur Beschleunigung und Stabilisierung des Lernprozesses.
\begin{itemize}
\itemsep0pt
\item actor and critic net
\item reducing variance $\rightarrow$ Advantage
\item extensions a3c
\end{itemize}

\section{Teeworlds}
Wie in der Einleitung erwähnt Erfolg die Umsetzung eines eigenen Environments mittels des Spiels Teeworlds (\url{https://teeworlds.com/}). Initial wird das Spiel erläutert und dessen grundlegende Struktur. Anschließend erfolgt eine Beschreibung zur Umsetzung des Environments, wobei folgende Themen kategorisiert werden:
\begin{itemize}
\itemsep0pt	
\item FOSS, teeworlds client, teeworlds server
\item custom environment (gym)
\item Bilder als Input $\rightarrow$ stack of images
\item Umsetzung einer vereinfachten Version des Spiels
\item die Aufgabe besteht darin, Schilde und Herzen einzusammeln und Level zu bestreiten
\item Implementierung erläutert mittels Listings
\item Herausforderungen bei der Implementierung und wie diese bewältigt wurden
\item Erklärung Hyperparameter, Lernprozess, Lernstats
\item Vergleich, Trainingsdauer, Auswertung, Erweiterungen
\item Beschreibung System Setup
\end{itemize}

\newpage
\bibliographystyle{plain}
\bibliography{references}
\newpage
\listoffigures 
\listoftables 
\end{document}
